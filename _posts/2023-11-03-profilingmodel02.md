---
layout: single
title:  "Part 01. ì˜¨ë¼ì¸ ë©”ì‹œì§€ ì‘ì„±ì í”„ë¡œíŒŒì¼ë§ ëª¨ë¸ ê°œë°œ: ìì—°ì–´ ì „ì²˜ë¦¬ ë° ë³€ìˆ˜ê°œë°œ"
categories: Project:Profileing_model
tag: [NLP, ë¶ˆìš©ì–´ì²˜ë¦¬, í‘œì œí™”, í† í¬ë‹ˆì œì´ì…˜, ë³€ìˆ˜ê°œë°œ]
---
<span style="color: #808080">#NLP #ë¶ˆìš©ì–´ ì²˜ë¦¬ #í‘œì œí™” #í† í¬ë‹ˆì œì´ì…˜ #ë³€ìˆ˜ê°œë°œ</span>
<hr>

{: .notice--primary} 
ğŸ’¡**í”„ë¡œì íŠ¸ ë°°ê²½**<br>

ê°œì¸ì •ë³´ ë³´í˜¸ì— ëŒ€í•œ ì‚¬íšŒì  ë¶„ìœ„ê¸°ì— ë”°ë¼ êµ¬ê¸€ ì¨ë“œíŒŒí‹° ì œê³µ ì¤‘ë‹¨, ì• í”Œ ì‚¬ìš©ì ì •ë³´ ê³µê°œ ì¤‘ë‹¨ ë“± ì‚¬ìš©ì ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì´ ì–´ë ¤ì›Œì§€ê³  ìˆìŒ. ìì‚¬ í”Œë«í¼ì— ê°€ì…í•œ ì‚¬ìš©ì ì™¸ ê³ ê° ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì€ ë”ìš± ì–´ë ¤ì›€.<br><br> 
ë§¥ë½ì •ë³´ë¥¼ í™œìš©í•œ ê³ ê° ë¶„ì„ ë° íƒ€ê²ŸíŒ… ì „ëµì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìœ¼ë‚˜ ëŒ€í‘œì ìœ¼ë¡œ ê³ ê°ì´ ìƒì‚°í•˜ëŠ” ë§¥ë½ì •ë³´ì¸ ì±„íŒ…, ë¦¬ë·°, í”¼ë“œ ë“±ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” ë§¤ìš° ì œí•œì ì„. íŠ¹íˆ, ì‚¬ìš©ìì˜ ì¸êµ¬í†µê³„ì  ì •ë³´ê°€ ì œê³µë˜ëŠ” ê²½ìš°ëŠ” ë§¤ìš° ë“œë¬¼ì–´ ê³ ê° ë¶„ì„ ë° íƒ€ê²ŸíŒ…ì— í™œìš©í•  ìˆ˜ ì—†ìŒ.<br><br> 
**ì±„íŒ…, ë¦¬ë·°, í”¼ë“œ ë“±ì—ì„œ ì‚¬ìš©ìì˜ ì¸êµ¬ í†µê³„ì  ì •ë³´ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´, ê³ ê° ë¶„ì„ ë° íƒ€ê²ŸíŒ…ì— í™œìš©í•  ìˆ˜ ìˆì„ ë¿ë§Œì•„ë‹ˆë¼ ì±—ë´‡/ë©”íƒ€ë²„ìŠ¤ ì„œë¹„ìŠ¤/CRM ì„œë¹„ìŠ¤ ë“±ì„ ê³ ë„í™”ì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë¨**<br><br>

{: .notice--primary} 
ğŸ¯**í”„ë¡œì íŠ¸ ëª©ì **<br>

ê³ ê°ì´ ì˜¨ë¼ì¸ ìƒì—ì„œ ìƒì‚°í•˜ëŠ” ë§¥ë½ì •ë³´(ì±„íŒ…, ë¦¬ë·°, í”¼ë“œ ë“±ì˜ í…ìŠ¤íŠ¸ ì •ë³´)ì—ì„œ ì‘ì„±ìì˜ ì¸êµ¬ í†µê³„ì  ì •ë³´(ì„±ë³„/ì—°ë ¹)ë¥¼ ì¶”ì •í•˜ëŠ” ì‘ì„±ì í”„ë¡œíŒŒì¼ë§ ëª¨ë¸ ê°œë°œ<br><br>


## ë³€ìˆ˜ ê°œë°œ ìš”ì•½<br>
**ë³€ìˆ˜ ê°œë°œì˜ í•„ìš”ì„±**
- ì¤„ì„ë§, ê³ ìœ  í‘œí˜„, ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œë§Œ êµ¬ì„±ëœ ë¶ˆì™„ì „ í‘œí˜„, ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ ë“±ì˜ í‘œí˜„ì€ **ê¸°ì¡´ì˜ NLP ì „ì²˜ë¦¬ ë° ì–¸ì–´ëª¨ë¸ì˜ Encoding ê³¼ì •ì—ì„œ ì œê±°ë¨**. í•˜ì§€ë§Œ, ì´ì™€ ê°™ì€ í‘œí˜„ë“¤ì´ **ë°œí™”ìì˜ ì„±ë³„/ì—°ë ¹ì— ëŒ€í•œ ì–¸ì–´ì  íŠ¹ì§•ìœ¼ë¡œ ë³´ì—¬ì§€ë¯€ë¡œ** ìƒê´€ê´€ê³„ íŒŒì•… ë° ëª¨ë¸ê°œë°œ ì‹œ ë°˜ì˜ì„ ìœ„í•œ feauture ê°œë°œ í•„ìš”
- ëª¨ë“  í‘œí˜„ì„ encodingí•˜ëŠ” ë°©ì‹ì€ ë¹„íš¨ìœ¨ì ì¼ë¿ë§Œ ì•„ë‹ˆë¼ ì‚¬ì „ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ **í‘œí˜„ì˜ íŠ¹ì„±ì„ ê³„ëŸ‰í™”í•œ ë³€ìˆ˜**ê°€ ìš”êµ¬ë¨
<br><br>

### 1. í‘œí˜„ì˜ ê¸¸ì´ì™€ ì² ìì˜ ë‹¤ì–‘ì„±ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ë³µì¡ë„ ê³„ì‚°
- **Complexity of Specific Expression(C, íŠ¹ìˆ˜í‘œí˜„ ë³µì¡ë„)**: **í‘œí˜„ tê°€ ë³µì¡í•œ ì •ë„**ë¥¼ ì„¤ëª…í•¨
    > ì „ì²˜ë¦¬ì—ì„œ ëŒ€ë¶€ë¶„ ì†Œì‹¤ë˜ëŠ” ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ í‘œí˜„, ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ë¡œ êµ¬ì„±ëœ í‘œí˜„ì— ëŒ€í•œ ë³µì¡ë„ë¥¼ ê³„ì‚°<br>
    > ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ í‘œí˜„ì„ í•œ ë¶„ë¥˜ë¡œ í•˜ê³ , ë¬¸ì¥ê¸°í˜¸/íŠ¹ìˆ˜ê¸°í˜¸/ì´ëª¨ì§€ë¥¼ í•œ ë¶„ë¥˜ë¡œ êµ¬ë¶„í•˜ì—¬ ê³„ì‚°(í˜•íƒœì†Œ ë¶„ë¥˜ ë° ì •ê·œì‹ ì²˜ë¦¬ì— ìš©ì´)
    > <br><br>
    > $C_i= ln[ 1/Î (U_i / L_i) * L_i]$
    > - $U_i$ : í‘œí˜„ $t_i$ ê°€ í¬í•¨í•˜ëŠ” ê³ ìœ í•œ ì² ì ë˜ëŠ” ê¸°í˜¸ì˜ ì¢…ë¥˜ ìˆ˜
    > - $L_i$ : í‘œí˜„ $t_i$ ì˜ ê¸¸ì´(ì² ì ë° ê¸°í˜¸ ê°œìˆ˜)
    > <br>
    > í‘œí˜„ì„ ì´ë£¨ëŠ” ê³ ìœ  ì² ì ë° ê¸°í˜¸ ë³„ ì ìœ ìœ¨ $U_i / L_i$ ì„ ëª¨ë‘ ê³±í•˜ê³ , í‘œí˜„ì˜ ê¸¸ì´ë¥¼ ê³±í•˜ì—¬ ê³ ìœ  ì² ìì˜ ì¢…ë¥˜ì™€ í‘œí˜„ì˜ ê¸¸ì´ì— ë¹„ë¡€í•˜ëŠ” ë³µì¡ë„ ì¸¡ì •. ë‹¤ì–‘í•œ ì² ìê°€ ì‚¬ìš©ë  ìˆ˜ë¡, ì‚¬ìš©ëœ ê³ ìœ  ì² ì ë° ê¸°í˜¸ ìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ë³µì¡ë„ê°€ ì»¤ì§
&nbsp;&nbsp;â‡’ ì „ì²˜ë¦¬ ì „ì˜ raw í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì¸¡ì •í•˜ë©°, ì „ì²˜ë¦¬ì—ì„œ ì†Œì‹¤ë˜ëŠ” íŠ¹ìˆ˜í‘œí˜„ë“¤ì˜ ì •ë³´ë¥¼ ì¼ë¶€ ë°˜ì˜í•¨
<br><br>

### 2. ê° í‘œí˜„ì˜ ì¢…ì†ë³€ìˆ˜(ì„±ë³„/ì—°ë ¹)ì— ëŒ€í•œ Odds Ratioë¥¼ ê³„ì‚°
- **Relative Bias(RB, ìƒëŒ€ í¸í–¥ë„)**: í‘œí˜„ tê°€ ë“±ì¥ í–ˆì„ ë•Œ, í…ìŠ¤íŠ¸ì˜ **ì‘ì„±ìê°€ íŠ¹ì • ì„±ë³„/ì—°ë ¹ì¸ ì •ë„**ë¥¼ ì„¤ëª…í•¨
    > **Relative Bias of Gender**(RBG, ìƒë³„ì— ëŒ€í•œ ìƒëŒ€ í¸í–¥ë„)
    > : í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¬¸ì¥ $s$ ì˜ ì‘ì„±ì ì„±ë³„ì´ ë‚¨ì $male$ ë˜ëŠ” ì—¬ì $female$ ì¸ ì •ë„<br>
    > <br>
    > $RBG_i = ln[ p( s_{male} â t_iâˆˆs_{male} ) / p( s_{female} â t_iâˆˆs_{female} ) ]$
    > - $t_i$ : ë¬¸ì„œ ë‚´ ië²ˆ ì§¸ í‘œí˜„<br>
    > - $s_{male}$ : ì‘ì„±ìì˜ ì„±ë³„ì´ ë‚¨ì„±(m)ì¸ ë¬¸ì¥
    > - $s_{female}$ : ì‘ì„±ìì˜ ì„±ë³„ì´ ì—¬ì„±(f)ì¸ ë¬¸ì¥
    > <br>
    > ì‘ì„±ìì˜ ì„±ë³„ì´ ë‚¨ì„±ì¸ ë¬¸ì¥ $s_{male}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨ Ã· ì—¬ì„±ì¸ ë¬¸ì¥ $s_{female}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨, 0~1 ì‚¬ì´ì˜ Skewedí•œ ê°’ì„ ê°€ì§ìœ¼ë¡œ logë¥¼ ì·¨í•´ ì •ê·œí™”
    > â‡’ $t_i$ ê°€ ë“±ì¥í–ˆì„ ë•Œ ì‘ì„±ìì˜ ì„±ë³„ì´ ë‚¨ì $m$ ë˜ëŠ” ì—¬ì $f$ ì¸ ì •ë„
    <br>    

    > **Relative Bias of Age**(RBG, ì—°ë ¹ì— ëŒ€í•œ ìƒëŒ€ í¸í–¥ë„)<br>
    > : í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¬¸ì¥ $s$ ì˜ ì‘ì„±ì ì—°ë ¹ì´ íŠ¹ì • ì—°ë ¹ëŒ€ $age$ ì¸ ì •ë„
    > &nbsp;&nbsp;ì—°ë ¹ëŒ€ëŠ” 20ëŒ€ ë¯¸ë§Œ/20ëŒ€/30ëŒ€/40ëŒ€/50ëŒ€ ì´ìƒ 5 classë¡œ ë¶„ë¥˜<br>
    > <br>
    > $RBAi = ln[ p( s_{age} â t_iâˆˆs_{age} ) / p( s_{other} â t_iâˆˆs_{other} ) ]$
    > - $t_i$ : ë¬¸ì„œ ë‚´ ië²ˆ ì§¸ í‘œí˜„
    > - $s_{age}$ : ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ ageì¸ ë¬¸ì¥
    > - $s_{other}$ : ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ ageê°€ ì•„ë‹Œ ë¬¸ì¥
    > <br>
    > ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ 20ëŒ€ì¸ ë¬¸ì¥ $s_{a20}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨ Ã· ì—°ë ¹ëŒ€ê°€ 20ëŒ€ê°€ ì•„ë‹Œ ë¬¸ì¥ $s_{other}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨, 0~1 ì‚¬ì´ì˜ Skewedí•œ ê°’ì„ ê°€ì§ìœ¼ë¡œ logë¥¼ ì·¨í•´ ì •ê·œí™”
    > â‡’ $t_i$ê°€ ë“±ì¥í–ˆì„ ë•Œ ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ ageì¸ ì •ë„
    <br>  
    
- **Relative frequency (RF, ìƒëŒ€ ë¹ˆì¶œë„)**: í…ìŠ¤íŠ¸ì˜ ì‘ì„±ìê°€ íŠ¹ì • ì„±ë³„/ì—°ë ¹ì¼ ë•Œ íƒ€ ì„±ë³„/ì—°ë ¹ ë³´ë‹¤ **í‘œí˜„ të¥¼ ìƒëŒ€ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì •ë„**ë¥¼ ì„¤ëª…í•¨
    > ìƒëŒ€ í¸í˜•ë„ëŠ” ë¬¸ì¥ì—ì„œ í‘œí˜„ì˜ ì¶œì—°ì—¬ë¶€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ë°˜ë©´, ìƒëŒ€ ë¹ˆì¶œë„ëŠ” ë¹ˆë„ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨
    > ìƒëŒ€ í¸í–¥ë„ì™€ ê³„ì‚° ë°©ì‹ì´ ìœ ì‚¬í•˜ë¯€ë¡œ ì„¤ëª…ì€ ìƒëµí•¨<br>
    > <br>
    > $RFG_i = ln[ p( t_i â s_{male} ) / p( t_i â s_{female} ) ]$
    > $RFAi = ln[ p( t_i â s_{age} ) / p( t_i â s_{other} ) ]$
    <br>
    
- ë¬¸ì¥ì´ í¬í•¨í•˜ëŠ” í† í° ì „ì²´ì— ëŒ€í•œ ìƒëŒ€ í¸í–¥ë„ ë° ìƒëŒ€ ë¹ˆì¶œë„ì˜ í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•¨ìœ¼ë¡œì¨ ê¸°ì¡´ì˜ NLP ë°©ì‹ì—ì„œ ì†Œì‹¤ë˜ëŠ” í† í° ì •ë³´ì˜ ì¼ë¶€ë¶„ ë³´ì™„í•  ìˆ˜ ìˆìŒ
- í‘œí˜„ë³„ ìƒëŒ€ í¸í–¥ë„ ë° ìƒëŒ€ í¸í–¥ë„ì— ëŒ€í•œ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•ì´ í•„ìš”
- Odds ratioì˜ ê°’ì„ ì¼ë°˜í™”í•  ìˆ˜ ìˆì„ ë§Œí¼ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì „ì œë˜ì–´ì•¼ í•˜ë©°, ë³¸ ë°ì´í„°ì…‹ì€ ê·¸ì— ì¤€í•œë‹¤ê³  ê°€ì •í•¨
- ìƒëŒ€ í¸í–¥ë„ì™€ ìƒëŒ€ ë¹ˆì¶œë„ê°€ ë§¤ìš° ìœ ì‚¬í•¨ìœ¼ë¡œ ìœ ì˜ì„± ê²€í† ë¥¼ í†µí•´ 1ê°€ì§€ë§Œ ì±„íƒ
<br> 




## 01. íŠ¹ìˆ˜í‘œí˜„ ë³µì¡ë„ ì¸¡ì •
- ë¶ˆìš©ì–´ ì²˜ë¦¬/í‘œì œí™” ì „ì— ìˆ˜í–‰
- ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë¶ˆì™„ì „ í‘œí˜„ / ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ í‘œí˜„ì„ êµ¬ë¶„í•˜ì—¬ ì¸¡ì •
- ë¬¸ì¥ì—ì„œ ì¶œí˜„í•˜ëŠ” íŠ¹ìˆ˜í‘œí˜„ì˜ ê°œìˆ˜, í‰ê· , í‘œì¤€í¸ì°¨, ìµœëŒ€ê°’, ìµœì†Œê°’ì„ ê³„ì‚°




```python
import pandas as pd
from tqdm import tqdm
tqdm.pandas()

import re
import numpy as np
from collections import Counter

df = pd.read_csv("SNS_FULL_Dataset(raw_ì¤‘ë³µëœ ë©”ì„¸ì§€ ì œê±°).csv")
```



<br><br>
### ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë¶ˆì™„ì „ í‘œí˜„ì˜ ë³µì¡ë„


```python
def spell_complexity(sentence):
    sequences = re.findall(r'[ã„±-ã…ã…-ã…£]+', sentence)
    specific_sequences = [seq for seq in sequences if len(seq) > 0]
    
    N = len(specific_sequences)
    
    if N != 0:
        complexity_list=[]
        for seq in specific_sequences:
            spell_list = []
            for s in seq:
                spell_list.append(s)  
```




```python
print(spell_complexity('ì•ˆë…• ã… ã…  ã… ã… ã…  ã… ã…œ ã…‡ã…‹ã…‡ã…‹ ã„·ã…‹-ã…ã…- ã… '))
```

    (7, 1.3451969221982076, 0.9131088298012547, 2.772588722239781, 0.0)




```python
print(spell_complexity('ì•ˆ ã… ã…  ã…‹ã…‹ã…‹'))
```

    (2, 0.8958797346140275, 0.20273255405408225, 1.0986122886681098, 0.6931471805599453)



<br><br> 
### ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ í‘œí˜„ì— ëŒ€í•œ ë³µì¡ë„




```python
def symbol_complexity(sentence):
    sequences = re.findall(r'[^ã„±-ã…ã…-ã…£ê°€-í£a-zA-Z0-9\s]+', sentence)
    specific_sequences = [seq.strip() for seq in sequences if seq.strip()]
    
    N = len(specific_sequences)
    
    if N != 0:
        complexity_list=[]
        for seq in specific_sequences:
            spell_list = []
            for s in seq:
                spell_list.append(s)  

            uniqe_list = Counter(spell_list)
            uniqe_freq = list(uniqe_list.values())

            L = len(spell_list)
            uniqe_num =len(uniqe_freq)


            ratio_list = []
            for fu in uniqe_freq:
                ratio = (fu/L)
                ratio_list.append(ratio)

            ans = 1
            for r in ratio_list:
                ans *= r

            complexity = np.log(1/ans*L)
            complexity_list.append(complexity)
            
        mean = np.mean(complexity_list)
        std =  np.std(complexity_list)
        max_ = np.max(complexity_list)
        min_ = np.min(complexity_list)
    else:
        mean = 0
        std = 0
        max_ = 0
        min_ = 0
        
    return N, mean, std, max_, min_
```


```python
print(symbol_complexity('ã… ã…  ì•ˆë…•! ??... ì˜ˆì‹œ: 2^^; -_-;; o_O ğŸ˜˜ ğŸ™ â˜ºï¸ â¤ï¸ğŸ§¡ğŸ’›'))
```

    (10, 1.970161458941443, 2.3466123014783795, 6.931471805599453, 0.0)



```python
print(symbol_complexity('ã… ã…  ì•ˆë…•ğŸ˜˜ğŸ˜˜ğŸ˜˜ ğŸ˜˜ğŸ˜˜'))
```

    (2, 0.8958797346140275, 0.20273255405408225, 1.0986122886681098, 0.6931471805599453)



```python
#ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ê°’ìœ¼ë¡œ ë³€í™˜
df['spell_complexity'] = df['contents'].progress_apply(lambda x:spell_complexity(str(x)))
df['spell_num'] = df['spell_complexity'].progress_apply(lambda x:x[0])
df['spell_mean'] = df['spell_complexity'].progress_apply(lambda x:x[1])
df['spell_std'] = df['spell_complexity'].progress_apply(lambda x:x[2])
df['spell_max'] = df['spell_complexity'].progress_apply(lambda x:x[3])
df['spell_min'] = df['spell_complexity'].progress_apply(lambda x:x[4])


df['symbol_complexity'] = df['contents'].progress_apply(lambda x:symbol_complexity(str(x)))
df['symbol_num'] = df['symbol_complexity'].progress_apply(lambda x:x[0])
df['symbol_mean'] = df['symbol_complexity'].progress_apply(lambda x:x[1])
df['symbol_std'] = df['symbol_complexity'].progress_apply(lambda x:x[2])
df['symbol_max'] = df['symbol_complexity'].progress_apply(lambda x:x[3])
df['symbol_min'] = df['symbol_complexity'].progress_apply(lambda x:x[4])


df = df.drop(columns={'spell_complexity', 'symbol_complexity'})
df = df.fillna(0)
df
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3564042/3564042 [04:53<00:00, 12152.45it/s]
    ...ìƒëµ...





<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>topic</th>
      <th>sex</th>
      <th>age</th>
      <th>resident</th>
      <th>contents</th>
      <th>length</th>
      <th>spell_num</th>
      <th>spell_mean</th>
      <th>spell_std</th>
      <th>spell_max</th>
      <th>spell_min</th>
      <th>symbol_num</th>
      <th>symbol_mean</th>
      <th>symbol_std</th>
      <th>symbol_max</th>
      <th>symbol_min</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>ë‚˜ì§€ê¸ˆë°¥ë¨¸ê±°2ì‹œê°„ê±¸ì–´ì„œ ë²ˆí™”ê°€ì°¾ì•˜ì–´..ã…œã…œ ì‰ã…œã…œ ã…ã…ã…ã…ì˜¤ì¢‹ê² ë„¤ ã…‹ã„±ã…‹ã„±ã„±ã„±ã„±ì•„ë‹ˆ...</td>
      <td>127</td>
      <td>6</td>
      <td>2.185021</td>
      <td>1.299761</td>
      <td>3.765840</td>
      <td>0.693147</td>
      <td>4</td>
      <td>0.173287</td>
      <td>0.300142</td>
      <td>0.693147</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>í— ã… ã…  ì–¸ë„í˜¸í…”ë“¤ê°€ã… ã…  ì—„ì²­í”¼ê±´í• ì²¸ë° ë‚˜ëŠ”ì¸ë‚«ëŸ¬ìš” ë‚˜ ë‘ì‹œì¶œê·¼ì´ë‹¤ã…ã…ã…ã… í€µìœ¼ë¡œí•œ...</td>
      <td>130</td>
      <td>6</td>
      <td>0.961387</td>
      <td>0.555163</td>
      <td>1.609438</td>
      <td>0.000000</td>
      <td>6</td>
      <td>2.041180</td>
      <td>1.296771</td>
      <td>4.212128</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>í•™ìƒì´ë©´ì¢‹êµ¬! ì™œí˜¼ìë‹¤ë‹ˆëƒê³ ì˜¤..... ì™€ ë‚´ì¹œêµ°í•™êµë‚˜ê° ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ ê·¸ë¥´ë„¤ ...</td>
      <td>56</td>
      <td>1</td>
      <td>2.197225</td>
      <td>0.000000</td>
      <td>2.197225</td>
      <td>2.197225</td>
      <td>3</td>
      <td>1.404043</td>
      <td>1.072424</td>
      <td>2.602690</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>í›” í•™ìƒ ì—†ëŠ”ë°...ì£¼ë³€ì—... ì•„ë‹ˆ ë³µí•™í•˜ê³  í•™êµë¥¼ ëª»ê°€ëŠ”ë° ì–´ì¼€ ì¹œêµ¬ê°€ìˆëƒ.. ...</td>
      <td>74</td>
      <td>1</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>2.079442</td>
      <td>4</td>
      <td>0.997246</td>
      <td>0.175572</td>
      <td>1.098612</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ì—¬ì„±</td>
      <td>30ëŒ€</td>
      <td>ì¶©ì²­ë¶ë„</td>
      <td>ì°¸ë‚˜ ë‚´ê°€ë­ì–¼ë§ˆë‚˜ê·¸ë¬ë‹¤ê³  ì›ƒê¸°ëŠ”ì‚¬ëŒì´ì•¼ì§€ì§œ ë„ˆë¬´í™”ë‚œë‹¹.. ê·¼ë°ì˜¤ë¹ ëŠ”ë§ì„ë˜ ì˜í•´ì„œ ë‚´...</td>
      <td>146</td>
      <td>2</td>
      <td>0.693147</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>1</td>
      <td>0.693147</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>
<p>3564042 rows Ã— 16 columns</p>

</div>




```python
df.to_csv('SNS_FULL_Dataset(raw_í‘œí˜„ ë³µì¡ë„ ê³„ì‚°).csv', index=False)
```



<br><br>
## 02. ìì—°ì–´ ì „ì²˜ë¦¬
- 3ë²ˆ ì´ìƒ ë°˜ë³µë˜ëŠ” ë™ì¼ ìŒì ˆì€ 3ìŒì ˆë¡œ í‘œì œí™” *ex. ã…‡ã…‡ã…‡ã…‡ã…‡ â‡’ ã…‡ã…‡ã…‡
- ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€, íŠ¹ìˆ˜í°íŠ¸ í‘œí˜„ í‘œì œí™” *ex. â¤ğŸ§¡ğŸ’› â‡’ â™¥ï¸â™¥ï¸â™¥ï¸ / ãƒ²ğ¨›ğŒ…â«¬ â‡’ ã…‹ã…‹ã…‹ã…‹
- ì—°ì†ëœ ë„ì–´ì“°ê¸°(ê³µë°±) í•œ ë²ˆìœ¼ë¡œ ì²˜ë¦¬
- 5ì–´ì ˆ ë¯¸ë§Œ ë§ë­‰ì¹˜ ë°ì´í„° ì œê±° 


```python
import re

raw = df.shape[0]
print('- raw data: {:,}'.format(raw))


#ê²°ì¸¡ê°’ ì œê±°
print('- null data: {:,}'.format(df['contents'].isnull().sum()))
df = df.dropna() 
deleted_null = df.shape[0]



#ì¤‘ë³µ ì œê±°
df = df.drop_duplicates() 
deleted_dup = df.shape[0]
print('- duplicated data: {:,}'.format(deleted_null - deleted_dup))



#ë¬¸ì¥ ê¸¸ì´ ì»¬ëŸ¼ ì¶”ê°€ê°€
df['contents_length'] = df['contents'].apply(lambda x : len(x))
```

    - raw data: 3,564,042
    - null data: 0
    - duplicated data: 0
    
<br><br>
### ì£¼ìš” ì´ëª¨ì§€ ë° íŠ¹ìˆ˜í‘œí˜„ í‘œì œí™”


```python
def emoji_lemmatization(sentence):
    heart_emoji = ['â™¡', 'â™¥', 'â¤', 'â¤ï¸', 'ğŸ§¡', 'ğŸ’›', 'ğŸ’š', 'ğŸ’™', 'ğŸ’œ', 'ğŸ’•'] #
    star_emoji = ['â˜†', 'â˜…', 'â­', 'ğŸŒŸ']
    kkk = ['ğ¨›', 'ğŒ…', 'â«¬', 'ãƒ²', 'åˆ', 'ã‰ª', 'ï½¦']
    Period = ['ã†', 'á†', 'ã†', 'â€¢', 'á†¢']
    quote = ['â€', 'â€˜', 'â€œ']
    ect = ['Â ', 'ã…¤']
    
    for i in range(len(sentence)):
        if sentence[i] in heart_emoji:
            sentence = sentence.replace(sentence[i], 'â™¥')
        elif sentence[i] in star_emoji:
            sentence = sentence.replace(sentence[i], 'â˜…')
        elif sentence[i] in kkk:
            sentence = sentence.replace(sentence[i], 'ã…‹')
        elif sentence[i] in Period:
            sentence = sentence.replace(sentence[i], '.')
        elif sentence[i] in quote:
            sentence = sentence.replace(sentence[i], '\'')
        elif sentence[i] in ect:
            sentence = sentence.replace(sentence[i], ' ')
        else:
            pass
    return(sentence)

def kkk_lemmatization(sentence):
    kkk2 =['ã…‹ê™¼Ìˆ', 'ã…‹Ì‘Ìˆ', 'ã…‹Ì†Ì', 'ã…‹ÌÌˆ', 'ã…‹ÌŠÌˆ', 'ã…‹Ì„Ìˆ', 'ã…‹Ì†Ìˆ', 'ã…‹ÌŠÌˆ', 'ã…‹ÌÌˆ', 'ã…‹Ì†Ì']
    
    for i in range(len(kkk2)):
        if kkk2[i] in sentence:
            sentence =  sentence.replace(kkk2[i], 'ã…‹')
        else:
            pass
    return(sentence)

text_sentence = 'â¤ğŸ§¡ğŸ’›í…ŒìŠ¤íŠ¸ğŸ’šğŸ’™ğŸ’œâ˜†ì…ë‹ˆë‹¤â˜…á†¢â­ ãƒ²ğ¨›ğŒ…â«¬ã…‹Ì„Ìˆã…‹ê™¼Ìˆã…‹Ì†Ìã…‹ÌÌˆã…‹ÌŠÌˆã…‹Ì„Ìˆã…‹ê™¼Ìˆã…‹Ì†Ìã…‹ÌÌˆã…‹ÌŠÌˆ'
text_sentence = emoji_lemmatization(text_sentence)
text_sentence = kkk_lemmatization(text_sentence)
text_sentence
```




    'â™¥â™¥â™¥í…ŒìŠ¤íŠ¸â™¥â™¥â™¥â˜…ì…ë‹ˆë‹¤â˜….â˜… ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹'




```python
df['contents'] = df['contents'].progress_apply(emoji_lemmatization)
df['contents'] = df['contents'].progress_apply(kkk_lemmatization)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3564042/3564042 [05:13<00:00, 11354.18it/s]
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3564042/3564042 [00:10<00:00, 330031.43it/s]
    
<br><br>
### ë°˜ë³µë˜ëŠ” ë™ì¼ ìŒì ˆ í‘œì œí™” ì²˜ë¦¬


```python
def duplicated_spelling_reduction(sentence):
    reduced_spellings = []
    duplicated_num = 1
    for i in range(len(sentence)):
        spelling = sentence[i]
        try:
            previous_spelling = sentence[i-1]
            
        except:
            previous_spelling = 'first_spelling'
        
        if spelling == previous_spelling:
            duplicated_num += 1
        else:
            duplicated_num = 1
            pass
        
        if duplicated_num <= 5:
            reduced_spellings.append(spelling)
        else:
            pass      
        
    reduced_sentence = ''.join(reduced_spellings).replace('   ', ' ').replace('  ', ' ')
    return(reduced_sentence)

text_sentence = '    ì•ˆë…•ì•ˆë…• í—¤í—¤í—¤ ã…‹ã…‹ã…‹ã…‹ ã…ã…ã…ã…ã…ã…ã…ã… ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹'
duplicated_spelling_reduction(text_sentence)
```




    ' ì•ˆë…•ì•ˆë…• í—¤í—¤í—¤ ã…‹ã…‹ã…‹ã…‹ ã…ã…ã…ã…ã… ã…‹ã…‹ã…‹ã…‹ã…‹'




```python
df['contents'] = df['contents'].progress_apply(duplicated_spelling_reduction)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3564042/3564042 [02:02<00:00, 29125.25it/s]
    
<br><br>
### ì—°ì†ëœ ë„ì–´ì“°ê¸° ì²˜ë¦¬/5ì–´ì ˆ ë¯¸ë§Œ ë¬¸ì¥ ì œê±°


```python
#ì—°ì†ë˜ ë„ì–´ì“°ê¸° ì²˜ë¦¬
df['contents'] = df['contents'].apply(lambda x : re.sub(r'\s', ' ', x))  #ë„ì–´ì“°ê¸° ì¤‘ë³µ ''ë¡œ ë³€ê²½
mask = df['contents'].isin([' '])
df = df[~mask].reset_index(drop = True) 
deleted_white = df.shape[0]
white = deleted_dup - deleted_white
print("- white space data: {:,}({}%)".format(white, round(white/deleted_dup*100, 2)))
```

    - white space data: 0(0.0%)


```python
#5ì–´ì ˆ ë¯¸ë§Œ ë¬¸ì¥ ì œê±°
df['word_bunch'] = df['contents'].apply(lambda x: len(x.split(' '))) # ì–´ì ˆ ì¹´ìš´íŒ…

cutoff = df.loc[df['word_bunch'] < 5].shape[0] 
df = df.loc[df['word_bunch'] >= 5] 
deleted_cutoff = df.shape[0]
print("- cutoff data: {:,}({}%)".format(cutoff, round(cutoff/deleted_white*100, 2)))
```

    - cutoff data: 61,130(1.72%)



```python
df.to_csv('SNS_FULL_Dataset(í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬).csv', index=False)
```

