---
layout: single
title:  "Part 02. ì˜¨ë¼ì¸ ë©”ì‹œì§€ ì‘ì„±ì í”„ë¡œíŒŒì¼ë§ ëª¨ë¸ ê°œë°œ: ì „ì²˜ë¦¬ ë° ë³€ìˆ˜ê°œë°œ"
categories: Project:í”„ë¡œíŒŒì¼ë§_ëª¨ë¸_ê°œë°œ
tag: [NLP, ë¶ˆìš©ì–´ì²˜ë¦¬, í‘œì œí™”, í† í¬ë‚˜ì´ì¦ˆ, ë³€ìˆ˜ê°œë°œ]
---
<span style="color: #808080">#NLP #ë¶ˆìš©ì–´ ì²˜ë¦¬ #í‘œì œí™” #í† í¬ë‚˜ì´ì¦ˆ #ë³€ìˆ˜ê°œë°œ #ìì—°ì–´ ê³„ëŸ‰</span>
<hr>

{: .notice--primary} 
ğŸ’¡**í”„ë¡œì íŠ¸ ë°°ê²½**<br>

ê°œì¸ì •ë³´ ë³´í˜¸ì— ëŒ€í•œ ì‚¬íšŒì  ë¶„ìœ„ê¸°ì— ë”°ë¼ êµ¬ê¸€ ì¨ë“œíŒŒí‹° ì œê³µ ì¤‘ë‹¨, ì• í”Œ ì‚¬ìš©ì ì •ë³´ ê³µê°œ ì¤‘ë‹¨ ë“± ì‚¬ìš©ì ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì´ ì–´ë ¤ì›Œì§€ê³  ìˆìŒ. ìì‚¬ í”Œë«í¼ì— ê°€ì…í•œ ì‚¬ìš©ì ì™¸ ê³ ê° ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì€ ë”ìš± ì–´ë ¤ì›€.<br><br> 
ë§¥ë½ì •ë³´ë¥¼ í™œìš©í•œ ê³ ê° ë¶„ì„ ë° íƒ€ê²ŸíŒ… ì „ëµì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìœ¼ë‚˜ ëŒ€í‘œì ìœ¼ë¡œ ê³ ê°ì´ ìƒì‚°í•˜ëŠ” ë§¥ë½ì •ë³´ì¸ ì±„íŒ…, ë¦¬ë·°, í”¼ë“œ ë“±ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” ë§¤ìš° ì œí•œì ì„. íŠ¹íˆ, ì‚¬ìš©ìì˜ ì¸êµ¬í†µê³„ì  ì •ë³´ê°€ ì œê³µë˜ëŠ” ê²½ìš°ëŠ” ë§¤ìš° ë“œë¬¼ì–´ ê³ ê° ë¶„ì„ ë° íƒ€ê²ŸíŒ…ì— í™œìš©í•  ìˆ˜ ì—†ìŒ.<br><br> 
**ì±„íŒ…, ë¦¬ë·°, í”¼ë“œ ë“±ì—ì„œ ì‚¬ìš©ìì˜ ì¸êµ¬ í†µê³„ì  ì •ë³´ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´, ê³ ê° ë¶„ì„ ë° íƒ€ê²ŸíŒ…ì— í™œìš©í•  ìˆ˜ ìˆì„ ë¿ë§Œì•„ë‹ˆë¼ ì±—ë´‡/ë©”íƒ€ë²„ìŠ¤ ì„œë¹„ìŠ¤/CRM ì„œë¹„ìŠ¤ ë“±ì„ ê³ ë„í™”ì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë¨**<br><br>
 
{: .notice--primary} 
ğŸ¯**í”„ë¡œì íŠ¸ ëª©ì **<br>

ê³ ê°ì´ ì˜¨ë¼ì¸ ìƒì—ì„œ ìƒì‚°í•˜ëŠ” ë§¥ë½ì •ë³´(ì±„íŒ…, ë¦¬ë·°, í”¼ë“œ ë“±ì˜ í…ìŠ¤íŠ¸ ì •ë³´)ì—ì„œ **ì‘ì„±ìì˜ ì¸êµ¬ í†µê³„ì  ì •ë³´(ì„±ë³„/ì—°ë ¹)ë¥¼ ì¶”ì •í•˜ëŠ” ì‘ì„±ì í”„ë¡œíŒŒì¼ë§ ëª¨ë¸ ê°œë°œ**<br><br>
 
 
## ë³€ìˆ˜ ê°œë°œ ìš”ì•½
**ë³€ìˆ˜ ê°œë°œì˜ í•„ìš”ì„±**
- ì¤„ì„ë§, ê³ ìœ  í‘œí˜„, ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œë§Œ êµ¬ì„±ëœ ë¶ˆì™„ì „ í‘œí˜„, ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ ë“±ì˜ í‘œí˜„ì€ **ê¸°ì¡´ì˜ NLP ì „ì²˜ë¦¬ ë° ì–¸ì–´ëª¨ë¸ì˜ Encoding ê³¼ì •ì—ì„œ ì œê±°ë¨**. í•˜ì§€ë§Œ, ì´ì™€ ê°™ì€ í‘œí˜„ë“¤ì´ **ë°œí™”ìì˜ ì„±ë³„/ì—°ë ¹ì— ëŒ€í•œ ì–¸ì–´ì  íŠ¹ì§•ìœ¼ë¡œ ë³´ì—¬ì§ìœ¼ë¡œ** ìƒê´€ê´€ê³„ íŒŒì•… ë° ëª¨ë¸ê°œë°œ ì‹œ ë°˜ì˜ì„ ìœ„í•œ feauture ê°œë°œ í•„ìš”
- ëª¨ë“  í‘œí˜„ì„ encodingí•˜ëŠ” ë°©ì‹ì€ ë¹„íš¨ìœ¨ì ì¼ë¿ë§Œ ì•„ë‹ˆë¼ ì‚¬ì „ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ **ì–¸ì–´ì ì˜ íŠ¹ì§•ì„ ê³„ëŸ‰í™”í•œ ë³€ìˆ˜**ê°€ ìš”êµ¬ë¨
<br>

 
### 1. í‘œí˜„ì˜ ê¸¸ì´ì™€ ì² ìì˜ ë‹¤ì–‘ì„±ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ë³µì¡ë„ ê³„ì‚°
**Complexity of Specific Expression(C, íŠ¹ìˆ˜í‘œí˜„ ë³µì¡ë„)** 
- í‘œí˜„ tê°€ ë³µì¡í•œ ì •ë„ë¥¼ ì„¤ëª…í•¨
- ì „ì²˜ë¦¬ì—ì„œ ëŒ€ë¶€ë¶„ ì†Œì‹¤ë˜ëŠ” ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ í‘œí˜„, ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ë¡œ êµ¬ì„±ëœ í‘œí˜„ì— ëŒ€í•œ ë³µì¡ë„ë¥¼ ê³„ì‚°
- ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ í‘œí˜„ì„ í•œ ë¶„ë¥˜ë¡œ í•˜ê³ , ë¬¸ì¥ê¸°í˜¸/íŠ¹ìˆ˜ê¸°í˜¸/ì´ëª¨ì§€ë¥¼ í•œ ë¶„ë¥˜ë¡œ êµ¬ë¶„í•˜ì—¬ ê³„ì‚°(í˜•íƒœì†Œ ë¶„ë¥˜ ë° ì •ê·œì‹ ì²˜ë¦¬ì— ìš©ì´)
> $C_i= ln[ 1/Î (U_i / L_i) * L_i]$<br>
> $U_i$ : í‘œí˜„ $t_i$ ê°€ í¬í•¨í•˜ëŠ” ê³ ìœ í•œ ì² ì ë˜ëŠ” ê¸°í˜¸ì˜ ì¢…ë¥˜ ìˆ˜
> $L_i$ : í‘œí˜„ $t_i$ ì˜ ê¸¸ì´(ì² ì ë° ê¸°í˜¸ ê°œìˆ˜)
> <br><br>
> í‘œí˜„ì„ ì´ë£¨ëŠ” ê³ ìœ  ì² ì ë° ê¸°í˜¸ ë³„ ì ìœ ìœ¨ $U_i / L_i$ ì„ ëª¨ë‘ ê³±í•˜ê³ , í‘œí˜„ì˜ ê¸¸ì´ë¥¼ ê³±í•˜ì—¬ ê³ ìœ  ì² ìì˜ ì¢…ë¥˜ì™€ í‘œí˜„ì˜ ê¸¸ì´ì— ë¹„ë¡€í•˜ëŠ” ë³µì¡ë„ ì¸¡ì •. ë‹¤ì–‘í•œ ì² ìê°€ ì‚¬ìš©ë  ìˆ˜ë¡, ì‚¬ìš©ëœ ê³ ìœ  ì² ì ë° ê¸°í˜¸ ìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ë³µì¡ë„ê°€ ì»¤ì§<br>
â‡’ ì „ì²˜ë¦¬ ì „ì˜ raw í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì¸¡ì •í•˜ë©°, ì „ì²˜ë¦¬ì—ì„œ ì†Œì‹¤ë˜ëŠ” íŠ¹ìˆ˜í‘œí˜„ë“¤ì˜ ì •ë³´ë¥¼ ì¼ë¶€ ë°˜ì˜í•¨

<br><br>
### 2. ê° í‘œí˜„ì˜ ì¢…ì†ë³€ìˆ˜(ì„±ë³„/ì—°ë ¹)ì— ëŒ€í•œ Odds Ratioë¥¼ ê³„ì‚°
**Relative Bias(RB, ìƒëŒ€ í¸í–¥ë„)**: í‘œí˜„ tê°€ ë“±ì¥ í–ˆì„ ë•Œ, í…ìŠ¤íŠ¸ì˜ **ì‘ì„±ìê°€ íŠ¹ì • ì„±ë³„/ì—°ë ¹ì¸ ì •ë„**ë¥¼ ì„¤ëª…í•¨
<br><br>
a. **Relative Bias of Gender**(RBG, ìƒë³„ì— ëŒ€í•œ ìƒëŒ€ í¸í–¥ë„)<br>
- í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¬¸ì¥ $s$ ì˜ ì‘ì„±ì ì„±ë³„ì´ ë‚¨ì $male$ ë˜ëŠ” ì—¬ì $female$ ì¸ ì •ë„
> $RBG_i = ln[ n(t_iâˆˆs_{male})/n(s_{male}) Ã· n(t_iâˆˆs_{female})/ n(s_{female}) ]$<br>
> $t_i$ : ë¬¸ì„œ ë‚´ ië²ˆ ì§¸ í‘œí˜„
> $s_{male}$ : ì‘ì„±ìì˜ ì„±ë³„ì´ ë‚¨ì„±(m)ì¸ ë¬¸ì¥
> $s_{female}$ : ì‘ì„±ìì˜ ì„±ë³„ì´ ì—¬ì„±(f)ì¸ ë¬¸ì¥
> <br><br>
> ì‘ì„±ìì˜ ì„±ë³„ì´ ë‚¨ì„±ì¸ ë¬¸ì¥ $s_{male}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨ Ã· ì—¬ì„±ì¸ ë¬¸ì¥ $s_{female}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨, 0~1 ì‚¬ì´ì˜ Skewedí•œ ê°’ì„ ê°€ì§ìœ¼ë¡œ logë¥¼ ì·¨í•´ ì •ê·œí™”<br>
  â‡’ $t_i$ ê°€ ë“±ì¥í–ˆì„ ë•Œ ì‘ì„±ìì˜ ì„±ë³„ì´ ë‚¨ì $m$ ë˜ëŠ” ì—¬ì $f$ ì¸ ì •ë„

<br><br>
 b. **Relative Bias of Age**(RBG, ì—°ë ¹ì— ëŒ€í•œ ìƒëŒ€ í¸í–¥ë„)<br>
  - í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¬¸ì¥ $s$ ì˜ ì‘ì„±ì ì—°ë ¹ì´ íŠ¹ì • ì—°ë ¹ëŒ€ $age$ ì¸ ì •ë„ë¥¼ ì„¤ëª…í•¨<br>
  - ì—°ë ¹ëŒ€ëŠ” 20ëŒ€ ë¯¸ë§Œ/20ëŒ€/30ëŒ€/40ëŒ€/50ëŒ€ ì´ìƒ 5 classë¡œ ë¶„ë¥˜
> $RBAi = ln[ n(t_iâˆˆs_{age})/n(s_{age}) Ã· n(t_iâˆˆs_{other})/n(s_{other}) ]$<br>
> $t_i$ : ë¬¸ì„œ ë‚´ ië²ˆ ì§¸ í‘œí˜„
> $s_{age}$ : ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ ageì¸ ë¬¸ì¥
> $s_{other}$ : ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ ageê°€ ì•„ë‹Œ ë¬¸ì¥
> <br><br>
> ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ 20ëŒ€ì¸ ë¬¸ì¥ $s_{a20}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨ Ã· ì—°ë ¹ëŒ€ê°€ 20ëŒ€ê°€ ì•„ë‹Œ ë¬¸ì¥ $s_{other}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨, 0~1 ì‚¬ì´ì˜ Skewedí•œ ê°’ì„ ê°€ì§ìœ¼ë¡œ logë¥¼ ì·¨í•´ ì •ê·œí™”<br>
â‡’ $t_i$ê°€ ë“±ì¥í–ˆì„ ë•Œ ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ ageì¸ ì •ë„<br>
<br>

 
**Relative frequency (RF, ìƒëŒ€ ë¹ˆì¶œë„)**
- í…ìŠ¤íŠ¸ì˜ ì‘ì„±ìê°€ íŠ¹ì • ì„±ë³„/ì—°ë ¹ì¼ ë•Œ íƒ€ ì„±ë³„/ì—°ë ¹ ë³´ë‹¤ **í‘œí˜„ të¥¼ ìƒëŒ€ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì •ë„**ë¥¼ ì„¤ëª…í•¨
- ìƒëŒ€ í¸í˜•ë„ëŠ” ë¬¸ì¥ì—ì„œ í‘œí˜„ì˜ ì¶œì—°ì—¬ë¶€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ë°˜ë©´, ìƒëŒ€ ë¹ˆì¶œë„ëŠ” ë¹ˆë„ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨<br>
- ìƒëŒ€ í¸í–¥ë„ì™€ ê³„ì‚° ë°©ì‹ì´ ìœ ì‚¬í•˜ë¯€ë¡œ ì„¤ëª…ì€ ìƒëµí•¨
>  $RFG_i = ln[ n(t_i | s_{male}) / n(s_{male}) Ã· n(t_i | s_{female}) / n(s_{female}) ]$<br>
>  $RFAi = ln[ n(t_i | s_{age}) / n(s_{age}) Ã· n(t_i | s_{other}) / n(s_{other}) ]$


<br><br>
- ë¬¸ì¥ì´ í¬í•¨í•˜ëŠ” í† í° ì „ì²´ì— ëŒ€í•œ ìƒëŒ€ í¸í–¥ë„ ë° ìƒëŒ€ ë¹ˆì¶œë„ì˜ í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•¨ìœ¼ë¡œì¨ ê¸°ì¡´ì˜ NLP ë°©ì‹ì—ì„œ ì†Œì‹¤ë˜ëŠ” í† í° ì •ë³´ì˜ ì¼ë¶€ë¶„ ë³´ì™„í•  ìˆ˜ ìˆìŒ
- í‘œí˜„ë³„ ìƒëŒ€ í¸í–¥ë„ ë° ìƒëŒ€ í¸í–¥ë„ì— ëŒ€í•œ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•ì´ í•„ìš”
- Odds ratioì˜ ê°’ì„ ì¼ë°˜í™”í•  ìˆ˜ ìˆì„ ë§Œí¼ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì „ì œë˜ì–´ì•¼ í•˜ë©°, ë³¸ ë°ì´í„°ì…‹ì€ ê·¸ì— ì¤€í•œë‹¤ê³  ê°€ì •í•¨
- ìƒëŒ€ í¸í–¥ë„ì™€ ìƒëŒ€ ë¹ˆì¶œë„ê°€ ë§¤ìš° ìœ ì‚¬í•¨ìœ¼ë¡œ ìœ ì˜ì„± ê²€í† ë¥¼ í†µí•´ 1ê°€ì§€ë§Œ ì±„íƒ
<br>

<br><br>
## 01. íŠ¹ìˆ˜í‘œí˜„ ë³µì¡ë„ ì¸¡ì •
- ë¶ˆìš©ì–´ ì²˜ë¦¬/í‘œì œí™” ì „ì— ìˆ˜í–‰
- ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë¶ˆì™„ì „ í‘œí˜„ / ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ í‘œí˜„ì„ êµ¬ë¶„í•˜ì—¬ ì¸¡ì •
- ë¬¸ì¥ì—ì„œ ì¶œí˜„í•˜ëŠ” íŠ¹ìˆ˜í‘œí˜„ì˜ ê°œìˆ˜, í‰ê· , í‘œì¤€í¸ì°¨, ìµœëŒ€ê°’, ìµœì†Œê°’ì„ ê³„ì‚°




```python
import pandas as pd
from tqdm import tqdm
tqdm.pandas()

import re
import numpy as np
from collections import Counter

df = pd.read_csv("ì¹´í†¡ëŒ€í™”_Dataset(raw_ì¤‘ë³µëœ ë©”ì„¸ì§€ ì œê±°).csv")
```



<br><br>
### ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë¶ˆì™„ì „ í‘œí˜„ì˜ ë³µì¡ë„


```python
def spell_complexity(sentence):
    sequences = re.findall(r'[ã„±-ã…ã…-ã…£]+', sentence)
    specific_sequences = [seq for seq in sequences if len(seq) > 0]
    
    N = len(specific_sequences)
    
    if N != 0:
        complexity_list=[]
        for seq in specific_sequences:
            spell_list = []
            for s in seq:
                spell_list.append(s)  
```




```python
print(spell_complexity('ì•ˆë…• ã… ã…  ã… ã… ã…  ã… ã…œ ã…‡ã…‹ã…‡ã…‹ ã„·ã…‹-ã…ã…- ã… '))
```

    (7, 1.3451969221982076, 0.9131088298012547, 2.772588722239781, 0.0)




```python
print(spell_complexity('ì•ˆ ã… ã…  ã…‹ã…‹ã…‹'))
```

    (2, 0.8958797346140275, 0.20273255405408225, 1.0986122886681098, 0.6931471805599453)



<br><br> 
### ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ í‘œí˜„ì— ëŒ€í•œ ë³µì¡ë„




```python
def symbol_complexity(sentence):
    sequences = re.findall(r'[^ã„±-ã…ã…-ã…£ê°€-í£a-zA-Z0-9\s]+', sentence)
    specific_sequences = [seq.strip() for seq in sequences if seq.strip()]
    
    N = len(specific_sequences)
    
    if N != 0:
        complexity_list=[]
        for seq in specific_sequences:
            spell_list = []
            for s in seq:
                spell_list.append(s)  

            uniqe_list = Counter(spell_list)
            uniqe_freq = list(uniqe_list.values())

            L = len(spell_list)
            uniqe_num =len(uniqe_freq)


            ratio_list = []
            for fu in uniqe_freq:
                ratio = (fu/L)
                ratio_list.append(ratio)

            ans = 1
            for r in ratio_list:
                ans *= r

            complexity = np.log(1/ans*L)
            complexity_list.append(complexity)
            
        mean = np.mean(complexity_list)
        std =  np.std(complexity_list)
        max_ = np.max(complexity_list)
        min_ = np.min(complexity_list)
    else:
        mean = 0
        std = 0
        max_ = 0
        min_ = 0
        
    return N, mean, std, max_, min_
```


```python
print(symbol_complexity('ã… ã…  ì•ˆë…•! ??... ì˜ˆì‹œ: 2^^; -_-;; o_O ğŸ˜˜ ğŸ™ â˜ºï¸ â¤ï¸ğŸ§¡ğŸ’›'))
```

    (10, 1.970161458941443, 2.3466123014783795, 6.931471805599453, 0.0)



```python
print(symbol_complexity('ã… ã…  ì•ˆë…•ğŸ˜˜ğŸ˜˜ğŸ˜˜ ğŸ˜˜ğŸ˜˜'))
```

    (2, 0.8958797346140275, 0.20273255405408225, 1.0986122886681098, 0.6931471805599453)



```python
df['spell_num'],df['spell_mean'], df['spell_std'], df['spell_max'], df['spell_min'] = zip(*df['contents'].progress_apply(lambda x: spell_complexity(str(x))))
df['symbol_num'], df['symbol_mean'],  df['symbol_std'], df['symbol_max'], df['symbol_min'] = zip(*df['contents'].progress_apply(lambda x: symbol_complexity(str(x))))

df = df.drop(columns=['topic', 'resident'])
df = df.fillna(0)
df
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3563533/3563533 [04:53<00:00, 12152.45it/s]
    ...ìƒëµ...





<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>topic</th>
      <th>sex</th>
      <th>age</th>
      <th>resident</th>
      <th>contents</th>
      <th>length</th>
      <th>spell_num</th>
      <th>spell_mean</th>
      <th>spell_std</th>
      <th>spell_max</th>
      <th>spell_min</th>
      <th>symbol_num</th>
      <th>symbol_mean</th>
      <th>symbol_std</th>
      <th>symbol_max</th>
      <th>symbol_min</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>ë‚˜ì§€ê¸ˆë°¥ë¨¸ê±°2ì‹œê°„ê±¸ì–´ì„œ ë²ˆí™”ê°€ì°¾ì•˜ì–´..ã…œã…œ ì‰ã…œã…œ ã…ã…ã…ã…ì˜¤ì¢‹ê² ë„¤ ã…‹ã„±ã…‹ã„±ã„±ã„±ã„±ì•„ë‹ˆ...</td>
      <td>127</td>
      <td>6</td>
      <td>2.185021</td>
      <td>1.299761</td>
      <td>3.765840</td>
      <td>0.693147</td>
      <td>4</td>
      <td>0.173287</td>
      <td>0.300142</td>
      <td>0.693147</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>í— ã… ã…  ì–¸ë„í˜¸í…”ë“¤ê°€ã… ã…  ì—„ì²­í”¼ê±´í• ì²¸ë° ë‚˜ëŠ”ì¸ë‚«ëŸ¬ìš” ë‚˜ ë‘ì‹œì¶œê·¼ì´ë‹¤ã…ã…ã…ã… í€µìœ¼ë¡œí•œ...</td>
      <td>130</td>
      <td>6</td>
      <td>0.961387</td>
      <td>0.555163</td>
      <td>1.609438</td>
      <td>0.000000</td>
      <td>6</td>
      <td>2.041180</td>
      <td>1.296771</td>
      <td>4.212128</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>í•™ìƒì´ë©´ì¢‹êµ¬! ì™œí˜¼ìë‹¤ë‹ˆëƒê³ ì˜¤..... ì™€ ë‚´ì¹œêµ°í•™êµë‚˜ê° ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ ê·¸ë¥´ë„¤ ...</td>
      <td>56</td>
      <td>1</td>
      <td>2.197225</td>
      <td>0.000000</td>
      <td>2.197225</td>
      <td>2.197225</td>
      <td>3</td>
      <td>1.404043</td>
      <td>1.072424</td>
      <td>2.602690</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>í›” í•™ìƒ ì—†ëŠ”ë°...ì£¼ë³€ì—... ì•„ë‹ˆ ë³µí•™í•˜ê³  í•™êµë¥¼ ëª»ê°€ëŠ”ë° ì–´ì¼€ ì¹œêµ¬ê°€ìˆëƒ.. ...</td>
      <td>74</td>
      <td>1</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>2.079442</td>
      <td>4</td>
      <td>0.997246</td>
      <td>0.175572</td>
      <td>1.098612</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ì—¬ì„±</td>
      <td>30ëŒ€</td>
      <td>ì¶©ì²­ë¶ë„</td>
      <td>ì°¸ë‚˜ ë‚´ê°€ë­ì–¼ë§ˆë‚˜ê·¸ë¬ë‹¤ê³  ì›ƒê¸°ëŠ”ì‚¬ëŒì´ì•¼ì§€ì§œ ë„ˆë¬´í™”ë‚œë‹¹.. ê·¼ë°ì˜¤ë¹ ëŠ”ë§ì„ë˜ ì˜í•´ì„œ ë‚´...</td>
      <td>146</td>
      <td>2</td>
      <td>0.693147</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>1</td>
      <td>0.693147</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>
<p>3564042 rows Ã— 16 columns</p>

</div>




```python
df.to_csv('ì¹´í†¡ëŒ€í™”_Dataset(raw_í‘œí˜„ ë³µì¡ë„ ê³„ì‚°).csv', index=False)
```



<br><br>
## 02. ìì—°ì–´ ì „ì²˜ë¦¬
- 3ë²ˆ ì´ìƒ ë°˜ë³µë˜ëŠ” ë™ì¼ ìŒì ˆì€ 3ìŒì ˆë¡œ í‘œì œí™” *ex. ã…‡ã…‡ã…‡ã…‡ã…‡ â‡’ ã…‡ã…‡ã…‡
- ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€, íŠ¹ìˆ˜í°íŠ¸ í‘œí˜„ í‘œì œí™” *ex. â¤ğŸ§¡ğŸ’› â‡’ â™¥ï¸â™¥ï¸â™¥ï¸ / ãƒ²ğ¨›ğŒ…â«¬ â‡’ ã…‹ã…‹ã…‹ã…‹
- ì—°ì†ëœ ë„ì–´ì“°ê¸°(ê³µë°±) í•œ ë²ˆìœ¼ë¡œ ì²˜ë¦¬
- 5ì–´ì ˆ ë¯¸ë§Œ ë§ë­‰ì¹˜ ë°ì´í„° ì œê±°


  
```python
processed_df = df[['sex', 'age', 'contents', 'length']].copy()
```



```python
import re

raw = processed_df.shape[0]
print('- raw data: {:,}'.format(raw))

#ê²°ì¸¡ê°’ ì œê±°
print('- null data: {:,}'.format(processed_df['contents'].isnull().sum()))
processed_df = processed_df.dropna() 
deleted_null = processed_df.shape[0]

#ì¤‘ë³µ ì œê±°
processed_df = processed_df.drop_duplicates() 
deleted_dup = processed_df.shape[0]
print('- duplicated data: {:,}'.format(deleted_null - deleted_dup))
```

    - raw data: 3,564,042
    - null data: 0
    - duplicated data: 0


<br><br>
### ìëª¨ í‘œí˜„, ìˆ«ì, ê¸°í˜¸ê°€ ê²°í•©ëœ í‘œí˜„ ìœ í˜•ë³„ ë¶„ë¦¬(ë„ì–´ì“°ê¸°)

```python
def split_re_types(sentence):
    KoreanParticle_pattern = r'[ã„±-ã…ã…-ã…£]+'
    Symbol_pattern = r'[^ã„±-ã…ã…-ã…£ê°€-í£a-zA-Z0-9\s]+'
    Number_pattern = r'(\d+)'
    
    sentence = re.sub(KoreanParticle_pattern, r' \g<0> ', sentence)
    sentence = re.sub(Symbol_pattern, r' \g<0> ', sentence)
    sentence = re.sub(Number_pattern, r' \g<0> ', sentence)
    
    return sentence
```
    
<br><br>
### ì£¼ìš” ì´ëª¨ì§€ ë° íŠ¹ìˆ˜í‘œí˜„ í‘œì œí™”


```python
def specific_lemmatization(sentence):
    heart_emoji = ['â™¡', 'â™¥', 'â¤', 'â¤ï¸', 'ğŸ§¡', 'ğŸ’›', 'ğŸ’š', 'ğŸ’™', 'ğŸ’œ', 'ğŸ–¤', 'ğŸ’•', 'â£ï¸'] #
    star_emoji = ['â˜†', 'â˜…', 'â­', 'ğŸŒŸ']
    kkk = ['ğ¨›', 'ğŒ…', 'â«¬', 'ãƒ²', 'åˆ', 'ã‰ª', 'ï½¦', 'ã…‹ê™¼Ìˆ', 'ã…‹Ì‘Ìˆ', 'ã…‹Ì†Ì', 'ã…‹ÌÌˆ', 'ã…‹ÌŠÌˆ', 'ã…‹Ì„Ìˆ', 'ã…‹Ì†Ìˆ', 'ã…‹ÌŠÌˆ', 'ã…‹ÌÌˆ', 'ã…‹Ì†Ì']
    Period = ['ã†', 'á†', 'ã†', 'â€¢', 'á†¢']
    quote = ['â€', 'â€˜', 'â€œ']
    ect = ['Â ', 'ã…¤']
    
    for h in heart_emoji:
        sentence = sentence.replace(h, 'â™¥')
    for s in star_emoji:
        sentence = sentence.replace(s, 'â˜…')
    for k in kkk:
        sentence = sentence.replace(k, 'ã…‹')
    for p in Period:
        sentence = sentence.replace(p, 'Â·')
    for q in quote:
        sentence = sentence.replace(q, '\'')
    for e in ect:
        sentence = sentence.replace(e, ' ')   
    return sentence

text_sentence = 'â¤ğŸ§¡ğŸ’›í…ŒìŠ¤íŠ¸ğŸ’šğŸ’™ğŸ’œâ˜†ì…ë‹ˆë‹¤â˜…á†¢â­ ãƒ²ğ¨›ğŒ…â«¬ã…‹Ì„Ìˆã…‹ê™¼Ìˆã…‹Ì†Ìã…‹ÌÌˆã…‹ÌŠÌˆã…‹Ì„Ìˆã…‹ê™¼Ìˆã…‹Ì†Ìã…‹ÌÌˆã…‹ÌŠÌˆ'
text_sentence = specific_lemmatization(text_sentence)
text_sentence
```




    'â™¥â™¥â™¥í…ŒìŠ¤íŠ¸â™¥â™¥â™¥â˜…ì…ë‹ˆë‹¤â˜….â˜… ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹'




    
<br><br>
### ë°˜ë³µë˜ëŠ” ë™ì¼ ì² ì í‘œì œí™” ì²˜ë¦¬


```python
def duplicated_character_reduction(sentence):
    words = sentence.split()

    for word in words:
        pattern = re.compile(r'([^0-9])\1{4,}')
        re_word = pattern.sub(r'\1' * 5, word)
        sentence = sentence.replace(word, re_word)
    return sentence

text_sentence = '      ^^^^^^^ !!!!!!! ì•ˆë…•ì•ˆë…• í—¤í—¤í—¤í—¤í—¤í—¤í—¤í—¤ ã…‹ã…‹ã…‹ã…‹ ã…ã…ã…ã…ã…ã…ã…ã… ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹'
duplicated_character_reduction(text_sentence)
```




    '      ^^^^^ !!!!! ì•ˆë…•ì•ˆë…• í—¤í—¤í—¤í—¤í—¤ ã…‹ã…‹ã…‹ã…‹ ã…ã…ã…ã…ã… ã…‹ã…‹ã…‹ã…‹ã…‹'




```python
processed_df['contents'] = processed_df['contents'].progress_apply(split_re_types)
processed_df['contents'] = processed_df['contents'].progress_apply(specific_lemmatization)
processed_df['contents'] = processed_df['contents'].progress_apply(duplicated_character_reduction)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3563533/3563533 [02:02<00:00, 29125.25it/s]
    ...ìƒëµ...
    
<br><br>
### ì—°ì†ëœ ë„ì–´ì“°ê¸° ì²˜ë¦¬/5ì–´ì ˆ ë¯¸ë§Œ ë¬¸ì¥ ì œê±°


```python
#ì—°ì†ë˜ ë„ì–´ì“°ê¸° ì²˜ë¦¬
processed_df['contents'] = processed_df['contents'].apply(lambda x : re.sub(r'\s+', ' ', x))  #ì—°ì†ëœ ë„ì–´ì“°ê¸° ' 'ë¡œ ë³€ê²½
mask = processed_df['contents'].isin([' '])
processed_df = processed_df[~mask].reset_index(drop = True) 
deleted_white = processed_df.shape[0]
white = deleted_dup - deleted_white
print("- white space data: {:,}({}%)".format(white, round(white/deleted_dup*100, 2)))

#ì–´ì ˆ ì¹´ìš´íŒ…
processed_df['word_bunch'] = processed_df['contents'].apply(lambda x: len(x.split(' ')))


#5ì–´ì ˆ ë¯¸ë§Œ ë¬¸ì¥ ì œê±°
cutoff = processed_df.loc[processed_df['word_bunch'] < 5].shape[0] 
processed_df = processed_df.loc[processed_df['word_bunch'] >= 5] 
deleted_cutoff = processed_df.shape[0]
print("- cutoff data: {:,}({}%)".format(cutoff, round(cutoff/deleted_white*100, 2)))

print("- total processed data: {:,}".format(deleted_cutoff))
```

    - white space data: 0(0.0%)
    - cutoff data: 9,073(0.25%)
    - total processed data: 3,554,460


```python
processed_df.to_csv('ì¹´í†¡ëŒ€í™”_Dataset(ì „ì²˜ë¦¬).csv', index=False)
```


<br><br>
### í† í¬ë‚˜ì´ì¦ˆ
- konlpy ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Oktë¥¼ ì‚¬ìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ë¦¬
- ë„ì–´ì“°ê¸°ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ '_(_)'ë¡œ ëŒ€ì‹ 
- ë™ì¼ ì² ìì´ë‚˜ ë‹¤ë¥¸ í˜•íƒœì†Œë¥¼ ê°–ëŠ” í† í°ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ 'í† í°(í˜•íƒœì†Œ)' í˜•ì‹ìœ¼ë¡œ í† í¬ë‚˜ì´ì¦ˆ

```python
import konlpy

#ë©”ëª¨ë¦¬ì•„ì›ƒ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì¼ì • í¬ê¸° ì´í•˜ë¡œ Split
def split_dataframe(dataframe, size=1000):
    total_length = len(dataframe)
    splited_li = []  # splited_liëŠ” ë°˜ë“œì‹œ ì´ˆê¸°í™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

    if len(dataframe) > size:
        split_size = size
        num_split = total_length // split_size + 1

        for i in range(num_split):
            start_idx = i * split_size
            end_idx = (i + 1) * split_size
            try:
                splited = dataframe[start_idx:end_idx]
            except:
                splited = dataframe[start_idx:]

            # ì‘ì€ ê·¸ë£¹ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            splited_li.append(splited)
    else:
        splited_li.append(dataframe)
    return splited_li


def pos_tokenizer(sentence):    #POS ê¸°ì¤€ í† í¬ë‚˜ì´ì œì´ì…˜
    tokens = sentence.split()
    with_space = ' SP '.join(tokens)
    
    token_li = []
    okt = konlpy.tag.Okt() 
    pos_list = okt.pos(str(with_space)) #í† í°/í˜•íƒœì†Œ íŠœí”Œì²˜ë¦¬
    for t, m in pos_list:
        token = '{}({})'.format(t, m)
        token_li.append(token)
    
    token_count = len(token_li)
    tokenized_sentence = ' '.join(token_li)
    tokenized_sentence = tokenized_sentence.replace('SP(Alpha)', '_(_)')
    return tokenized_sentence, token_count
```




```python
import gc

df_li = split_dataframe(df, size=1000) # ë°ì´í„°ê°€ ì»¤ì„œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬
tokenized_df = pd.DataFrame()
i = 0
for d in tqdm(df_li, total=len(df_li), desc='í† í¬ë‚˜ì´ì§•'):
    data = d.copy()
    data['tokenized'], data['token_count'] = zip(*data['contents'].apply(lambda x: pos_tokenizer(x)))
    tokenized_df = pd.concat([tokenized_df, data], ignore_index=True)
    
    i += 1
    if i%500 == 0:
        gc.collect()
    
tokenized_df
```

    í† í¬ë‚˜ì´ì§•: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3555/3555 [3:20:39<00:00,  3.44s/it]





<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>age</th>
      <th>contents</th>
      <th>length</th>
      <th>word_bunch</th>
      <th>tokenized</th>
      <th>token_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>ë‚˜ì§€ê¸ˆë°¥ë¨¸ê±°2ì‹œê°„ê±¸ì–´ì„œ ë²ˆí™”ê°€ì°¾ì•˜ì–´..ã…œã…œ ì‰ã…œã…œ ã…ã…ã…ã…ì˜¤ì¢‹ê² ë„¤ ã…‹ã„±ã…‹ã„±ã„±ã„±ã„±ì•„ë‹ˆ...</td>
      <td>127</td>
      <td>16</td>
      <td>ë‚˜(Noun) ì§€ê¸ˆ(Noun) ë°¥(Noun) ë¨¸ê±°(Verb) 2ì‹œê°„(Number) ...</td>
      <td>54</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>í— ã… ã…  ì–¸ë„í˜¸í…”ë“¤ê°€ã… ã…  ì—„ì²­í”¼ê±´í• ì²¸ë° ë‚˜ëŠ”ì¸ë‚«ëŸ¬ìš” ë‚˜ ë‘ì‹œì¶œê·¼ì´ë‹¤ã…ã…ã…ã… í€µìœ¼ë¡œí•œ...</td>
      <td>130</td>
      <td>18</td>
      <td>í—(Verb) ã… ã… (KoreanParticle) ì–¸(Modifier) ë„(Noun)...</td>
      <td>49</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>í•™ìƒì´ë©´ì¢‹êµ¬! ì™œí˜¼ìë‹¤ë‹ˆëƒê³ ì˜¤..... ì™€ ë‚´ì¹œêµ°í•™êµë‚˜ê° ã…‹ã…‹ã…‹ã…‹ã…‹ ê·¸ë¥´ë„¤ ë§‰ì¡¸ì—…í•œ...</td>
      <td>56</td>
      <td>7</td>
      <td>í•™ìƒ(Noun) ì´(Suffix) ë©´(Josa) ì¢‹êµ¬(Adjective) !(Pun...</td>
      <td>25</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>í›” í•™ìƒ ì—†ëŠ”ë°...ì£¼ë³€ì—... ì•„ë‹ˆ ë³µí•™í•˜ê³  í•™êµë¥¼ ëª»ê°€ëŠ”ë° ì–´ì¼€ ì¹œêµ¬ê°€ìˆëƒ.. ...</td>
      <td>74</td>
      <td>15</td>
      <td>í›”(Noun) í•™ìƒ(Noun) ì—†ëŠ”ë°(Adjective) ...(Punctuatio...</td>
      <td>31</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ì—¬ì„±</td>
      <td>30ëŒ€</td>
      <td>ì°¸ë‚˜ ë‚´ê°€ë­ì–¼ë§ˆë‚˜ê·¸ë¬ë‹¤ê³  ì›ƒê¸°ëŠ”ì‚¬ëŒì´ì•¼ì§€ì§œ ë„ˆë¬´í™”ë‚œë‹¹.. ê·¼ë°ì˜¤ë¹ ëŠ”ë§ì„ë˜ ì˜í•´ì„œ ë‚´...</td>
      <td>146</td>
      <td>19</td>
      <td>ì°¸ë‚˜(Noun) ë‚´(Noun) ê°€(Josa) ë­(Noun) ì–¼ë§ˆë‚˜(Noun) ê·¸ë¬ë‹¤...</td>
      <td>63</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>
<p>3554460 rows Ã— 7 columns</p>




```python
tokenized_df.to_csv('ì¹´í†¡ëŒ€í™”_Tokenized(pos ë¹„êµì •).csv', index = False)
```


<br><br>
### í† í° ë”•ì…”ë„ˆë¦¬ êµ¬ì¶• ë° ìŠ¤í¬ë¦¬ë‹
**ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•**
- ë¹ˆë„ 0.0001 ì´ìƒì˜ í† í°ë§Œ ì‚¬ìš©<br>
[í† í° ë”•ì…”ë„ˆë¦¬(êµ¬ê¸€ ìŠ¤í”„ë ˆë“œ)](https://docs.google.com/spreadsheets/d/1wKv4hAfJD_ToORv1Q7QGWsCjYUQTZKHRzZMNCxVPvZ4/edit?usp=sharing)<br>
https://docs.google.com/spreadsheets/d/1wKv4hAfJD_ToORv1Q7QGWsCjYUQTZKHRzZMNCxVPvZ4/edit?usp=sharing

**ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼**
- ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ í† í¬ë‚˜ì´ì¦ˆëœ ë”•ì…”ë„ˆë¦¬, í˜•íƒœì†Œ ê¸°ì¤€ìœ¼ë¡œ í† í¬ë‚˜ì´ì¦ˆëœ ë”•ì…”ë„ˆë¦¬, ì› ë¬¸ì¥, í† í¬ë‚˜ì´ì¦ˆ ë¬¸ì¥ì„ ë¹„êµ

  
- Okt ë”•ì…”ë„ˆë¦¬ì— í¬í•¨ë˜ì§€ ì•Šì€ í‘œí˜„ì´ íŒŒê´´ë¨<br>
  â‡’ ë‘ë“œëŸ¬ì§€ëŠ” í‘œí˜„ êµì •(ë¶„ë¦¬ëœ í† í° ê²°í•©)

       
- ë¶„ë¥˜ê°€ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶€ì •í™•í•œ í˜•íƒœì†Œ íŒŒì•…
  - ì˜ë¯¸ì ìœ¼ë¡œ í•´ì„ì´ ì–´ë ¤ìš´ Eomi, VerbPrefix, Suffix<br>
    â‡’ ì „/í›„ í† í°ê³¼ ë³‘í•© ë° í˜•íƒœì†Œ ì¬ë¶„ë¥˜ 
  - Modifier,Determiner ë‘ í˜•íƒœì†Œì˜ ê°œë…ì  êµ¬ë¶„ì´ ëª…í™•í•˜ì§€ ì•ŠìŒ<br>
    â‡’ ë‘ í˜•íƒœì†Œ ë³‘í•© 
  - Foreign ë¶„ë¥˜ê°€ ë¶€ì •í™•(íŠ¹ìˆ˜ê¸°í˜¸ ë° ê¸°íƒ€ í‘œí˜„ í¬í•¨)<br>
    â‡’ í˜•íƒœì†Œë³„ ì¬ë¶„ë¥˜ 




```python
from collections import Counter

# ë„ì–´ì“°ê¸° ê¸°ì¤€ì˜ í† í¬ë‚˜ì´ì €
def space_token_countor(df):
    total_length = len(df)
    splited_li = split_dataframe(df, size=1000)

    space_dict = {}
    for splited_df in tqdm(splited_li, total=len(splited_li), desc="ì–´ì ˆ ì‚¬ì „ ìƒì„±"): 
        merge_list = []
        for index, row in splited_df.iterrows():
            sentence = row['contents'].split()
            for token in sentence:
                merge_list.append(token)

        unique_dic = Counter(merge_list)
        for key, value in unique_dic.items():
            if key in space_dict:
                space_dict[key] += value
            else:
                space_dict[key] = value

    token_li = space_dict.keys()
    freq_li = space_dict.values()

    space_dic = pd.DataFrame({'Token': token_li, 'Token_freq': freq_li})
    space_dic = space_dic.sort_values(by=['Token_freq'], ascending=False).reset_index(drop=True) 
    space_dic['Total_ratio'] = space_dic['Token_freq'].apply(lambda x: x/total_length)
    space_dic = space_dic.loc[space_dic['Token_freq'] >= 300]
    return space_dic


# í˜•íƒœì†Œ ê¸°ì¤€ì˜ í† í¬ë‚˜ì´ì €
def pos_token_countor(df):
    total_length = len(df)
    splited_li = split_dataframe(df, size=1000)

    occur_dict = {}
    for splited_df in tqdm(splited_li, total=len(splited_li), desc="í˜•íƒœì†Œ ì‚¬ì „ ìƒì„±"): 
        merge_list = []
        for index, row in splited_df.iterrows():
            token_list = row['tokenized'].split()
            for token in token_list:
                merge_list.append(token)

        unique_dic = Counter(merge_list)
        
        for key, value in unique_dic.items():
            if key in occur_dict:
                occur_dict[key] += value
            else:
                occur_dict[key] = value

    token_li = occur_dict.keys()
    freq_li = occur_dict.values()

    occur_dic = pd.DataFrame({'Token': token_li, 'Token_freq': freq_li})
    occur_dic = occur_dic.sort_values(by=['Token_freq'], ascending=False).reset_index(drop=True) 
    occur_dic['Total_ratio'] = occur_dic['Token_freq'].apply(lambda x: x/total_length)
    occur_dic = occur_dic.loc[occur_dic['Total_ratio'] >= 0.0001]
    return occur_dic

def word_pos_split(token):
    index_li = []
    for i, char in enumerate(token):
        if char == '(':
            index_li.append(i)
    return token[:index_li[-1]], token[index_li[-1]+1:-1]
```

```python
space_dic = space_token_countor(df)
space_dic
```


```python
pos_dic = pos_token_countor(tokenized_df)
pos_dic['word'], pos_dic['pos'] = zip(*pos_dic['Token'].apply(lambda x: word_pos_split(x)))
pos_dic = pos_dic[['Token', 'word', 'pos', 'Token_freq', 'Total_ratio']]
pos_dic
```

    ë”•ì…”ë„ˆë¦¬ êµ¬ì„±:  100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3503/3503 [03:24<00:00, 17136.46it/s]

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Token</th>
      <th>word</th>
      <th>pos</th>
      <th>Token_freq</th>
      <th>Total_ratio</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>?(Punctuation)</td>
      <td>?</td>
      <td>Punctuation</td>
      <td>1865214</td>
      <td>0.532477</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ã…‹ã…‹ã…‹ã…‹ã…‹(KoreanParticle)</td>
      <td>ã…‹ã…‹ã…‹ã…‹ã…‹</td>
      <td>KoreanParticle</td>
      <td>1454297</td>
      <td>0.415169</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ì—(Josa)</td>
      <td>ì—</td>
      <td>Josa</td>
      <td>1307936</td>
      <td>0.373386</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ê°€(Josa)</td>
      <td>ê°€</td>
      <td>Josa</td>
      <td>1142959</td>
      <td>0.326289</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ì´(Josa)</td>
      <td>ì´</td>
      <td>Josa</td>
      <td>913782</td>
      <td>0.260864</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>
<p>18194 rows Ã— 5 columns</p>



```python
pos_dic.to_csv('ì¹´í†¡ëŒ€í™”_pos_dic.csv', index=False)
```




<br><br>
### ì£¼ìš” ì˜¤ë¶„ë¥˜  í˜•íƒœì†Œ ì¬ë¶„ë¥˜


```python
# ì˜¤ë¶„ë¥˜ë˜ëŠ” ì£¼ìš” í† í° í˜•íƒœì†Œ ì¬ë¶„ë¥˜
def fix_foreign(tokenized):
    missing_Adverb = ['í›„ì—(Foreign)', 'í›„(Foreign)', 'ì´ˆì—(Foreign)', 'ì¯¤ì—(Foreign)',
                      'ì¯¤(Foreign)', 'ì •ë„ì—(Foreign)', 'ì •ë„(Foreign)', 'ì „ì—ëŠ”(Foreign)',
                      'ì „ì—(Foreign)', 'ì „(Foreign)', 'ì´ìƒ(Foreign)', 'ì•ˆì—(Foreign)',
                      'ë¶€í„°ëŠ”(Foreign)', 'ë¶€í„°(Foreign)', 'ë°˜ì¯¤(Foreign)', 'ë°˜ì—(Foreign)',
                      'ë°˜ë¶€í„°(Foreign)', 'ë°˜ê¹Œì§€(Foreign)', 'ë°˜(Foreign)', 'ë°–ì—(Foreign)',
                      'ë§ì—(Foreign)', 'ë§(Foreign)', 'ë§Œì—(Foreign)', 'ë§Œ(Foreign)',
                      'ë§ˆë‹¤(Foreign)', 'ë’¤ì—(Foreign)', 'ë’¤(Foreign)', 'ë™ì•ˆ(Foreign)',
                    
                      'ë„˜ì–´ì„œ(Foreign)', 'ë„˜ê²Œ(Foreign)', 'ë‚¨ìŒ(Foreign)', 'êº¼(Foreign)',
                      'ê¹Œì§„ë°(Foreign)', 'ê¹Œì§„(Foreign)', 'ê¹Œì§€ì•¼(Foreign)', 'ê¹Œì§€ë§Œ(Foreign)',
                      'ê¹Œì§€ëŠ”(Foreign)', 'ê¹Œì§€(Foreign)', 'ê°„(Foreign)', 'ì´í›„(Foreign)',
                      'ì´í›„ì—(Foreign)', 'ë‚´ë¡œ(Foreign)', 'ê²½(Foreign)', 'ë§ê¹Œì§€(Foreign)',
                      'ì „ê¹Œì§€(Foreign)', 'ì¤‘ì—(Foreign)', 'ì¦˜(Foreign)', 'ë‚´ë‚´(Foreign)',
                      'ì •ë„ëŠ”(Foreign)', 'ì´ˆ(Foreign)', 'ì–¼ë§ˆ(Foreign)', 'ì •ë„ë©´(Foreign)',
                      'ì´ë‚´(Foreign)', 'ë‚´(Foreign)', 'ê°„ì˜(Foreign)', 'ê°„ì€(Foreign)',
                      'ì•½(Foreign)', 'ë³´ë‹¤(Foreign)', 'ì „ì—”(Foreign)', 'ê¹Œì§€ë‹ˆê¹Œ(Foreign)',
                      'ì •ë„ë§Œ(Foreign)', 'ì‚¬ì´ì—(Foreign)', 'ë’¤ë©´(Foreign)', 'ì‹(Foreign)'
                     ]

    
    missing_Josa = ['ì´ë©´(Foreign)', 'ì´ë‘(Foreign)', 'ì´ë¼ì„œ(Foreign)', 'ì´ë¼ë„(Foreign)',
                    'ì´ë¼ê³ (Foreign)', 'ì´ë¼(Foreign)', 'ì´ë‚˜(Foreign)', 'ì´ê³ (Foreign)',
                    'ì´(Foreign)', 'ì˜(Foreign)', 'ì„(Foreign)', 'ì€(Foreign)',
                    'ìœ¼ë¡œ(Foreign)', 'ì—”(Foreign)', 'ì—ì„œ(Foreign)', 'ì—ë„(Foreign)',
                    'ì—ëŠ”(Foreign)', 'ì—(Foreign)', 'ë©´(Foreign)', 'ë¡œ(Foreign)',
                    'ëŠ”(Foreign)', 'ë‚˜(Foreign)', 'ê°€(Foreign)', 'ë¼ê³ (Foreign)'
                    'ì´ë¼ëŠ”(Foreign)', 'ê»˜(Foreign)', 'ë¥¼(Foreign)', 'ê»˜(Foreign)',
                    'ê³ (Foreign)'
                   ]
    
    
    missing_Verb = ['í•˜ë©´(Foreign)', 'í•˜ê³ (Foreign)', 'ì£¼ê³ (Foreign)', 'ë˜ë©´(Foreign)',
                    'ëœ(Foreign)', 'í•´ì„œ(Foreign)', 'ì´ë©°(Foreign)'
                   ]
    
    
    
    missing_Suffix = ['ì–´ì¹˜(Foreign)', 'ì¹˜(Foreign)', 'ì°¨(Foreign)', 'ì§¸(Foreign)',
                      'ì§œë¦¬(Foreign)', 'ì”©(Foreign)', 'ìƒ(Foreign)', 'ëª…(Foreign)',
                      'ëŒ€(Foreign)', 'ë‹¬ì—(Foreign)', 'ë‹¬(Foreign)', 'ë„ì—(Foreign)',
                      'ë„(Foreign)', 'ë‚ (Foreign)', 'ê¸‰(Foreign)', 'ì–¸(Foreign)',
                      'ê°œ(Foreign)', 'ìš©(Foreign)', 'í˜•(Foreign)', 'ëŒ€ì˜(Foreign)',
                      'ê°€ëŸ‰(Foreign)', 'ê¸°(Foreign)', 'ë¶€(Foreign)', 'ê¸‰ì˜(Foreign)',
                      'ì œ(Foreign)', 'ë‹¹(Foreign)', 'ê°œì˜(Foreign)', 'ê¶Œ(Foreign)',
                      'ë¶ˆ(Foreign)', 'ë•Œ(Foreign)', 'ì§œë¦¬ê°€(Foreign)'
                     ]
    
    
    missing_Noun = ['ë„ì°©(Foreign)', 'í‡´ê·¼(Foreign)', 'ì»·(Foreign)', 'ê°(Foreign)',
                    'ê±¸ë¦¼(Foreign)', 'ì¶œë°œ(Foreign)', 'ë¦¬ì¦ˆ(Foreign)', 'ë„ì„œ(Foreign)',
                    'í€ë”©(Foreign)', 'ê²°ì œ(Foreign)', 'ë°œì†¡(Foreign)', 'ë°°ì†¡(Foreign)',
                    'ê¸°ì¤€(Foreign)', 'ê·œì •(Foreign)', 'ì™€ë””ì¦ˆ(Foreign)', 'ë¬´ìƒ(Foreign)',
                    'ë¦¬ì›Œë“œ(Foreign)', 'ì¶œê·¼(Foreign)', 'ê±°ë¦¬(Foreign)'
                   ]
    
    
    missing_Eomi = ['ì…ë‹ˆë‹¤(Foreign)', 'ì„(Foreign)', 'ì¸ë””(Foreign)', 'ì¸ë°(Foreign)',
                    'ì¸ê°€(Foreign)', 'ì´ì§€(Foreign)', 'ì´ìš”(Foreign)', ',ì—¬(Foreign)',
                    'ì´ì•¼(Foreign)', 'ì´ë˜(Foreign)', 'ì´ë¼ë‹ˆ(Foreign)', ', ì´ë‹¤(Foreign)',
                    'ì´ë‹ˆê¹Œ(Foreign)', 'ì´ë„¤(Foreign)', 'ìš”(Foreign)', 'ì•¼(Foreign)',
                    'ë¼(Foreign)', 'ë‹¤(Foreign)', 'ë„¤(Foreign)', ', ì´ì–Œ(Foreign)',
                    'ì´ë‹ˆ(Foreign)', 'ì´ë¼ëŠ”ë°(Foreign)', 'ëŒ€ì—(Foreign)', 'ë‹ˆê¹Œ(Foreign)',
                    'ì´ë„¹(Foreign)', 'ì´ë˜ë°(Foreign)', 'ì§€(Foreign)', 'ì—ë‚˜(Foreign)',
                    'í•¨(Foreign)', 'ì´ì—ˆëŠ”ë°(Foreign)', 'ì´ê±°ë“ (Foreign)', 'ì´ì—ˆë‚˜(Foreign)',
                    'ì´ì—ìš”(Foreign)'
                   ]
    
    missing_list = [missing_Adverb, missing_Josa, missing_Verb, missing_Suffix, missing_Noun, missing_Eomi]
    fit_list = ['(Adverb)', '(Josa)', '(Verb)', '(Suffix)', '(Noun)', '(Eomi)']
    for i in range(len(missing_list)):
        missing = missing_list[i]
        fit = fit_list[i]
        for n in range(len(missing)):
            if missing[n] in tokenized:
                fixed_pos = missing[n].replace('(Foreign)', fit)
                tokenized = tokenized.replace(missing[n], fixed_pos)
            else:
                pass
    return(tokenized)


def fix_suffix_Noun(tokenized):
    missing = ['ì˜¤ë¹ (Suffix)', 'ì–¸ë‹ˆ(Suffix)', 'ëˆ„ë‚˜(Suffix)', 'í˜•(Suffix)',
               'ì—„ë§ˆ(Suffix)', 'ì•„ë¹ (Suffix)', 'ë‹˜(Suffix)', 'ë¶„ë“¤(Suffix)',
               'User(Alpha)', 'UserUser(Alpha)', 'UserUserUser(Alpha)']
    
    for i in range(len(missing)):
        if missing[i] in tokenized:
            fixed_pos = missing[i].replace('(Suffix)', '(Noun)').replace('(Alpha)', '(Noun)')
            tokenized = tokenized.replace(missing[i], fixed_pos)

    return(tokenized)
```


```python
# í•´ì„ì´ ëª¨í˜¸í•˜ê±°ë‚˜ íŒŒê´´ëœ í† í° ê²°í—™ ë° í˜•íƒœì†Œ ì¬ë¶„ë¥˜
single_words = ['í• ì¸ê°€',  'í• ì¸ê°€ê²©', 'í• ì¸ê¸ˆ', 'í• ì¸ê¸ˆì•¡',
                'í€ë”©ê°€', 'í€ë”©ê°€ê²©', 'í€ë”©ê¸ˆ', 'í€ë”©ê¸ˆì•¡',
                'ì˜ˆì •ê°€', 'ì˜ˆì •ê°€ê²©',
                'ì •ìƒê°€', 'ì •ìƒê°€ê²©', 
                'ê²°ì œì¼',
                'ì•Œë¦¼ì‹ ì²­',
                'ì–¼ë¦¬ë²„ë“œ',
                'ë¦¬ì›Œë“œ', 
                'ì‚¬ì€í’ˆ',
                'êµ¬ì„±í’ˆ',
                'ìˆ˜ë ¹ì¼',
                'ë°°ì†¡ì¼',
                'ì¢…ë£Œì¼',
                'íƒë°°ì‚¬',
                'íƒë°°ë°œì†¡',
                'ë³´ì¦ê¸°ê°„',
                'Cíƒ€ì…',
                'ìƒˆì†Œì‹',
                'ê³ ê°ìƒŒí„°', 'ê³ ê°ì„¼í„°',
                'í›„ì›ê¸ˆ', 'í›„ì›ê¸ˆì•¡',
                'ëª¨ë¸ëª…',
                'í¬ë¼ìš°ë“œ',
                'íŠ¹í—ˆì¦',
                'ì‚¬ìš©ë²•', 'ì‚¬ìš©ë°©ë²•',
                'ì œì¡°êµ­',
                'ìƒì‚°ì§€',
                'ì ‘ìˆ˜ì²˜',
                'ì½œë“œë¸Œë£¨', 'ì½œë“œë¶€ë¥´',
                'í‚¤ë³´ë“œ',
                'ë°›ì¹¨ëŒ€',
                'ë§ì¶¤í˜•',
                'ê°€ì—´ì‹',
                'ì¼ì²´í˜•',
                'ëíŒì™•',
                'ì „ë¬¸ê°€ìš©',
                'ìˆ™ë ¨ì', 'ìˆ™ë ¨ììš©',
                'ëª…ì•”ë¹„',
                'í’€íŒ¨í‚¤ì§€',
                'ì™€ë””ì¦ˆ',
                'ì•„ë‹µí„°'
                'ì„œí¬í„°ì¦ˆ'
                'ì•ˆë‚´', 'ì•ˆë‚´ë¬¸'
                'ë‹¤ë“¤',
                'ë„ì°©',
                'ê·¸ìµ¸',
                'ì¶©ë™êµ¬ë§¤'
               ]

destroyed_words = {'í• ì¸(Noun)' : ['ê°€', 'ê¸ˆ'],
                   'í€ë”©(Noun)' : ['ê°€', 'ê¸ˆ'],
                   'ì˜ˆì •(Noun)' : ['ê°€'],
                   'ì •ìƒ(Noun)' : ['ê°€'],
                   'ê²°ì œ(Noun)' : ['ì¼'],
                   'ì•Œë¦¼(Noun)' : ['ì‹ '],
                   'ì–¼ë¦¬(Verb)' : ['ë²„'],
                   'ë¦¬(Noun)' : ['ì›Œ'], 
                   'ì‚¬ì€(Noun)': ['í’ˆ'],
                   'êµ¬(Modifier)' : ['ì„±'],
                   'ìˆ˜ë ¹(Noun)' : ['ì¼'],
                   'ë°°ì†¡(Noun)' : ['ì¼'],
                   'ì¢…ë£Œ(Noun)' : ['ì¼'],
                   'íƒë°°(Noun)' : ['ì‚¬', 'ë°œ'],
                   'ë³´ì¦(Noun)' : ['ê¸°ê°„'],
                   'C(Alpha)' : ['íƒ€'],
                   'ìƒˆ(Modifier)' : ['ì†Œ'],
                   'ê³ ê°(Noun)' : ['ìƒŒ', 'ì„¼'],
                   'í›„(Noun)' : ['ì›'],
                   'í›„ì›(Noun)' : ['ê¸ˆ'],
                   'ëª¨ë¸(Noun)' : ['ëª…'],
                   'í¬ë¼(Verb)' : ['ìš°'],       
                   'íŠ¹í—ˆ(Noun)' : ['ì¦'],
                   'ì‚¬ìš©(Noun)' : ['ë²•', 'ë°©'],
                   'ì‚¬(Modifier)' : ['ìš©'],
                   'ì œ(Modifier)' : ['ì¡°'],
                   'ìƒì‚°(Noun)' : ['ì§€'],
                   'ì ‘ìˆ˜(Noun)' : ['ì²˜'],
                   'ì½œë“œ(Noun)' : ['ë¸Œ', 'ë¶€'],
                   'í‚¤(Noun)' : ['ë³´'],
                   'ë°›ì¹¨(Noun)' : ['ëŒ€'],
                   'ë§ì¶¤(Noun)' : ['í˜•'],
                   'ê°€ì—´(Noun)' : ['ì‹'],
                   'ì¼ì²´(Noun)' : ['í˜•'],
                   'ëíŒ(Noun)' : ['ì™•'],
                   'ì „ë¬¸(Noun)' : ['ê°€'],
                   'ìˆ™ë ¨(Noun)' : ['ì'],
                   'ëª…ì•”(Noun)' : ['ë¹„'],
                   'í’€(Noun)' : ['íŒ¨'],
                   'ì™€ë””(Noun)' : ['ì¦ˆ'],
                   'ì•„(Exclamation)' : ['ë‹µ'],
                   'ì„œí¬í„°(Noun)' :['ì¦ˆ'],
                   'ì•ˆ(VerbPrefix)' : ['ë‚´'],
                   'ì•ˆë‚´(VerbPrefix)' : ['ë¬¸'],
                   'ë‹¤(Adverb)' : ['ë“¤'],
                   'ë„(Suffix)' : ['ì°©'],
                   'ê·¸(Noun)' : ['ìµ¸'],
                   'ì¶©ë™(Noun)' : ['êµ¬'],
                   'ë“¤(Suffix)' : ['ê°€']
                  }



def restore_word(tokenized_sentence):
    import re
    import numpy as np
    global re_sentence
    global pos_label
    
    token_list = tokenized_sentence.split()
    
    destroyed_keys = destroyed_words.keys()
    for dest_front in destroyed_keys:
        if dest_front in token_list:
            fw_indexs = np.where(np.array(token_list) == dest_front)[0].tolist()
            fron_word = dest_front
            
            for fw_index in fw_indexs:
                bw_index = int(fw_index + 1)
                
                if bw_index < len(token_list):
                    back_word = token_list[bw_index]
                    
                    dest_back_list = destroyed_words[dest_front]
                    for dest_back in dest_back_list:
                        if dest_back == back_word[0]:
                            
                            assemble_word = re.sub(pattern = r'\([^)]*\)', repl='', string = str(fron_word + back_word))

                            for s_word in single_words:
                                if s_word in assemble_word and s_word == assemble_word:
                                    re_fw = assemble_word + '(Noun)'

                                    token_list[fw_index] = re_fw
                                    token_list[bw_index] = ''

                                elif s_word in assemble_word and s_word != assemble_word:
                                    sw_lenght = len(s_word)
                                    re_fw = assemble_word[:sw_lenght] + '(Noun)'
                                    re_bw = assemble_word[sw_lenght:] + '(Josa)'

                                    token_list[fw_index] = re_fw
                                    token_list[bw_index] = re_bw
                                    
    ws_indexs = np.where(np.array(token_list) == '')[0].tolist()
    w = 0
    for ws in ws_indexs:
        ws -= w
        del token_list[ws]
        w += 1
    
    re_sentence = ' '.join(token_list)

    return(re_sentence)
```

```python
def refine_pos1(tokenized_sentence):
    token_list = tokenized_sentence.split()
    
    for i in range(len(token_list)-1):
        ft_index = i
        bt_index = i+1
        
        f_token = token_list[ft_index]
        b_token = token_list[bt_index]
        
        
        if f_token == '_(_)' and b_token == 'ì—‡(VerbPrefix)':          
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Exclamation)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
        
        
        if '(Noun)' in f_token or '(Adjective)' in f_token:
            if b_token == 'ìµ¸(VerbPrefix)':          
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Adjective)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
        
        
        if f_token == 'í•˜(Suffix)' and '(Josa)' in b_token:      
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Josa)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token


        
        if f_token == 'ì´(Determiner)' or f_token == 'ì €(Determiner)' or f_token == 'ê·¸(Determiner)':
            if b_token == 'ê±°(Noun)' or b_token == 'ê±¸(Noun)' or b_token == 'ê²ƒ(Noun)' or b_token == 'ê²Œ(Noun)' or b_token == 'ê³ (Suffix)' or b_token == 'ê¸°(Suffix)': 
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Determiner)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
                
                
        if f_token == 'ê·¸(Determiner)':
            if b_token == 'ì •ë„(Noun)' or b_token == 'ëŸ°ê°€(Noun)' or b_token == 'ë˜ì•¼(Noun)': 
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Adverb)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
        
        
        if f_token == 'ì´(Determiner)':
            if b_token == 'ì§€(Suffix)' or b_token == 'ë‹¹(Noun)':
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Eomi)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
        
        
        if f_token == 'ë„¤(Determiner)' and b_token == 'ì—¬(Noun)':
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

            f_token = assemble_token + '(Eomi)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
                
        
        if f_token == 'ì´(Determiner)' and b_token == 'ê¸°ì (Noun)':
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

            f_token = assemble_token + '(Adjective)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
            
        if f_token == 'ì´(Determiner)' or f_token == 'ê·¸(Determiner)' :
            if b_token == 'ì œ(Suffix)' or b_token == 'ì  (Noun)':
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Adverb)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
            
            
            
        if f_token == 'ì €(Determiner)' and b_token == 'ë‚˜(Noun)':
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

            f_token = assemble_token + '(Noun)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
        
        
        if f_token == 'ë‚´(Determiner)' or f_token == 'ë‘(Determiner)':
            if b_token == 'ê³ (Modifier)':
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Verb)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
        
        
        if f_token == 'ì´(Determiner)' and b_token == 'ê³ (Modifier)':
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

            f_token = assemble_token + '(Determiner)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
            
        if f_token == 'ë‚´(Determiner)':
            if b_token == 'ì¼(Suffix)' or b_token == 'ì¼(Modifier)':
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Noun)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
            
            
        if f_token == 'ì´(Determiner)': 
            if b_token == 'ì‚¬(Modifier)' or b_token == 'ëª¨(Modifier)' or b_token == 'ì‚¬(Suffix)' or b_token == 'ëª¨(Suffix)':
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Noun)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
            
        if f_token == 'ê·¸(Determiner)':
            if b_token == 'ë‹ˆ(Suffix)' or b_token == 'ë‹ˆ(Modifier)':
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Determiner)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
        
        if f_token == 'ì´(Determiner)' and b_token == 'ì—¿(Modifier)':
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

            f_token = assemble_token + '(Eomi)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
        
        
        if '(Number)' in f_token and  b_token == 'í¼(PreEomi)':          
            b_token = b_token.replace('PreEomi', 'Suffix')

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token

            
        if '(Noun)' in f_token and  b_token == 'ë‘(Josa)':          
            
            f_token = f_token
            b_token = b_token.replace('Determiner', 'Suffix')

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
            
        if f_token == 'ë„ˆ(Modifier)' and b_token == 'ë„¤(Modifier)':
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

            f_token = assemble_token + '(Modifier)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
        
        
        if '(Modifier)' in f_token and len(f_token) == 11 and f_token != 'ë„ˆ(Modifier)':
            if '(Modifier)' in b_token and len(b_token) == 11:           
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Modifier)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token  
            
            
        if f_token == 'ì˜(VerbPrefix)' and b_token == 'ëª»(VerbPrefix)':    
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Adjective)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
            
        if '(Noun)' in f_token:
            if b_token == 'ì (Suffix)' or b_token == 'ê³„(Suffix)' or b_token == 'ê¸‰(Suffix)':
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Adjective)'
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
            
            
        if '(Noun)' in f_token and b_token == 'ì–´(Suffix)':
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

            f_token = assemble_token + '(Verb)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
            
    ws_indexs = np.where(np.array(token_list) == '')[0].tolist()
    w = 0
    for ws in ws_indexs:
        ws -= w
        del token_list[ws]
        w += 1
    
    re_sentence = ' '.join(token_list)

    return(re_sentence)
            
            
            
def refine_pos2(tokenized_sentence):
    token_list = tokenized_sentence.split()
    
    for i in range(len(token_list)-1):
        ft_index = i
        bt_index = i+1
        
        f_token = token_list[ft_index]
        b_token = token_list[bt_index]
        
        
        if f_token == 'ì´(Suffix)' or f_token == 'ì¤‘(Suffix)':
            if '(Josa)' in b_token:
                assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))

                f_token = assemble_token + '(Adverb)' 
                b_token = ''

                token_list[ft_index] = f_token
                token_list[bt_index] = b_token
        
        
        if '(Josa)' in f_token and '(Josa)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Eomi)' 
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token


        if '(PreEomi)' in f_token and '(Eomi)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Eomi)' 
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token

        
        if '(Verb)' in f_token and '(Eomi)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Verb)' 
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
            
        if '(Adjective)' in f_token and '(Eomi)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Adjective)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
            
    ws_indexs = np.where(np.array(token_list) == '')[0].tolist()
    w = 0
    for ws in ws_indexs:
        ws -= w
        del token_list[ws]
        w += 1
    
    re_sentence = ' '.join(token_list)

    return(re_sentence)
```

```python
tokenized_df['tokenized'] = tokenized_df['tokenized'].progress_apply(fix_foreign)
tokenized_df['tokenized'] = tokenized_df['tokenized'].progress_apply(fix_suffix_Noun)
tokenized_df['tokenized'] = tokenized_df['tokenized'].progress_apply(restore_word)
tokenized_df['tokenized'] = tokenized_df['tokenized'].progress_apply(refine_pos1)
tokenized_df['tokenized'] = tokenized_df['tokenized'].progress_apply(refine_pos2)
tokenized_df = tokenized_df[['sex', 'age', 'contents', 'tokenized', 'token_count']]
tokenized_df
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3502912/3502912 [03:24<00:00, 17136.46it/s]
    ...ìƒëµ...



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>age</th>
      <th>contents</th>
      <th>tokenized</th>
      <th>token_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>ë‚˜ì§€ê¸ˆë°¥ë¨¸ê±°2ì‹œê°„ê±¸ì–´ì„œ ë²ˆí™”ê°€ì°¾ì•˜ì–´..ã…œã…œ ì‰ã…œã…œ ã…ã…ã…ã…ì˜¤ì¢‹ê² ë„¤ ã…‹ã„±ã…‹ã„±ã„±ã„±ã„±ì•„ë‹ˆ...</td>
      <td>ë‚˜(Noun) ì§€ê¸ˆ(Noun) ë°¥(Noun) ë¨¸ê±°(Verb) 2ì‹œê°„(Number) ...</td>
      <td>54</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>í— ã… ã…  ì–¸ë„í˜¸í…”ë“¤ê°€ã… ã…  ì—„ì²­í”¼ê±´í• ì²¸ë° ë‚˜ëŠ”ì¸ë‚«ëŸ¬ìš” ë‚˜ ë‘ì‹œì¶œê·¼ì´ë‹¤ã…ã…ã…ã… í€µìœ¼ë¡œí•œ...</td>
      <td>í—(Verb) ã… ã… (KoreanParticle) ì–¸ë„(Noun) í˜¸í…”ë“¤(Noun) ...</td>
      <td>49</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>í•™ìƒì´ë©´ì¢‹êµ¬! ì™œí˜¼ìë‹¤ë‹ˆëƒê³ ì˜¤..... ì™€ ë‚´ì¹œêµ°í•™êµë‚˜ê° ã…‹ã…‹ã…‹ã…‹ã…‹ ê·¸ë¥´ë„¤ ë§‰ì¡¸ì—…í•œ...</td>
      <td>í•™ìƒì´(Noun) ë©´(Josa) ì¢‹êµ¬(Adjective) !(Punctuation)...</td>
      <td>25</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>í›” í•™ìƒ ì—†ëŠ”ë°...ì£¼ë³€ì—... ì•„ë‹ˆ ë³µí•™í•˜ê³  í•™êµë¥¼ ëª»ê°€ëŠ”ë° ì–´ì¼€ ì¹œêµ¬ê°€ìˆëƒ.. ...</td>
      <td>í›”(Noun) í•™ìƒ(Noun) ì—†ëŠ”ë°(Adjective) ...(Punctuatio...</td>
      <td>31</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ì—¬ì„±</td>
      <td>30ëŒ€</td>
      <td>ì°¸ë‚˜ ë‚´ê°€ë­ì–¼ë§ˆë‚˜ê·¸ë¬ë‹¤ê³  ì›ƒê¸°ëŠ”ì‚¬ëŒì´ì•¼ì§€ì§œ ë„ˆë¬´í™”ë‚œë‹¹.. ê·¼ë°ì˜¤ë¹ ëŠ”ë§ì„ë˜ ì˜í•´ì„œ ë‚´...</td>
      <td>ì°¸ë‚˜(Noun) ë‚´(Noun) ê°€(Josa) ë­(Noun) ì–¼ë§ˆë‚˜(Noun) ê·¸ë¬ë‹¤...</td>
      <td>63</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>
<p>3554460 rows Ã— 5 columns</p>




```python
tokenized_df.to_csv('ì¹´í†¡ëŒ€í™”_Tokenized.csv(pos êµì •)', index = False)
```




<br><br>
## 03. ìƒëŒ€ í¸í–¥ë„/ìƒëŒ€ ë¹ˆì¶œë„ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•
### í˜•íƒœì†Œ ì¬ë¶„ë¥˜í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±




```python
re_dic = pos_token_countor(tokenized_df)
re_dic['word'], re_dic['pos'] = zip(*re_dic['Token'].apply(lambda x: word_pos_split(x)))
re_dic = re_dic[['Token', 'word', 'pos', 'Token_freq', 'Total_ratio']]
re_dic.to_csv('ì¹´í†¡ëŒ€í™”_pos_dic(pos êµì •).csv', index=False)
```




<br><br>
### í† í°ì˜ ìƒëŒ€ ë¹ˆì¶œë„ ë° ìƒëŒ€ í¸í–¥ë„ ê³„ì‚°


```python
def feature_extractor(data, dic , col, classes):
    classes_dic = dic.copy()
    for cls in classes:
        cls_df = data.loc[data[f'{col}'] == cls]

        Freq_voca = {t:0.1 for t in dic['Token']} # ë¬¸ì„œì˜ ì‘ì„±ìê°€ clsì¼ ë•Œ, íŠ¹ì • í† í°ì˜ ë¹ˆë„ ìˆ˜ì˜ ì´í•©
        Bias_voca = Freq_voca.copy() # ë¬¸ì„œì˜ ì‘ì„±ìê°€ clsì¼ ë•Œ, íŠ¹ì • í† í°ì˜ ì¶œí˜„ì—¬ë¶€ì˜ ì´í•©
        
        splited_li = split_dataframe(cls_df, size=1000)
        n = 0
        for splited_df in tqdm(splited_li, total=len(splited_li), desc=f"{cls}"): 
            if n%500 == 0:
                gc.collect()
            n+=1
            
            for index, row in splited_df.iterrows():
                token_list = row['tokenized'].split(' ') # ë¬¸ì„œ í† í¬ë‚˜ì´ì§•  

                for token in token_list: # íŠ¹ì • í† í°ì˜ ë¹ˆë„ ìˆ˜ ì¹´ìš´íŒ…
                    if token in Freq_voca:
                        Freq_voca[token] += 1

                unique_tokens = list(set(token_list)) # íŠ¹ì • í† í°ì˜ ì¶œì—°ì—¬ë¶€ ì¹´ìš´íŒ…
                for unique in unique_tokens:
                    if unique in Bias_voca:
                        Bias_voca[unique] += 1
  
        
        Freq_dic = pd.DataFrame(Freq_voca.items(), columns=['Token', f'Freq_{cls}'])
        Freq_dic[f'Freq_ratio_{cls}'] = Freq_dic[f'Freq_{cls}']/len(cls_df)
        
        Bias_dic = pd.DataFrame(Bias_voca.items(), columns=['Token', f'Bias_{cls}'])
        Bias_dic[f'Bias_ratio_{cls}'] = Bias_dic[f'Bias_{cls}']/len(cls_df) 
        
        cls_dic = pd.merge(dic[['Token']], Freq_dic, how='left', on='Token')                        
        cls_dic = pd.merge(cls_dic, Bias_dic, how='left', on='Token')
              
        classes_dic = pd.merge(classes_dic, cls_dic, how='left', on='Token')
        
    return classes_dic 
```


```python
gender_class = ['ë‚¨ì„±', 'ì—¬ì„±']
gender_dic = feature_extractor(tokenized_df, re_dic, 'sex', gender_class)

gender_dic['Gender_Freq'] = np.log(gender_dic['Freq_ratio_ë‚¨ì„±']/gender_dic['Freq_ratio_ì—¬ì„±']) # ì„±ë³„ ìƒëŒ€ ë¹ˆì¶œë„
gender_dic['Gender_Bias'] = np.log(gender_dic['Bias_ratio_ë‚¨ì„±']/gender_dic['Bias_ratio_ì—¬ì„±']) # ì„±ë³„ ìƒëŒ€ í¸í–¥ë„
gender_dic = gender_dic[['Token', 'word', 'pos', 'Token_freq', 'Total_ratio', 'Gender_Freq', 'Gender_Bias']]
```

    ë‚¨ì„±: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 791/791 [00:48<00:00, 16.28it/s]
    ...ìƒëµ...




```python
age_class = ['20ëŒ€ ë¯¸ë§Œ', '20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€', '70ëŒ€ ì´ìƒ'] 
age_dic = feature_extractor(tokenized_df, re_dic, 'age', age_class)

other_dic = token_dic[['Token']].copy().reset_index(drop=True)
for age in age_class:
    ages = ['20ëŒ€ ë¯¸ë§Œ', '20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€', '70ëŒ€ ì´ìƒ'] 
    others_num = len(df.loc[df['age'] != age])
    ages.remove(age)
    
    other_dic[f'Freq_ratio_others_{age}'] = (age_dic[[f'Freq_{a}' for a in ages]].sum(axis=1))/others_num
    other_dic[f'Bias_ratio_others_{age}'] = (age_dic[[f'Bias_{a}' for a in ages]].sum(axis=1))/others_num
    
    age_dic[f'{age}_Freq'] = np.log(age_dic[f'Freq_ratio_{age}']/other_dic[f'Freq_ratio_others_{age}']) 
    age_dic[f'{age}_Bias'] = np.log(age_dic[f'Bias_ratio_{age}']/other_dic[f'Bias_ratio_others_{age}']) 
    
age_dic = age_dic[['Token', 
                   '20ëŒ€ ë¯¸ë§Œ_Freq', '20ëŒ€ ë¯¸ë§Œ_Bias', '20ëŒ€_Freq', '20ëŒ€_Bias',
                   '30ëŒ€_Freq', '30ëŒ€_Bias', '40ëŒ€_Freq', '40ëŒ€_Bias',
                   '50ëŒ€_Freq', '50ëŒ€_Bias', '60ëŒ€_Freq', '60ëŒ€_Bias',
                   '70ëŒ€ ì´ìƒ_Freq', '70ëŒ€ ì´ìƒ_Bias']]

age_dic = age_dic.rename(columns={'20ëŒ€ ë¯¸ë§Œ_Freq':'A10_Freq', '20ëŒ€ ë¯¸ë§Œ_Bias':'A10_Bias',
                        '20ëŒ€_Freq':'A20_Freq', '20ëŒ€_Bias':'A20_Bias',
                        '30ëŒ€_Freq':'A30_Freq', '30ëŒ€_Bias':'A30_Bias',
                        '40ëŒ€_Freq':'A40_Freq', '40ëŒ€_Bias':'A40_Bias',
                        '50ëŒ€_Freq':'A50_Freq', '50ëŒ€_Bias':'A50_Bias',
                        '60ëŒ€_Freq':'A60_Freq', '60ëŒ€_Bias':'A60_Bias',
                        '70ëŒ€ ì´ìƒ_Freq':'A70_Freq', '70ëŒ€ ì´ìƒ_Bias':'A70_Bias'})
```

    20ëŒ€ ë¯¸ë§Œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:07<00:00, 14.59it/s]
    ...ìƒëµ...




```python
feature_dic = pd.merge(gender_dic, age_dic, how='left', on='Token')
feature_dic
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Token</th>
      <th>word</th>
      <th>pos</th>
      <th>Token_freq</th>
      <th>Total_ratio</th>
      <th>Gender_Freq</th>
      <th>Gender_Bias</th>
      <th>A10_Freq</th>
      <th>A10_Bias</th>
      <th>A20_Freq</th>
      <th>...</th>
      <th>A30_Freq</th>
      <th>A30_Bias</th>
      <th>A40_Freq</th>
      <th>A40_Bias</th>
      <th>A50_Freq</th>
      <th>A50_Bias</th>
      <th>A60_Freq</th>
      <th>A60_Bias</th>
      <th>A70_Freq</th>
      <th>A70_Bias</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>?(Punctuation)</td>
      <td>?</td>
      <td>Punctuation</td>
      <td>1865214</td>
      <td>0.532475</td>
      <td>-1.092652</td>
      <td>-1.125282</td>
      <td>1.582501</td>
      <td>1.639460</td>
      <td>4.709142</td>
      <td>...</td>
      <td>3.807751</td>
      <td>3.728915</td>
      <td>1.845276</td>
      <td>1.620177</td>
      <td>1.337092</td>
      <td>1.128453</td>
      <td>-0.383700</td>
      <td>-0.437217</td>
      <td>-2.976230</td>
      <td>-2.885823</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ã…‹ã…‹ã…‹ã…‹ã…‹(KoreanParticle)</td>
      <td>ã…‹ã…‹ã…‹ã…‹ã…‹</td>
      <td>KoreanParticle</td>
      <td>1526323</td>
      <td>0.435730</td>
      <td>-1.762353</td>
      <td>-1.680824</td>
      <td>1.680877</td>
      <td>1.665422</td>
      <td>4.995337</td>
      <td>...</td>
      <td>3.763199</td>
      <td>3.676334</td>
      <td>-0.645103</td>
      <td>-0.455351</td>
      <td>-1.662415</td>
      <td>-1.496536</td>
      <td>-3.916414</td>
      <td>-3.791524</td>
      <td>-11.291175</td>
      <td>-10.853299</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ì—(Josa)</td>
      <td>ì—</td>
      <td>Josa</td>
      <td>1376727</td>
      <td>0.393024</td>
      <td>-1.354588</td>
      <td>-1.337686</td>
      <td>1.516475</td>
      <td>1.556426</td>
      <td>4.815396</td>
      <td>...</td>
      <td>3.781199</td>
      <td>3.756026</td>
      <td>1.484062</td>
      <td>1.430571</td>
      <td>0.922323</td>
      <td>0.894123</td>
      <td>-0.292875</td>
      <td>-0.305520</td>
      <td>-3.032982</td>
      <td>-3.025299</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ê°€(Josa)</td>
      <td>ê°€</td>
      <td>Josa</td>
      <td>1144262</td>
      <td>0.326660</td>
      <td>-1.330090</td>
      <td>-1.316493</td>
      <td>1.611810</td>
      <td>1.626960</td>
      <td>4.836498</td>
      <td>...</td>
      <td>3.745612</td>
      <td>3.730188</td>
      <td>1.468008</td>
      <td>1.432860</td>
      <td>0.856439</td>
      <td>0.860018</td>
      <td>-0.144169</td>
      <td>-0.195058</td>
      <td>-2.928766</td>
      <td>-2.935806</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ì´(Josa)</td>
      <td>ì´</td>
      <td>Josa</td>
      <td>919870</td>
      <td>0.262602</td>
      <td>-1.318207</td>
      <td>-1.311745</td>
      <td>1.399605</td>
      <td>1.440919</td>
      <td>4.757490</td>
      <td>...</td>
      <td>3.840997</td>
      <td>3.813802</td>
      <td>1.523747</td>
      <td>1.475925</td>
      <td>1.019233</td>
      <td>0.979362</td>
      <td>0.074839</td>
      <td>-0.029539</td>
      <td>-2.691890</td>
      <td>-2.770597</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>18356</th>
      <td>ì•”ë°(Noun)</td>
      <td>ì•”ë°</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
      <td>-1.481605</td>
      <td>-1.486564</td>
      <td>1.325666</td>
      <td>1.337359</td>
      <td>4.437628</td>
      <td>...</td>
      <td>4.217711</td>
      <td>4.220826</td>
      <td>1.701743</td>
      <td>1.713539</td>
      <td>0.075802</td>
      <td>0.087326</td>
      <td>0.492802</td>
      <td>0.095993</td>
      <td>-2.913589</td>
      <td>-2.902128</td>
    </tr>
    <tr>
      <th>18357</th>
      <td>ë¼ë“œ(Noun)</td>
      <td>ë¼ë“œ</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
      <td>-1.094817</td>
      <td>-1.090738</td>
      <td>0.757370</td>
      <td>0.795543</td>
      <td>4.912486</td>
      <td>...</td>
      <td>3.725778</td>
      <td>3.774007</td>
      <td>1.336311</td>
      <td>1.374823</td>
      <td>0.774695</td>
      <td>0.812868</td>
      <td>0.783361</td>
      <td>0.821534</td>
      <td>-2.913589</td>
      <td>-2.875849</td>
    </tr>
    <tr>
      <th>18358</th>
      <td>ì•¡ìƒ(Noun)</td>
      <td>ì•¡ìƒ</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
      <td>-0.445995</td>
      <td>-0.421465</td>
      <td>0.466524</td>
      <td>0.173400</td>
      <td>5.630803</td>
      <td>...</td>
      <td>3.085743</td>
      <td>3.107728</td>
      <td>0.767727</td>
      <td>0.184045</td>
      <td>0.075516</td>
      <td>0.190725</td>
      <td>-2.916976</td>
      <td>-2.802428</td>
      <td>-2.913874</td>
      <td>-2.799325</td>
    </tr>
    <tr>
      <th>18359</th>
      <td>ìŠ¤í‹°(Noun)</td>
      <td>ìŠ¤í‹°</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
      <td>-1.704748</td>
      <td>-1.638637</td>
      <td>0.983399</td>
      <td>1.039861</td>
      <td>4.927054</td>
      <td>...</td>
      <td>3.824710</td>
      <td>3.831202</td>
      <td>0.477456</td>
      <td>0.533584</td>
      <td>0.075802</td>
      <td>0.131765</td>
      <td>0.084469</td>
      <td>0.140431</td>
      <td>-2.913589</td>
      <td>-2.857938</td>
    </tr>
    <tr>
      <th>18380</th>
      <td>ë‹¤ë¥¸íŒ€(Noun)</td>
      <td>ë‹¤ë¥¸íŒ€</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
      <td>-1.500599</td>
      <td>-1.477266</td>
      <td>0.466524</td>
      <td>0.501595</td>
      <td>5.463502</td>
      <td>...</td>
      <td>3.345351</td>
      <td>3.364846</td>
      <td>0.068836</td>
      <td>0.103804</td>
      <td>-0.620491</td>
      <td>-0.585624</td>
      <td>-2.916976</td>
      <td>-2.882200</td>
      <td>-2.913874</td>
      <td>-2.879098</td>
    </tr>
  </tbody>
</table>
<p>18381 rows Ã— 21 columns</p>






```python
feature_dic.to_csv('ì¹´í†¡ëŒ€í™”_feature_dic.csv', index=False)
```


