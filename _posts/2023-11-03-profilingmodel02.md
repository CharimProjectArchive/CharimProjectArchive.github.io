---
layout: single
title:  "Part 02. ì˜¨ë¼ì¸ ë©”ì‹œì§€ ì‘ì„±ì í”„ë¡œíŒŒì¼ë§ ëª¨ë¸ ê°œë°œ: ì „ì²˜ë¦¬ ë° ë³€ìˆ˜ê°œë°œ"
categories: Project:í”„ë¡œíŒŒì¼ë§_ëª¨ë¸_ê°œë°œ
tag: [NLP, ë¶ˆìš©ì–´ì²˜ë¦¬, í‘œì œí™”, í† í¬ë‚˜ì´ì¦ˆ, ë³€ìˆ˜ê°œë°œ]
---
<span style="color: #808080">#NLP #ë¶ˆìš©ì–´ ì²˜ë¦¬ #í‘œì œí™” #í† í¬ë‚˜ì´ì¦ˆ #ë³€ìˆ˜ê°œë°œ #ìì—°ì–´ ê³„ëŸ‰</span>
<hr>

{: .notice--primary} 
ğŸ’¡**í”„ë¡œì íŠ¸ ë°°ê²½**<br>

ê°œì¸ì •ë³´ ë³´í˜¸ì— ëŒ€í•œ ì‚¬íšŒì  ë¶„ìœ„ê¸°ì— ë”°ë¼ êµ¬ê¸€ ì¨ë“œíŒŒí‹° ì œê³µ ì¤‘ë‹¨, ì• í”Œ ì‚¬ìš©ì ì •ë³´ ê³µê°œ ì¤‘ë‹¨ ë“± ì‚¬ìš©ì ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì´ ì–´ë ¤ì›Œì§€ê³  ìˆìŒ. ìì‚¬ í”Œë«í¼ì— ê°€ì…í•œ ì‚¬ìš©ì ì™¸ ê³ ê° ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒì€ ë”ìš± ì–´ë ¤ì›€.<br><br> 
ë§¥ë½ì •ë³´ë¥¼ í™œìš©í•œ ê³ ê° ë¶„ì„ ë° íƒ€ê²ŸíŒ… ì „ëµì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìœ¼ë‚˜ ëŒ€í‘œì ìœ¼ë¡œ ê³ ê°ì´ ìƒì‚°í•˜ëŠ” ë§¥ë½ì •ë³´ì¸ ì±„íŒ…, ë¦¬ë·°, í”¼ë“œ ë“±ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” ë§¤ìš° ì œí•œì ì„. íŠ¹íˆ, ì‚¬ìš©ìì˜ ì¸êµ¬í†µê³„ì  ì •ë³´ê°€ ì œê³µë˜ëŠ” ê²½ìš°ëŠ” ë§¤ìš° ë“œë¬¼ì–´ ê³ ê° ë¶„ì„ ë° íƒ€ê²ŸíŒ…ì— í™œìš©í•  ìˆ˜ ì—†ìŒ.<br><br> 
**ì±„íŒ…, ë¦¬ë·°, í”¼ë“œ ë“±ì—ì„œ ì‚¬ìš©ìì˜ ì¸êµ¬ í†µê³„ì  ì •ë³´ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´, ê³ ê° ë¶„ì„ ë° íƒ€ê²ŸíŒ…ì— í™œìš©í•  ìˆ˜ ìˆì„ ë¿ë§Œì•„ë‹ˆë¼ ì±—ë´‡/ë©”íƒ€ë²„ìŠ¤ ì„œë¹„ìŠ¤/CRM ì„œë¹„ìŠ¤ ë“±ì„ ê³ ë„í™”ì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë¨**<br><br>
 
{: .notice--primary} 
ğŸ¯**í”„ë¡œì íŠ¸ ëª©ì **<br>

ê³ ê°ì´ ì˜¨ë¼ì¸ ìƒì—ì„œ ìƒì‚°í•˜ëŠ” ë§¥ë½ì •ë³´(ì±„íŒ…, ë¦¬ë·°, í”¼ë“œ ë“±ì˜ í…ìŠ¤íŠ¸ ì •ë³´)ì—ì„œ **ì‘ì„±ìì˜ ì¸êµ¬ í†µê³„ì  ì •ë³´(ì„±ë³„/ì—°ë ¹)ë¥¼ ì¶”ì •í•˜ëŠ” ì‘ì„±ì í”„ë¡œíŒŒì¼ë§ ëª¨ë¸ ê°œë°œ**<br><br>
 
 
## ë³€ìˆ˜ ê°œë°œ ìš”ì•½<br>
**ë³€ìˆ˜ ê°œë°œì˜ í•„ìš”ì„±**
- ì¤„ì„ë§, ê³ ìœ  í‘œí˜„, ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œë§Œ êµ¬ì„±ëœ ë¶ˆì™„ì „ í‘œí˜„, ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ ë“±ì˜ í‘œí˜„ì€ **ê¸°ì¡´ì˜ NLP ì „ì²˜ë¦¬ ë° ì–¸ì–´ëª¨ë¸ì˜ Encoding ê³¼ì •ì—ì„œ ì œê±°ë¨**. í•˜ì§€ë§Œ, ì´ì™€ ê°™ì€ í‘œí˜„ë“¤ì´ **ë°œí™”ìì˜ ì„±ë³„/ì—°ë ¹ì— ëŒ€í•œ ì–¸ì–´ì  íŠ¹ì§•ìœ¼ë¡œ ë³´ì—¬ì§€ë¯€ë¡œ** ìƒê´€ê´€ê³„ íŒŒì•… ë° ëª¨ë¸ê°œë°œ ì‹œ ë°˜ì˜ì„ ìœ„í•œ feauture ê°œë°œ í•„ìš”
- ëª¨ë“  í‘œí˜„ì„ encodingí•˜ëŠ” ë°©ì‹ì€ ë¹„íš¨ìœ¨ì ì¼ë¿ë§Œ ì•„ë‹ˆë¼ ì‚¬ì „ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ **í‘œí˜„ì˜ íŠ¹ì„±ì„ ê³„ëŸ‰í™”í•œ ë³€ìˆ˜**ê°€ ìš”êµ¬ë¨
<br>

 
### 1. í‘œí˜„ì˜ ê¸¸ì´ì™€ ì² ìì˜ ë‹¤ì–‘ì„±ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ë³µì¡ë„ ê³„ì‚°
**Complexity of Specific Expression(C, íŠ¹ìˆ˜í‘œí˜„ ë³µì¡ë„)** 
- í‘œí˜„ tê°€ ë³µì¡í•œ ì •ë„ë¥¼ ì„¤ëª…í•¨
- ì „ì²˜ë¦¬ì—ì„œ ëŒ€ë¶€ë¶„ ì†Œì‹¤ë˜ëŠ” ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ í‘œí˜„, ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ë¡œ êµ¬ì„±ëœ í‘œí˜„ì— ëŒ€í•œ ë³µì¡ë„ë¥¼ ê³„ì‚°
- ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ í‘œí˜„ì„ í•œ ë¶„ë¥˜ë¡œ í•˜ê³ , ë¬¸ì¥ê¸°í˜¸/íŠ¹ìˆ˜ê¸°í˜¸/ì´ëª¨ì§€ë¥¼ í•œ ë¶„ë¥˜ë¡œ êµ¬ë¶„í•˜ì—¬ ê³„ì‚°(í˜•íƒœì†Œ ë¶„ë¥˜ ë° ì •ê·œì‹ ì²˜ë¦¬ì— ìš©ì´)
> $C_i= ln[ 1/Î (U_i / L_i) * L_i]$
> $U_i$ : í‘œí˜„ $t_i$ ê°€ í¬í•¨í•˜ëŠ” ê³ ìœ í•œ ì² ì ë˜ëŠ” ê¸°í˜¸ì˜ ì¢…ë¥˜ ìˆ˜
> $L_i$ : í‘œí˜„ $t_i$ ì˜ ê¸¸ì´(ì² ì ë° ê¸°í˜¸ ê°œìˆ˜)
> <br><br>
> í‘œí˜„ì„ ì´ë£¨ëŠ” ê³ ìœ  ì² ì ë° ê¸°í˜¸ ë³„ ì ìœ ìœ¨ $U_i / L_i$ ì„ ëª¨ë‘ ê³±í•˜ê³ , í‘œí˜„ì˜ ê¸¸ì´ë¥¼ ê³±í•˜ì—¬ ê³ ìœ  ì² ìì˜ ì¢…ë¥˜ì™€ í‘œí˜„ì˜ ê¸¸ì´ì— ë¹„ë¡€í•˜ëŠ” ë³µì¡ë„ ì¸¡ì •. ë‹¤ì–‘í•œ ì² ìê°€ ì‚¬ìš©ë  ìˆ˜ë¡, ì‚¬ìš©ëœ ê³ ìœ  ì² ì ë° ê¸°í˜¸ ìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ë³µì¡ë„ê°€ ì»¤ì§<br>
â‡’ ì „ì²˜ë¦¬ ì „ì˜ raw í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì¸¡ì •í•˜ë©°, ì „ì²˜ë¦¬ì—ì„œ ì†Œì‹¤ë˜ëŠ” íŠ¹ìˆ˜í‘œí˜„ë“¤ì˜ ì •ë³´ë¥¼ ì¼ë¶€ ë°˜ì˜í•¨

<br><br>
### 2. ê° í‘œí˜„ì˜ ì¢…ì†ë³€ìˆ˜(ì„±ë³„/ì—°ë ¹)ì— ëŒ€í•œ Odds Ratioë¥¼ ê³„ì‚°
**Relative Bias(RB, ìƒëŒ€ í¸í–¥ë„)**: í‘œí˜„ tê°€ ë“±ì¥ í–ˆì„ ë•Œ, í…ìŠ¤íŠ¸ì˜ **ì‘ì„±ìê°€ íŠ¹ì • ì„±ë³„/ì—°ë ¹ì¸ ì •ë„**ë¥¼ ì„¤ëª…í•¨
<br><br>
a. **Relative Bias of Gender**(RBG, ìƒë³„ì— ëŒ€í•œ ìƒëŒ€ í¸í–¥ë„)<br>
- í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¬¸ì¥ $s$ ì˜ ì‘ì„±ì ì„±ë³„ì´ ë‚¨ì $male$ ë˜ëŠ” ì—¬ì $female$ ì¸ ì •ë„
> $RBG_i = ln[ p( s_{male} â t_iâˆˆs_{male} ) / p( s_{female} â t_iâˆˆs_{female} ) ]$
>  $t_i$ : ë¬¸ì„œ ë‚´ ië²ˆ ì§¸ í‘œí˜„<br>
> $s_{male}$ : ì‘ì„±ìì˜ ì„±ë³„ì´ ë‚¨ì„±(m)ì¸ ë¬¸ì¥
>  $s_{female}$ : ì‘ì„±ìì˜ ì„±ë³„ì´ ì—¬ì„±(f)ì¸ ë¬¸ì¥
> <br><br>
> ì‘ì„±ìì˜ ì„±ë³„ì´ ë‚¨ì„±ì¸ ë¬¸ì¥ $s_{male}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨ Ã· ì—¬ì„±ì¸ ë¬¸ì¥ $s_{female}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨, 0~1 ì‚¬ì´ì˜ Skewedí•œ ê°’ì„ ê°€ì§ìœ¼ë¡œ logë¥¼ ì·¨í•´ ì •ê·œí™”<br>
  â‡’ $t_i$ ê°€ ë“±ì¥í–ˆì„ ë•Œ ì‘ì„±ìì˜ ì„±ë³„ì´ ë‚¨ì $m$ ë˜ëŠ” ì—¬ì $f$ ì¸ ì •ë„

<br><br>
 b. **Relative Bias of Age**(RBG, ì—°ë ¹ì— ëŒ€í•œ ìƒëŒ€ í¸í–¥ë„)<br>
  - í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¬¸ì¥ $s$ ì˜ ì‘ì„±ì ì—°ë ¹ì´ íŠ¹ì • ì—°ë ¹ëŒ€ $age$ ì¸ ì •ë„ë¥¼ ì„¤ëª…í•¨<br>
  - ì—°ë ¹ëŒ€ëŠ” 20ëŒ€ ë¯¸ë§Œ/20ëŒ€/30ëŒ€/40ëŒ€/50ëŒ€ ì´ìƒ 5 classë¡œ ë¶„ë¥˜
> $RBAi = ln[ p( s_{age} â t_iâˆˆs_{age} ) / p( s_{other} â t_iâˆˆs_{other} ) ]$
> $t_i$ : ë¬¸ì„œ ë‚´ ië²ˆ ì§¸ í‘œí˜„
> $s_{age}$ : ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ ageì¸ ë¬¸ì¥
> $s_{other}$ : ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ ageê°€ ì•„ë‹Œ ë¬¸ì¥
> <br><br>
> ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ 20ëŒ€ì¸ ë¬¸ì¥ $s_{a20}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨ Ã· ì—°ë ¹ëŒ€ê°€ 20ëŒ€ê°€ ì•„ë‹Œ ë¬¸ì¥ $s_{other}$ ì—ì„œ í‘œí˜„ $t_i$ ê°€ ë“±ì¥í•œ ë¹„ìœ¨, 0~1 ì‚¬ì´ì˜ Skewedí•œ ê°’ì„ ê°€ì§ìœ¼ë¡œ logë¥¼ ì·¨í•´ ì •ê·œí™”<br>
â‡’ $t_i$ê°€ ë“±ì¥í–ˆì„ ë•Œ ì‘ì„±ìì˜ ì—°ë ¹ëŒ€ê°€ ageì¸ ì •ë„<br>
<br>

 
**Relative frequency (RF, ìƒëŒ€ ë¹ˆì¶œë„)**
- í…ìŠ¤íŠ¸ì˜ ì‘ì„±ìê°€ íŠ¹ì • ì„±ë³„/ì—°ë ¹ì¼ ë•Œ íƒ€ ì„±ë³„/ì—°ë ¹ ë³´ë‹¤ **í‘œí˜„ të¥¼ ìƒëŒ€ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì •ë„**ë¥¼ ì„¤ëª…í•¨
- ìƒëŒ€ í¸í˜•ë„ëŠ” ë¬¸ì¥ì—ì„œ í‘œí˜„ì˜ ì¶œì—°ì—¬ë¶€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ë°˜ë©´, ìƒëŒ€ ë¹ˆì¶œë„ëŠ” ë¹ˆë„ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨<br>
- ìƒëŒ€ í¸í–¥ë„ì™€ ê³„ì‚° ë°©ì‹ì´ ìœ ì‚¬í•˜ë¯€ë¡œ ì„¤ëª…ì€ ìƒëµí•¨
>  $RFG_i = ln[ p( t_i â s_{male} ) / p( t_i â s_{female} ) ]$
>  $RFAi = ln[ p( t_i â s_{age} ) / p( t_i â s_{other} ) ]$


<br><br>
- ë¬¸ì¥ì´ í¬í•¨í•˜ëŠ” í† í° ì „ì²´ì— ëŒ€í•œ ìƒëŒ€ í¸í–¥ë„ ë° ìƒëŒ€ ë¹ˆì¶œë„ì˜ í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•¨ìœ¼ë¡œì¨ ê¸°ì¡´ì˜ NLP ë°©ì‹ì—ì„œ ì†Œì‹¤ë˜ëŠ” í† í° ì •ë³´ì˜ ì¼ë¶€ë¶„ ë³´ì™„í•  ìˆ˜ ìˆìŒ
- í‘œí˜„ë³„ ìƒëŒ€ í¸í–¥ë„ ë° ìƒëŒ€ í¸í–¥ë„ì— ëŒ€í•œ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•ì´ í•„ìš”
- Odds ratioì˜ ê°’ì„ ì¼ë°˜í™”í•  ìˆ˜ ìˆì„ ë§Œí¼ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì „ì œë˜ì–´ì•¼ í•˜ë©°, ë³¸ ë°ì´í„°ì…‹ì€ ê·¸ì— ì¤€í•œë‹¤ê³  ê°€ì •í•¨
- ìƒëŒ€ í¸í–¥ë„ì™€ ìƒëŒ€ ë¹ˆì¶œë„ê°€ ë§¤ìš° ìœ ì‚¬í•¨ìœ¼ë¡œ ìœ ì˜ì„± ê²€í† ë¥¼ í†µí•´ 1ê°€ì§€ë§Œ ì±„íƒ
<br>

<br><br>
## 01. íŠ¹ìˆ˜í‘œí˜„ ë³µì¡ë„ ì¸¡ì •
- ë¶ˆìš©ì–´ ì²˜ë¦¬/í‘œì œí™” ì „ì— ìˆ˜í–‰
- ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë¶ˆì™„ì „ í‘œí˜„ / ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ í‘œí˜„ì„ êµ¬ë¶„í•˜ì—¬ ì¸¡ì •
- ë¬¸ì¥ì—ì„œ ì¶œí˜„í•˜ëŠ” íŠ¹ìˆ˜í‘œí˜„ì˜ ê°œìˆ˜, í‰ê· , í‘œì¤€í¸ì°¨, ìµœëŒ€ê°’, ìµœì†Œê°’ì„ ê³„ì‚°




```python
import pandas as pd
from tqdm import tqdm
tqdm.pandas()

import re
import numpy as np
from collections import Counter

df = pd.read_csv("SNS_FULL_Dataset(raw_ì¤‘ë³µëœ ë©”ì„¸ì§€ ì œê±°).csv")
```



<br><br>
### ììŒ ë˜ëŠ” ëª¨ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë¶ˆì™„ì „ í‘œí˜„ì˜ ë³µì¡ë„


```python
def spell_complexity(sentence):
    sequences = re.findall(r'[ã„±-ã…ã…-ã…£]+', sentence)
    specific_sequences = [seq for seq in sequences if len(seq) > 0]
    
    N = len(specific_sequences)
    
    if N != 0:
        complexity_list=[]
        for seq in specific_sequences:
            spell_list = []
            for s in seq:
                spell_list.append(s)  
```




```python
print(spell_complexity('ì•ˆë…• ã… ã…  ã… ã… ã…  ã… ã…œ ã…‡ã…‹ã…‡ã…‹ ã„·ã…‹-ã…ã…- ã… '))
```

    (7, 1.3451969221982076, 0.9131088298012547, 2.772588722239781, 0.0)




```python
print(spell_complexity('ì•ˆ ã… ã…  ã…‹ã…‹ã…‹'))
```

    (2, 0.8958797346140275, 0.20273255405408225, 1.0986122886681098, 0.6931471805599453)



<br><br> 
### ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€ í‘œí˜„ì— ëŒ€í•œ ë³µì¡ë„




```python
def symbol_complexity(sentence):
    sequences = re.findall(r'[^ã„±-ã…ã…-ã…£ê°€-í£a-zA-Z0-9\s]+', sentence)
    specific_sequences = [seq.strip() for seq in sequences if seq.strip()]
    
    N = len(specific_sequences)
    
    if N != 0:
        complexity_list=[]
        for seq in specific_sequences:
            spell_list = []
            for s in seq:
                spell_list.append(s)  

            uniqe_list = Counter(spell_list)
            uniqe_freq = list(uniqe_list.values())

            L = len(spell_list)
            uniqe_num =len(uniqe_freq)


            ratio_list = []
            for fu in uniqe_freq:
                ratio = (fu/L)
                ratio_list.append(ratio)

            ans = 1
            for r in ratio_list:
                ans *= r

            complexity = np.log(1/ans*L)
            complexity_list.append(complexity)
            
        mean = np.mean(complexity_list)
        std =  np.std(complexity_list)
        max_ = np.max(complexity_list)
        min_ = np.min(complexity_list)
    else:
        mean = 0
        std = 0
        max_ = 0
        min_ = 0
        
    return N, mean, std, max_, min_
```


```python
print(symbol_complexity('ã… ã…  ì•ˆë…•! ??... ì˜ˆì‹œ: 2^^; -_-;; o_O ğŸ˜˜ ğŸ™ â˜ºï¸ â¤ï¸ğŸ§¡ğŸ’›'))
```

    (10, 1.970161458941443, 2.3466123014783795, 6.931471805599453, 0.0)



```python
print(symbol_complexity('ã… ã…  ì•ˆë…•ğŸ˜˜ğŸ˜˜ğŸ˜˜ ğŸ˜˜ğŸ˜˜'))
```

    (2, 0.8958797346140275, 0.20273255405408225, 1.0986122886681098, 0.6931471805599453)



```python
#ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ê°’ìœ¼ë¡œ ë³€í™˜
df['spell_complexity'] = df['contents'].progress_apply(lambda x:spell_complexity(str(x)))
df['spell_num'] = df['spell_complexity'].progress_apply(lambda x:x[0])
df['spell_mean'] = df['spell_complexity'].progress_apply(lambda x:x[1])
df['spell_std'] = df['spell_complexity'].progress_apply(lambda x:x[2])
df['spell_max'] = df['spell_complexity'].progress_apply(lambda x:x[3])
df['spell_min'] = df['spell_complexity'].progress_apply(lambda x:x[4])


df['symbol_complexity'] = df['contents'].progress_apply(lambda x:symbol_complexity(str(x)))
df['symbol_num'] = df['symbol_complexity'].progress_apply(lambda x:x[0])
df['symbol_mean'] = df['symbol_complexity'].progress_apply(lambda x:x[1])
df['symbol_std'] = df['symbol_complexity'].progress_apply(lambda x:x[2])
df['symbol_max'] = df['symbol_complexity'].progress_apply(lambda x:x[3])
df['symbol_min'] = df['symbol_complexity'].progress_apply(lambda x:x[4])


df = df.drop(columns={'spell_complexity', 'symbol_complexity'})
df = df.fillna(0)
df
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3564042/3564042 [04:53<00:00, 12152.45it/s]
    ...ìƒëµ...





<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>topic</th>
      <th>sex</th>
      <th>age</th>
      <th>resident</th>
      <th>contents</th>
      <th>length</th>
      <th>spell_num</th>
      <th>spell_mean</th>
      <th>spell_std</th>
      <th>spell_max</th>
      <th>spell_min</th>
      <th>symbol_num</th>
      <th>symbol_mean</th>
      <th>symbol_std</th>
      <th>symbol_max</th>
      <th>symbol_min</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>ë‚˜ì§€ê¸ˆë°¥ë¨¸ê±°2ì‹œê°„ê±¸ì–´ì„œ ë²ˆí™”ê°€ì°¾ì•˜ì–´..ã…œã…œ ì‰ã…œã…œ ã…ã…ã…ã…ì˜¤ì¢‹ê² ë„¤ ã…‹ã„±ã…‹ã„±ã„±ã„±ã„±ì•„ë‹ˆ...</td>
      <td>127</td>
      <td>6</td>
      <td>2.185021</td>
      <td>1.299761</td>
      <td>3.765840</td>
      <td>0.693147</td>
      <td>4</td>
      <td>0.173287</td>
      <td>0.300142</td>
      <td>0.693147</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>í— ã… ã…  ì–¸ë„í˜¸í…”ë“¤ê°€ã… ã…  ì—„ì²­í”¼ê±´í• ì²¸ë° ë‚˜ëŠ”ì¸ë‚«ëŸ¬ìš” ë‚˜ ë‘ì‹œì¶œê·¼ì´ë‹¤ã…ã…ã…ã… í€µìœ¼ë¡œí•œ...</td>
      <td>130</td>
      <td>6</td>
      <td>0.961387</td>
      <td>0.555163</td>
      <td>1.609438</td>
      <td>0.000000</td>
      <td>6</td>
      <td>2.041180</td>
      <td>1.296771</td>
      <td>4.212128</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>í•™ìƒì´ë©´ì¢‹êµ¬! ì™œí˜¼ìë‹¤ë‹ˆëƒê³ ì˜¤..... ì™€ ë‚´ì¹œêµ°í•™êµë‚˜ê° ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ ê·¸ë¥´ë„¤ ...</td>
      <td>56</td>
      <td>1</td>
      <td>2.197225</td>
      <td>0.000000</td>
      <td>2.197225</td>
      <td>2.197225</td>
      <td>3</td>
      <td>1.404043</td>
      <td>1.072424</td>
      <td>2.602690</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>ê²½ê¸°ë„</td>
      <td>í›” í•™ìƒ ì—†ëŠ”ë°...ì£¼ë³€ì—... ì•„ë‹ˆ ë³µí•™í•˜ê³  í•™êµë¥¼ ëª»ê°€ëŠ”ë° ì–´ì¼€ ì¹œêµ¬ê°€ìˆëƒ.. ...</td>
      <td>74</td>
      <td>1</td>
      <td>2.079442</td>
      <td>0.000000</td>
      <td>2.079442</td>
      <td>2.079442</td>
      <td>4</td>
      <td>0.997246</td>
      <td>0.175572</td>
      <td>1.098612</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ê°œì¸ë°ê´€ê³„</td>
      <td>ì—¬ì„±</td>
      <td>30ëŒ€</td>
      <td>ì¶©ì²­ë¶ë„</td>
      <td>ì°¸ë‚˜ ë‚´ê°€ë­ì–¼ë§ˆë‚˜ê·¸ë¬ë‹¤ê³  ì›ƒê¸°ëŠ”ì‚¬ëŒì´ì•¼ì§€ì§œ ë„ˆë¬´í™”ë‚œë‹¹.. ê·¼ë°ì˜¤ë¹ ëŠ”ë§ì„ë˜ ì˜í•´ì„œ ë‚´...</td>
      <td>146</td>
      <td>2</td>
      <td>0.693147</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
      <td>1</td>
      <td>0.693147</td>
      <td>0.000000</td>
      <td>0.693147</td>
      <td>0.693147</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>
<p>3564042 rows Ã— 16 columns</p>

</div>




```python
df.to_csv('SNS_FULL_Dataset(raw_í‘œí˜„ ë³µì¡ë„ ê³„ì‚°).csv', index=False)
```



<br><br>
## 02. ìì—°ì–´ ì „ì²˜ë¦¬
- 3ë²ˆ ì´ìƒ ë°˜ë³µë˜ëŠ” ë™ì¼ ìŒì ˆì€ 3ìŒì ˆë¡œ í‘œì œí™” *ex. ã…‡ã…‡ã…‡ã…‡ã…‡ â‡’ ã…‡ã…‡ã…‡
- ë¬¸ì¥ê¸°í˜¸, íŠ¹ìˆ˜ê¸°í˜¸, ì´ëª¨ì§€, íŠ¹ìˆ˜í°íŠ¸ í‘œí˜„ í‘œì œí™” *ex. â¤ğŸ§¡ğŸ’› â‡’ â™¥ï¸â™¥ï¸â™¥ï¸ / ãƒ²ğ¨›ğŒ…â«¬ â‡’ ã…‹ã…‹ã…‹ã…‹
- ì—°ì†ëœ ë„ì–´ì“°ê¸°(ê³µë°±) í•œ ë²ˆìœ¼ë¡œ ì²˜ë¦¬
- 5ì–´ì ˆ ë¯¸ë§Œ ë§ë­‰ì¹˜ ë°ì´í„° ì œê±° 


```python
import re

raw = df.shape[0]
print('- raw data: {:,}'.format(raw))


#ê²°ì¸¡ê°’ ì œê±°
print('- null data: {:,}'.format(df['contents'].isnull().sum()))
df = df.dropna() 
deleted_null = df.shape[0]



#ì¤‘ë³µ ì œê±°
df = df.drop_duplicates() 
deleted_dup = df.shape[0]
print('- duplicated data: {:,}'.format(deleted_null - deleted_dup))



#ë¬¸ì¥ ê¸¸ì´ ì»¬ëŸ¼ ì¶”ê°€ê°€
df['contents_length'] = df['contents'].apply(lambda x : len(x))
```

    - raw data: 3,564,042
    - null data: 0
    - duplicated data: 0
    
<br><br>
### ì£¼ìš” ì´ëª¨ì§€ ë° íŠ¹ìˆ˜í‘œí˜„ í‘œì œí™”


```python
def emoji_lemmatization(sentence):
    heart_emoji = ['â™¡', 'â™¥', 'â¤', 'â¤ï¸', 'ğŸ§¡', 'ğŸ’›', 'ğŸ’š', 'ğŸ’™', 'ğŸ’œ', 'ğŸ’•'] #
    star_emoji = ['â˜†', 'â˜…', 'â­', 'ğŸŒŸ']
    kkk = ['ğ¨›', 'ğŒ…', 'â«¬', 'ãƒ²', 'åˆ', 'ã‰ª', 'ï½¦']
    Period = ['ã†', 'á†', 'ã†', 'â€¢', 'á†¢']
    quote = ['â€', 'â€˜', 'â€œ']
    ect = ['Â ', 'ã…¤']
    
    for i in range(len(sentence)):
        if sentence[i] in heart_emoji:
            sentence = sentence.replace(sentence[i], 'â™¥')
        elif sentence[i] in star_emoji:
            sentence = sentence.replace(sentence[i], 'â˜…')
        elif sentence[i] in kkk:
            sentence = sentence.replace(sentence[i], 'ã…‹')
        elif sentence[i] in Period:
            sentence = sentence.replace(sentence[i], '.')
        elif sentence[i] in quote:
            sentence = sentence.replace(sentence[i], '\'')
        elif sentence[i] in ect:
            sentence = sentence.replace(sentence[i], ' ')
        else:
            pass
    return(sentence)

def kkk_lemmatization(sentence):
    kkk2 =['ã…‹ê™¼Ìˆ', 'ã…‹Ì‘Ìˆ', 'ã…‹Ì†Ì', 'ã…‹ÌÌˆ', 'ã…‹ÌŠÌˆ', 'ã…‹Ì„Ìˆ', 'ã…‹Ì†Ìˆ', 'ã…‹ÌŠÌˆ', 'ã…‹ÌÌˆ', 'ã…‹Ì†Ì']
    
    for i in range(len(kkk2)):
        if kkk2[i] in sentence:
            sentence =  sentence.replace(kkk2[i], 'ã…‹')
        else:
            pass
    return(sentence)

text_sentence = 'â¤ğŸ§¡ğŸ’›í…ŒìŠ¤íŠ¸ğŸ’šğŸ’™ğŸ’œâ˜†ì…ë‹ˆë‹¤â˜…á†¢â­ ãƒ²ğ¨›ğŒ…â«¬ã…‹Ì„Ìˆã…‹ê™¼Ìˆã…‹Ì†Ìã…‹ÌÌˆã…‹ÌŠÌˆã…‹Ì„Ìˆã…‹ê™¼Ìˆã…‹Ì†Ìã…‹ÌÌˆã…‹ÌŠÌˆ'
text_sentence = emoji_lemmatization(text_sentence)
text_sentence = kkk_lemmatization(text_sentence)
text_sentence
```




    'â™¥â™¥â™¥í…ŒìŠ¤íŠ¸â™¥â™¥â™¥â˜…ì…ë‹ˆë‹¤â˜….â˜… ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹'




```python
df['contents'] = df['contents'].progress_apply(emoji_lemmatization)
df['contents'] = df['contents'].progress_apply(kkk_lemmatization)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3564042/3564042 [05:13<00:00, 11354.18it/s]
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3564042/3564042 [00:10<00:00, 330031.43it/s]
    
<br><br>
### ë°˜ë³µë˜ëŠ” ë™ì¼ ìŒì ˆ í‘œì œí™” ì²˜ë¦¬


```python
def duplicated_spelling_reduction(sentence):
    reduced_spellings = []
    duplicated_num = 1
    for i in range(len(sentence)):
        spelling = sentence[i]
        try:
            previous_spelling = sentence[i-1]
            
        except:
            previous_spelling = 'first_spelling'
        
        if spelling == previous_spelling:
            duplicated_num += 1
        else:
            duplicated_num = 1
            pass
        
        if duplicated_num <= 5:
            reduced_spellings.append(spelling)
        else:
            pass      
        
    reduced_sentence = ''.join(reduced_spellings).replace('   ', ' ').replace('  ', ' ')
    return(reduced_sentence)

text_sentence = '    ì•ˆë…•ì•ˆë…• í—¤í—¤í—¤ ã…‹ã…‹ã…‹ã…‹ ã…ã…ã…ã…ã…ã…ã…ã… ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹'
duplicated_spelling_reduction(text_sentence)
```




    ' ì•ˆë…•ì•ˆë…• í—¤í—¤í—¤ ã…‹ã…‹ã…‹ã…‹ ã…ã…ã…ã…ã… ã…‹ã…‹ã…‹ã…‹ã…‹'




```python
df['contents'] = df['contents'].progress_apply(duplicated_spelling_reduction)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3564042/3564042 [02:02<00:00, 29125.25it/s]
    
<br><br>
### ì—°ì†ëœ ë„ì–´ì“°ê¸° ì²˜ë¦¬/5ì–´ì ˆ ë¯¸ë§Œ ë¬¸ì¥ ì œê±°


```python
#ì—°ì†ë˜ ë„ì–´ì“°ê¸° ì²˜ë¦¬
df['contents'] = df['contents'].apply(lambda x : re.sub(r'\s', ' ', x))  # ì—°ì†ëœ ë„ì–´ì“°ê¸°ë¥¼ ë„ì–´ì“°ê¸° 1ì¹¸ìœ¼ë¡œ ì²˜ë¦¬
mask = df['contents'].isin([' ']) # ë„ì–´ì“°ê¸°ë§Œ ì¡´ì œí•˜ëŠ” ë°ì´í„° ì œê±°
df = df[~mask].reset_index(drop = True) 
deleted_white = df.shape[0]
white = deleted_dup - deleted_white
print("- white space data: {:,}({}%)".format(white, round(white/deleted_dup*100, 2)))
```

    - white space data: 0(0.0%)


```python
#5ì–´ì ˆ ë¯¸ë§Œ ë¬¸ì¥ ì œê±°
df['word_bunch'] = df['contents'].apply(lambda x: len(x.split(' '))) # ì–´ì ˆ ì¹´ìš´íŒ…

cutoff = df.loc[df['word_bunch'] < 5].shape[0] 
df = df.loc[df['word_bunch'] >= 5] 
deleted_cutoff = df.shape[0]
print("- cutoff data: {:,}({}%)".format(cutoff, round(cutoff/deleted_white*100, 2)))
```

    - cutoff data: 61,130(1.72%)



```python
df.to_csv('SNS_FULL_Dataset(í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬).csv', index=False)
```


<br><br>
### í† í¬ë‚˜ì´ì¦ˆ
- konlpy ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Oktë¥¼ ì‚¬ìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ë¦¬
- ë™ì¼ ì² ìì´ë‚˜ ë‹¤ë¥¸ í˜•íƒœì†Œë¥¼ ê°–ëŠ” í† í°ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ 'í† í°(í˜•íƒœì†Œ)' í˜•ì‹ìœ¼ë¡œ í† í¬ë‚˜ì´ì¦ˆì¦ˆ

```python
def pos_tokenizer(sentence):    #POS ê¸°ì¤€ í† í¬ë‚˜ì´ì œì´ì…˜
    import konlpy
    okt = konlpy.tag.Okt() 
    pos_list = okt.pos(str(sentence)) #í† í°/í˜•íƒœì†Œ íŠœí”Œì²˜ë¦¬

    token_arry = []
    for t, m in pos_list:
        token = '{}({})'.format(t, m)
        token_arry.append(token)
    
    token_count = len(token_arry)
    tokenized_sentence = ', '.join(token_arry)
    return tokenized_sentence, token_count
```


```python
# ë°ì´í„°ê°€ ì»¤ì„œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬
df1 = df[:600000]
df2 = df[600000:1200000]
df3 = df[1200000:1800000]
df4 = df[1800000:2400000]
df5 = df[2400000:3000000]
df6 = df[3000000:]

df_li = [df1, df2, df3, df4, df5, df6]

for d in df_li:
    d['tokenized'] = d['contents'].progress_apply(pos_tokenizer)
    d['tokenized'] = d['tokenized'].progress_apply(lambda x : x[0])
    d['token_count'] = d['tokenized'].progress_apply(lambda x: x[1])
    
tokenized_df = pd.concat(df_li, ignore_index=True)
tokenized_df = tokenized_df.drop(columns={'tokenized'})
tokenized_df[['sex', 'age', 'contents', 'tokenized', 'token_count']]
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600000/600000 [36:11<00:00, 276.37it/s]
    ...ìƒëµ...





<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>age</th>
      <th>contents</th>
      <th>tokenized</th>
      <th>token_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>ë‚˜ì§€ê¸ˆë°¥ë¨¸ê±°2ì‹œê°„ê±¸ì–´ì„œ ë²ˆí™”ê°€ì°¾ì•˜ì–´..ã…œã…œ ì‰ã…œã…œ ã…ã…ã…ã…ì˜¤ì¢‹ê² ë„¤ ã…‹ã„±ã…‹ã„±ã„±ã„±ã„±ì•„ë‹ˆ...</td>
      <td>ë‚˜(Noun), ì§€ê¸ˆ(Noun), ë°¥(Noun), ë¨¸ê±°(Verb), 2ì‹œê°„(Numb...</td>
      <td>54</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>í— ã… ã…  ì–¸ë„í˜¸í…”ë“¤ê°€ã… ã…  ì—„ì²­í”¼ê±´í• ì²¸ë° ë‚˜ëŠ”ì¸ë‚«ëŸ¬ìš” ë‚˜ ë‘ì‹œì¶œê·¼ì´ë‹¤ã…ã…ã…ã… í€µìœ¼ë¡œí•œ...</td>
      <td>í—(Verb), ã… ã… (KoreanParticle), ì–¸(Modifier), ë„(No...</td>
      <td>49</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ì—¬ì„±</td>
      <td>20ëŒ€</td>
      <td>í•™ìƒì´ë©´ì¢‹êµ¬! ì™œí˜¼ìë‹¤ë‹ˆëƒê³ ì˜¤..... ì™€ ë‚´ì¹œêµ°í•™êµë‚˜ê° ã…‹ã…‹ã…‹ã…‹ã…‹ ê·¸ë¥´ë„¤ ë§‰ì¡¸ì—…í•œ...</td>
      <td>í•™ìƒ(Noun), ì´(Suffix), ë©´(Josa), ì¢‹êµ¬(Adjective), !...</td>
      <td>25</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ë‚¨ì„±</td>
      <td>20ëŒ€</td>
      <td>í›” í•™ìƒ ì—†ëŠ”ë°...ì£¼ë³€ì—... ì•„ë‹ˆ ë³µí•™í•˜ê³  í•™êµë¥¼ ëª»ê°€ëŠ”ë° ì–´ì¼€ ì¹œêµ¬ê°€ìˆëƒ.. ...</td>
      <td>í›”(Noun), í•™ìƒ(Noun), ì—†ëŠ”ë°(Adjective), ...(Punctua...</td>
      <td>31</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ì—¬ì„±</td>
      <td>30ëŒ€</td>
      <td>ì°¸ë‚˜ ë‚´ê°€ë­ì–¼ë§ˆë‚˜ê·¸ë¬ë‹¤ê³  ì›ƒê¸°ëŠ”ì‚¬ëŒì´ì•¼ì§€ì§œ ë„ˆë¬´í™”ë‚œë‹¹.. ê·¼ë°ì˜¤ë¹ ëŠ”ë§ì„ë˜ ì˜í•´ì„œ ë‚´...</td>
      <td>ì°¸ë‚˜(Noun), ë‚´(Noun), ê°€(Josa), ë­(Noun), ì–¼ë§ˆë‚˜(Noun)...</td>
      <td>63</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
  </tbody>
</table>
<p>3502912 rows Ã— 20 columns</p>
</div>




```python
tokenized_df.to_csv('350ë§Œ_Tokenized.csv', index = False)
```


<br><br>
### í† í° ë”•ì…”ë„ˆë¦¬ êµ¬ì¶• ë° ìŠ¤í¬ë¦¬ë‹
**ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•**
- ë¹ˆë„ 0.0001 ì´ìƒì˜ í† í°ë§Œ ì‚¬ìš©<br>
[í† í° ë”•ì…”ë„ˆë¦¬(êµ¬ê¸€ ìŠ¤í”„ë ˆë“œ)](https://docs.google.com/spreadsheets/d/1wKv4hAfJD_ToORv1Q7QGWsCjYUQTZKHRzZMNCxVPvZ4/edit?usp=sharing)
https://docs.google.com/spreadsheets/d<br>/1wKv4hAfJD_ToORv1Q7QGWsCjYUQTZKHRzZMNCxVPvZ4/edit?usp=sharing

**ìŠ¤í¬ë¦¬ë‹ ê²°ê³¼**
- Okt ë”•ì…”ë„ˆë¦¬ì— í¬í•¨ë˜ì§€ ì•Šì€ í‘œí˜„ì´ íŒŒê´´ë¨<br>
  â‡’ ë‘ë“œëŸ¬ì§€ëŠ” í‘œí˜„ êµì •(ë¶„ë¦¬ëœ í† í° ê²°í•©)

       
- ë¶„ë¥˜ê°€ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶€ì •í™•í•œ í˜•íƒœì†Œ íŒŒì•…
  - ì˜ë¯¸ì ìœ¼ë¡œ í•´ì„ì´ ì–´ë ¤ìš´ Eomi, VerbPrefix, Suffix<br>
    â‡’ ì „/í›„ í† í°ê³¼ ë³‘í•© ë° í˜•íƒœì†Œ ì¬ë¶„ë¥˜ 
  - Modifier,Determiner ë‘ í˜•íƒœì†Œì˜ ê°œë…ì  êµ¬ë¶„ì´ ëª…í™•í•˜ì§€ ì•ŠìŒ<br>
    â‡’ ë‘ í˜•íƒœì†Œ ë³‘í•© 
  - Foreign ë¶„ë¥˜ê°€ ë¶€ì •í™•(íŠ¹ìˆ˜ê¸°í˜¸ ë° ê¸°íƒ€ í‘œí˜„ í¬í•¨)<br>
    â‡’ í˜•íƒœì†Œë³„ ì¬ë¶„ë¥˜ 



```python
#ë©”ëª¨ë¦¬ì•„ì›ƒ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì¼ì • í¬ê¸° ì´í•˜ë¡œ Split
def split_dataframe(dataframe):
    total_length = len(dataframe)
    splited_li = []  # splited_liëŠ” ë°˜ë“œì‹œ ì´ˆê¸°í™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

    if len(dataframe) > 100000:
        split_size = 100000
        num_split = total_length // split_size + 1

        for i in range(num_split):
            start_idx = i * split_size
            end_idx = (i + 1) * split_size
            try:
                splited = dataframe[start_idx:end_idx]
            except:
                splited = dataframe[start_idx:]

            # ì‘ì€ ê·¸ë£¹ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            splited_li.append(splited)
    else:
        splited_li.append(dataframe)
    return splited_li
```



```python
def occur_countor(df):
    total_length = len(df)
    splited_li = split_dataframe(df)

    Occur_dict = {}
    for splited_df in splited_li: 
        merge_list = []
        for index, row in tqdm(splited_df.iterrows(), total=len(splited_df), desc="ì „ì²´ í† í° ë³‘í•©", mininterval=0.1):
            token_list = row['tokenized'].split(', ')
            for token in token_list:
                merge_list.append(token)

        count_list = Counter(merge_list).most_common()

        sort_arry = []
        for d, c in tqdm(count_list, total=len(count_list), desc="ë¹ˆë„ìˆœ ì •ë ¬", mininterval=0.1):
            for i in range(c):
                sort_arry.append(d)

        unique_dic = Counter(sort_arry)
        unique_token = list(unique_dic.keys())
        unique_freq = list(unique_dic.values())
        
        for key, value in zip(unique_token, unique_freq):  # zipì„ ì‚¬ìš©í•˜ì—¬ ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ìˆœíšŒ
            if key in Occur_dict:
                Occur_dict[key] += value
            else:
                Occur_dict[key] = value

    token_li = Occur_dict.keys()
    freq_li = Occur_dict.values()

    Occur_dic = pd.DataFrame({'Token': token_li, 'Token_freq': freq_li})
    Occur_dic = Occur_dic.sort_values(by=['Token_freq'], ascending=False).reset_index(drop=True) 
    Occur_dic['Total_ratio'] = Occur_dic['Token_freq'].apply(lambda x: x/total_length)
    Occur_dic = Occur_dic.loc[Occur_dic['Total_ratio'] >= 0.0001]
    return Occur_dic
```




```python
Occur_dic['word'] = Occur_dic['Token'].apply(lambda x: ''.join(x.split('(')[:-1]))
Occur_dic['pos'] = Occur_dic['Token'].apply(lambda x: x.split('(')[-1].replace(')', ''))
Occur_dic = Occur_dic[['Token', 'word', 'pos', 'Token_freq', 'Total_ratio']]
Occur_dic
```



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Token</th>
      <th>word</th>
      <th>pos</th>
      <th>Token_freq</th>
      <th>Total_ratio</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>?(Punctuation)</td>
      <td>?</td>
      <td>Punctuation</td>
      <td>1865214</td>
      <td>0.532475</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ã…‹ã…‹ã…‹ã…‹ã…‹(KoreanParticle)</td>
      <td>ã…‹ã…‹ã…‹ã…‹ã…‹</td>
      <td>KoreanParticle</td>
      <td>1526323</td>
      <td>0.435730</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ì—(Josa)</td>
      <td>ì—</td>
      <td>Josa</td>
      <td>1307922</td>
      <td>0.373381</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ê°€(Josa)</td>
      <td>ê°€</td>
      <td>Josa</td>
      <td>1142959</td>
      <td>0.326288</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ì´(Josa)</td>
      <td>ì´</td>
      <td>Josa</td>
      <td>913782</td>
      <td>0.260864</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>18142</th>
      <td>ì˜¤ëŠ”êµ¬ë‚˜(Verb)</td>
      <td>ì˜¤ëŠ”êµ¬ë‚˜</td>
      <td>Verb</td>
      <td>351</td>
      <td>0.000100</td>
    </tr>
    <tr>
      <th>18143</th>
      <td>the(Alpha)</td>
      <td>the</td>
      <td>Alpha</td>
      <td>351</td>
      <td>0.000100</td>
    </tr>
    <tr>
      <th>18144</th>
      <td>ë¬¸ìƒ(Noun)</td>
      <td>ë¬¸ìƒ</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
    </tr>
    <tr>
      <th>18145</th>
      <td>ì•š(Noun)</td>
      <td>ì•š</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
    </tr>
    <tr>
      <th>18146</th>
      <td>ì„¼íŠ¸ëŸ´(Noun)</td>
      <td>ì„¼íŠ¸ëŸ´</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
    </tr>
  </tbody>
</table>
<p>18147 rows Ã— 5 columns</p>



```python
Occur_dic.to_csv('350ë§Œ_Occur_dic.csv', index=False)
```




<br><br>
### ì£¼ìš” ì˜¤ë¶„ë¥˜  í˜•íƒœì†Œ ì¬ë¶„ë¥˜


```python
# ì˜¤ë¶„ë¥˜ë˜ëŠ” ì£¼ìš” í† í° í˜•íƒœì†Œ ì¬ë¶„ë¥˜
def fix_foreign(tokenized):
    missing_Adverb = [', í›„ì—(Foreign)', ', í›„(Foreign)', ', ì´ˆì—(Foreign)', ', ì¯¤ì—(Foreign)',
                      ', ì¯¤(Foreign)', ', ì •ë„ì—(Foreign)', ', ì •ë„(Foreign)', ', ì „ì—ëŠ”(Foreign)',
                      ', ì „ì—(Foreign)', ', ì „(Foreign)', ', ì´ìƒ(Foreign)', ', ì•ˆì—(Foreign)',
                      ', ë¶€í„°ëŠ”(Foreign)', ', ë¶€í„°(Foreign)', ', ë°˜ì¯¤(Foreign)', ', ë°˜ì—(Foreign)',
                      ', ë°˜ë¶€í„°(Foreign)', ', ë°˜ê¹Œì§€(Foreign)', ', ë°˜(Foreign)', ', ë°–ì—(Foreign)',
                      ', ë§ì—(Foreign)', ', ë§(Foreign)', ', ë§Œì—(Foreign)', ', ë§Œ(Foreign)',
                      ', ë§ˆë‹¤(Foreign)', ', ë’¤ì—(Foreign)', ', ë’¤(Foreign)', ', ë™ì•ˆ(Foreign)',
                    
                      ', ë„˜ì–´ì„œ(Foreign)', ', ë„˜ê²Œ(Foreign)', ', ë‚¨ìŒ(Foreign)', ', êº¼(Foreign)',
                      ', ê¹Œì§„ë°(Foreign)', ', ê¹Œì§„(Foreign)', ', ê¹Œì§€ì•¼(Foreign)', ', ê¹Œì§€ë§Œ(Foreign)',
                      ', ê¹Œì§€ëŠ”(Foreign)', ', ê¹Œì§€(Foreign)', ', ê°„(Foreign)', ', ì´í›„(Foreign)',
                      ', ì´í›„ì—(Foreign)', ', ë‚´ë¡œ(Foreign)', ', ê²½(Foreign)', ', ë§ê¹Œì§€(Foreign)',
                      ', ì „ê¹Œì§€(Foreign)', ', ì¤‘ì—(Foreign)', ', ì¦˜(Foreign)', ', ë‚´ë‚´(Foreign)',
                      ', ì •ë„ëŠ”(Foreign)', ', ì´ˆ(Foreign)', ', ì–¼ë§ˆ(Foreign)', ', ì •ë„ë©´(Foreign)',
                      ', ì´ë‚´(Foreign)', ', ë‚´(Foreign)', ', ê°„ì˜(Foreign)', ', ê°„ì€(Foreign)',
                      ', ì•½(Foreign)', ', ë³´ë‹¤(Foreign)', ', ì „ì—”(Foreign)', ', ê¹Œì§€ë‹ˆê¹Œ(Foreign)',
                      ', ì •ë„ë§Œ(Foreign)', ', ì‚¬ì´ì—(Foreign)', ', ë’¤ë©´(Foreign)', ', ì‹(Foreign)'
                     ]

    
    missing_Josa = [', ì´ë©´(Foreign)', ', ì´ë‘(Foreign)', ', ì´ë¼ì„œ(Foreign)', ', ì´ë¼ë„(Foreign)',
                    ', ì´ë¼ê³ (Foreign)', ', ì´ë¼(Foreign)', ', ì´ë‚˜(Foreign)', ', ì´ê³ (Foreign)',
                    ', ì´(Foreign)', ', ì˜(Foreign)', ', ì„(Foreign)', ', ì€(Foreign)',
                    ', ìœ¼ë¡œ(Foreign)', ', ì—”(Foreign)', ', ì—ì„œ(Foreign)', ', ì—ë„(Foreign)',
                    ', ì—ëŠ”(Foreign)', ', ì—(Foreign)', ', ë©´(Foreign)', ', ë¡œ(Foreign)',
                    ', ëŠ”(Foreign)', ', ë‚˜(Foreign)', ', ê°€(Foreign)', ', ë¼ê³ (Foreign)'
                    ', ì´ë¼ëŠ”(Foreign)', ', ê»˜(Foreign)', ', ë¥¼(Foreign)', ', ê»˜(Foreign)',
                    ', ê³ (Foreign)'
                   ]
    
    
    missing_Verb = [', í•˜ë©´(Foreign)', ', í•˜ê³ (Foreign)', ', ì£¼ê³ (Foreign)', ', ë˜ë©´(Foreign)',
                    ', ëœ(Foreign)', ', í•´ì„œ(Foreign)', ', ì´ë©°(Foreign)'
                   ]
    
    
    
    missing_Suffix = [', ì–´ì¹˜(Foreign)', ', ì¹˜(Foreign)', ', ì°¨(Foreign)', ', ì§¸(Foreign)',
                    ', ì§œë¦¬(Foreign)', ', ì”©(Foreign)', ', ìƒ(Foreign)', ', ëª…(Foreign)',
                    ', ëŒ€(Foreign)', ', ë‹¬ì—(Foreign)', ', ë‹¬(Foreign)', ', ë„ì—(Foreign)',
                    ', ë„(Foreign)', ', ë‚ (Foreign)', ', ê¸‰(Foreign)', ', ì–¸(Foreign)',
                    ', ê°œ(Foreign)', 'ìš©(Foreign)', 'í˜•(Foreign)', 'ëŒ€ì˜(Foreign)',
                    ', ê°€ëŸ‰(Foreign)', ', ê¸°(Foreign)', ', ë¶€(Foreign)', ', ê¸‰ì˜(Foreign)',
                    ', ì œ(Foreign)', ', ë‹¹(Foreign)', ', ê°œì˜(Foreign)', ', ê¶Œ(Foreign)',
                    ', ë¶ˆ(Foreign)', ', ë•Œ(Foreign)', ', ì§œë¦¬ê°€(Foreign)'
                     ]
    
    
    missing_Noun = [', ë„ì°©(Foreign)', ', í‡´ê·¼(Foreign)', ', ì»·(Foreign)', ', ê°(Foreign)',
                    ', ê±¸ë¦¼(Foreign)', ', ì¶œë°œ(Foreign)', ', ë¦¬ì¦ˆ(Foreign)', ', ë„ì„œ(Foreign)',
                    ', í€ë”©(Foreign)', ', ê²°ì œ(Foreign)', ', ë°œì†¡(Foreign)', ', ë°°ì†¡(Foreign)',
                    ', ê¸°ì¤€(Foreign)', ', ê·œì •(Foreign)', ', ì™€ë””ì¦ˆ(Foreign)', ', ë¬´ìƒ(Foreign)',
                    ', ë¦¬ì›Œë“œ(Foreign)', ', ì¶œê·¼(Foreign)', ', ê±°ë¦¬(Foreign)'
                   ]
    
    
    missing_Eomi = [', ì…ë‹ˆë‹¤(Foreign)', ', ì„(Foreign)', ', ì¸ë””(Foreign)', ', ì¸ë°(Foreign)',
                    ', ì¸ê°€(Foreign)', ', ì´ì§€(Foreign)', ', ì´ìš”(Foreign)', ', ì´ì—¬(Foreign)',
                    ', ì´ì•¼(Foreign)', ', ì´ë˜(Foreign)', ', ì´ë¼ë‹ˆ(Foreign)', ', ì´ë‹¤(Foreign)',
                    ', ì´ë‹ˆê¹Œ(Foreign)', ', ì´ë„¤(Foreign)', ', ìš”(Foreign)', ', ì•¼(Foreign)',
                    ', ë¼(Foreign)', ', ë‹¤(Foreign)', ', ë„¤(Foreign)', ', ì´ì–Œ(Foreign)',
                    ', ì´ë‹ˆ(Foreign)', ', ì´ë¼ëŠ”ë°(Foreign)', ', ëŒ€ì—(Foreign)', ', ë‹ˆê¹Œ(Foreign)',
                    ', ì´ë„¹(Foreign)', ', ì´ë˜ë°(Foreign)', ', ì§€(Foreign)', ', ì—ë‚˜(Foreign)',
                    ', í•¨(Foreign)', ', ì´ì—ˆëŠ”ë°(Foreign)', ', ì´ê±°ë“ (Foreign)', ', ì´ì—ˆë‚˜(Foreign)',
                    ', ì´ì—ìš”(Foreign)'
                   ]
    
    missing_list = [missing_Adverb, missing_Josa, missing_Verb, missing_Suffix, missing_Noun, missing_Eomi]
    fit_list = ['(Adverb)', '(Josa)', '(Verb)', '(Suffix)', '(Noun)', '(Eomi)']
    for i in range(len(missing_list)):
        missing = missing_list[i]
        fit = fit_list[i]
        for n in range(len(missing)):
            if missing[n] in tokenized:
                fixed_pos = missing[n].replace('(Foreign)', fit)
                tokenized = tokenized.replace(missing[n], fixed_pos)
            else:
                pass
    return(tokenized)


def fix_suffix_Noun(tokenized):
    missing = [', ì˜¤ë¹ (Suffix)', ', ì–¸ë‹ˆ(Suffix)', ', ëˆ„ë‚˜(Suffix)', ', í˜•(Suffix)',
               ', ì—„ë§ˆ(Suffix)', ', ì•„ë¹ (Suffix)', 
               ', User(Alpha)', ', UserUser(Alpha)', ', UserUserUser(Alpha)']
    
    for i in range(len(missing)):
        if missing[i] in tokenized:
            fixed_pos = missing[i].replace('(Suffix)', '(Noun)')
            tokenized = tokenized.replace(missing[i], fixed_pos)
        elif missing[i] in tokenized:
            fixed_pos = missing[i].replace('(Alpha)', '(Noun)')
            tokenized = tokenized.replace(missing[i], fixed_pos)
            
        else:
            pass
    return(tokenized)


def fix_suffix_Noun2(tokenized):
    missing = [', ë‹˜(Suffix)', ', ë¶„ë“¤(Suffix)']   
    for i in range(len(missing)):
        if missing[i] in tokenized:
            fixed_pos = missing[i].replace('(Suffix)', '(Noun)')
            tokenized = tokenized.replace(missing[i], fixed_pos)
        else:
            pass
    return(tokenized)
```


```python
# í•´ì„ì´ ëª¨í˜¸í•˜ê±°ë‚˜ íŒŒê´´ëœ í† í° ê²°í—™ ë° í˜•íƒœì†Œ ì¬ë¶„ë¥˜
def restore_pos(tokenized_sentence):
    import re
    import numpy as np
    global re_sentence
    global pos_label
    
    token_list = tokenized_sentence.split(', ')

    for i in range(len(token_list)-1):
        ft_index = i
        bt_index = i+1
        
        f_token = token_list[ft_index]
        b_token = token_list[bt_index]
        
        if '(Verb)' in f_token and '(Eomi)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Verb)' 
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
        
        if '(Adjective)' in f_token and '(Eomi)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Adjective)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
        
        
        if '(VerbPrefix)' in f_token and '(VerbPrefix)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Noun)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
    
    
        if '(VerbPrefix)' in f_token and '(Verb)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Verb)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
        
        if '(VerbPrefix)' in f_token and '(Adjective)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Adjective)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
        
        
        if '(Verb)' in f_token and '(VerbPrefix)' in b_token :
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Noun)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
        
        if 'í•˜(Suffix)' in f_token and '(Josa)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            f_token = assemble_token + '(Verb)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token 
        
        
        if '(Noun)' in f_token and '(Suffix)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            f_token = assemble_token + '(Noun)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
        if '(Adjective)' in f_token and '(Suffix)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            f_token = assemble_token + '(Adjective)' 
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
        if '(Alpha)' in f_token and '(Suffix)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            f_token = assemble_token + '(Noun)' 
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
             
        
        if '(PreEomi)' in f_token and '(Eomi)' in b_token:
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Eomi)' 
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
    
        
        if '(Modifier)' in f_token and '(Modifier)' in b_token :           
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Noun)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
            
        if '(Modifier)' in f_token and '(Noun)' in b_token :          
            assemble_token = re.sub(pattern = r'\([^)]*\)', repl='', string = str(f_token + b_token))
            
            f_token = assemble_token + '(Noun)'
            b_token = ''

            token_list[ft_index] = f_token
            token_list[bt_index] = b_token
      

        
    ws_indexs = np.where(np.array(token_list) == '')[0].tolist()
    w = 0
    for ws in ws_indexs:
        ws -= w
        del token_list[ws]
        w += 1
    
    re_sentence = ', '.join(token_list)

    return(re_sentence)
```


```python
tokenized_df['tokenized'] = tokenized_df['tokenized'].progress_apply(fix_foreign)
tokenized_df['tokenized'] = tokenized_df['tokenized'].progress_apply(fix_suffix_Noun)
tokenized_df['tokenized'] = tokenized_df['tokenized'].progress_apply(restore_pos)
tokenized_df['tokenized'] = tokenized_df['tokenized'].progress_apply(restore_pos)
tokenized_df['tokenized'] = tokenized_df['tokenized'].progress_apply(fix_suffix_Noun2)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3502912/3502912 [03:24<00:00, 17136.46it/s]
    ...ìƒëµ...

```python
tokenized_df.to_csv('350ë§Œ_Tokenized.csv(pos êµì •)', index = False)
```




<br><br>
## 03. ìƒëŒ€ í¸í–¥ë„/ìƒëŒ€ ë¹ˆì¶œë„ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•
### í˜•íƒœì†Œ ì¬ë¶„ë¥˜í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±


```python
#ë©”ëª¨ë¦¬ì•„ì›ƒ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì¼ì • í¬ê¸° ì´í•˜ë¡œ Split
def split_dataframe(dataframe):
    total_length = len(dataframe)
    splited_li = []  # splited_liëŠ” ë°˜ë“œì‹œ ì´ˆê¸°í™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

    if len(dataframe) > 100000:
        split_size = 100000
        num_split = total_length // split_size + 1

        for i in range(num_split):
            start_idx = i * split_size
            end_idx = (i + 1) * split_size
            try:
                splited = dataframe[start_idx:end_idx]
            except:
                splited = dataframe[start_idx:]

            # ì‘ì€ ê·¸ë£¹ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            splited_li.append(splited)
    else:
        splited_li.append(dataframe)
    return splited_li
```


```python
def occur_countor(df):
    total_length = len(df)
    splited_li = split_dataframe(df)

    Occur_dict = {}
    for splited_df in splited_li: 
        merge_list = []
        for index, row in tqdm(splited_df.iterrows(), total=len(splited_df), desc="ì „ì²´ í† í° ë³‘í•©", mininterval=0.1):
            token_list = row['tokenized'].split(', ')
            for token in token_list:
                merge_list.append(token)

        count_list = Counter(merge_list).most_common()

        sort_arry = []
        for d, c in tqdm(count_list, total=len(count_list), desc="ë¹ˆë„ìˆœ ì •ë ¬", mininterval=0.1):
            for i in range(c):
                sort_arry.append(d)

        unique_dic = Counter(sort_arry)
        unique_token = list(unique_dic.keys())
        unique_freq = list(unique_dic.values())
        
        for key, value in zip(unique_token, unique_freq):  # zipì„ ì‚¬ìš©í•˜ì—¬ ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ìˆœíšŒ
            if key in Occur_dict:
                Occur_dict[key] += value
            else:
                Occur_dict[key] = value

    token_li = Occur_dict.keys()
    freq_li = Occur_dict.values()

    Occur_dic = pd.DataFrame({'Token': token_li, 'Token_freq': freq_li})
    Occur_dic = Occur_dic.sort_values(by=['Token_freq'], ascending=False).reset_index(drop=True) 
    Occur_dic['Total_ratio'] = Occur_dic['Token_freq'].apply(lambda x: x/total_length)
    Occur_dic = Occur_dic.loc[Occur_dic['Total_ratio'] >= 0.0001]
    return Occur_dic
```


```python
Occur_dic = occur_countor(tokenized_df)
Occur_dic
```

    ì „ì²´ í† í° ë³‘í•©: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:04<00:00, 23878.38it/s]
    ë¹ˆë„ìˆœ ì •ë ¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212146/212146 [00:00<00:00, 502799.24it/s]
    ...ìƒëµ...




```python
Occur_dic.to_csv('350ë§Œ_Occur_dic(pos êµì •).csv', index=False)
```


<br><br>
### í† í°ì˜ ìƒëŒ€ ë¹ˆì¶œë„ ë° ìƒëŒ€ í¸í–¥ë„ ê³„ì‚°


```python
def feature_extractor(Data, Occur_dict , col, Feature_list):
    token_dic = Occur_dict['Token'].to_list()
    
    merged_df = pd.DataFrame()
    merged_df['Token'] = Occur_dict['Token']
    for feature in Feature_list:
        feature_df = Data.loc[Data[f'{col}'] == feature]

        Token_Freq_in_feature_Documents = [0 for i in range(len(token_dic))] # íŠ¹ì„± ë¬¸ì„œì—ì„œ íŠ¹ì • í† í°ì´ ì¶œí˜„í•œ íšŸìˆ˜ 
        feature_Documents_Freq_by_Token = [0 for i in range(len(token_dic))] # íŠ¹ì • í† í°ì´ ì¶œí˜„í•œ íŠ¹ì„± ë¬¸ì„œì˜ ìˆ˜
        
        total_length = len(feature_df)
        splited_li = split_dataframe(feature_df)

        for batch, splited_df in enumerate(splited_li):  
            for index, row in tqdm(splited_df.iterrows(), total=len(splited_df), desc=f"{feature}_{batch+1}/{len(splited_li)}", mininterval=0.1):
                token_list = row['tokenized'].split(', ') # ë¬¸ì„œ í† í¬ë‚˜ì´ì§•  

                for token in token_list: # ë¬¸ì„œì—ì„œ íŠ¹ì • í† í°ì´ ì¶œí˜„í•œ ìˆ˜
                    if token in token_dic:
                        idx = token_dic.index(token)
                        Token_Freq_in_feature_Documents[idx] += 1

                    else:
                        pass

                uniqe_token_list = list(set(token_list)) # í† í¬ë‚˜ì´ì§• ëœ ë¬¸ì¥ì—ì„œ ê³ ìœ í•œ í† í°ë§Œ ë‚¨ê¹€, í•œ ë¬¸ì„œì—ì„œ ë™ì¼í•œ í† í°ì´ ì—¬ëŸ¬ë²ˆ ì¹´ìš´íŒ… ë˜ëŠ” ê²ƒì„ ë°©ì§€
                for uniqe_token in uniqe_token_list: # íŠ¹ì • í† í°ì´ ì¶œí˜„í•œ ë¬¸ì„œì˜ ìˆ˜
                    if uniqe_token in token_dic: 
                        idx = token_dic.index(uniqe_token)
                        feature_Documents_Freq_by_Token[idx] += 1
                    else:
                        pass  

        Token_Freq_in_feature_Documents_list = []
        for f in Token_Freq_in_feature_Documents:
            if f == 0:
                f = 0.1
            else:
                pass
            Token_Freq_in_feature_Documents_list.append(f)

        feature_Documents_Freq_by_Token_list = []
        for b in feature_Documents_Freq_by_Token:
            if b == 0:
                b = 0.1
            else:
                pass
            feature_Documents_Freq_by_Token_list.append(b)
        
        feature_df = pd.DataFrame()
        feature_df['Token'] = Occur_dic['Token']
        
        feature_Documents_num = len(feature_df)
        feature_df[f'Freq_{feature}'] = Token_Freq_in_feature_Documents_list #ì„±ë³„ì— ë”°ë¥¸ í† í° ì¶œí˜„ ë¹ˆë„
        feature_df[f'Freq_ratio_{feature}'] = np.array(feature_df[f'Freq_{feature}'])/feature_Documents_num #ì„±ë³„ì— ë”°ë¥¸ í† í° ì¶œí˜„ìœ¨
        feature_df[f'Bias_{feature}'] = feature_Documents_Freq_by_Token_list #íŠ¹ì • í† í° ì¶œí˜„ ì‹œ, ì‘ì„±ì ì„±ë³„ì´ 'sex'ì¸ ë¹ˆë„
        feature_df[f'Bias_ratio_{feature}'] = np.array(feature_df[f'Bias_{feature}'])/feature_Documents_num #íŠ¹ì • í† í° ì¶œí˜„ ì‹œ, ì‘ì„±ì ì„±ë³„ì´ 'sex'ì¸ ë¹„ìœ¨
    
        merged_df = pd.merge(merged_df, feature_df, how='left', on='Token')
    return merged_df 
```


```python
gender_class = ['ë‚¨ì„±', 'ì—¬ì„±']
gender_dic = feature_extractor(df, Occur_dic, 'sex', gender_class)
```

    ë‚¨ì„±_1/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [11:33<00:00, 144.14it/s]
    ...ìƒëµ...




```python
age_class = ['20ëŒ€ ë¯¸ë§Œ', '20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€', '70ëŒ€ ì´ìƒ'] #
age_dic = feature_extractor(df, Occur_dic, 'age', age_class)
```

    20ëŒ€ ë¯¸ë§Œ_1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [11:40<00:00, 142.79it/s]
    ...ìƒëµ...




```python
other_dic = pd.DataFrame()
other_dic['Token'] = age_dic['Token']

other10_Douments_num = len(df.loc[df['age'] != '20ëŒ€ ë¯¸ë§Œ'])
other_dic['Freq_ratio_other10'] = (age_dic['Freq_20ëŒ€'] + age_dic['Freq_30ëŒ€'] + age_dic['Freq_40ëŒ€'] + age_dic['Freq_50ëŒ€'] + age_dic['Freq_60ëŒ€'] + age_dic['Freq_70ëŒ€ ì´ìƒ']) / other10_Douments_num # 
other_dic['Bias_ratio_other10'] = (age_dic['Bias_20ëŒ€'] + age_dic['Bias_30ëŒ€'] + age_dic['Bias_40ëŒ€'] + age_dic['Bias_50ëŒ€'] + age_dic['Bias_60ëŒ€'] + age_dic['Bias_70ëŒ€ ì´ìƒ']) / other10_Douments_num # 

other20_Douments_num = len(df.loc[df['age'] != '20ëŒ€'])
other_dic['Freq_ratio_other20'] = (age_dic['Freq_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Freq_30ëŒ€'] + age_dic['Freq_40ëŒ€'] + age_dic['Freq_50ëŒ€'] + age_dic['Freq_60ëŒ€'] + age_dic['Freq_70ëŒ€ ì´ìƒ']) / other20_Douments_num # 
other_dic['Bias_ratio_other20'] = (age_dic['Bias_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Bias_30ëŒ€'] + age_dic['Bias_40ëŒ€'] + age_dic['Bias_50ëŒ€'] + age_dic['Bias_60ëŒ€'] + age_dic['Bias_70ëŒ€ ì´ìƒ']) / other20_Douments_num # 

other30_Douments_num = len(df.loc[df['age'] != '30ëŒ€'])
other_dic['Freq_ratio_other30'] = (age_dic['Freq_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Freq_20ëŒ€'] + age_dic['Freq_40ëŒ€'] + age_dic['Freq_50ëŒ€'] + age_dic['Freq_60ëŒ€'] + age_dic['Freq_70ëŒ€ ì´ìƒ']) / other30_Douments_num # 
other_dic['Bias_ratio_other30'] = (age_dic['Bias_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Bias_20ëŒ€'] + age_dic['Bias_40ëŒ€'] + age_dic['Bias_50ëŒ€'] + age_dic['Bias_60ëŒ€'] + age_dic['Bias_70ëŒ€ ì´ìƒ']) / other30_Douments_num # 

other40_Douments_num = len(df.loc[df['age'] != '40ëŒ€'])
other_dic['Freq_ratio_other40'] = (age_dic['Freq_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Freq_20ëŒ€'] + age_dic['Freq_30ëŒ€'] + age_dic['Freq_50ëŒ€'] + age_dic['Freq_60ëŒ€'] + age_dic['Freq_70ëŒ€ ì´ìƒ']) / other40_Douments_num # 
other_dic['Bias_ratio_other40'] = (age_dic['Bias_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Bias_20ëŒ€'] + age_dic['Bias_30ëŒ€'] + age_dic['Bias_50ëŒ€'] + age_dic['Bias_60ëŒ€'] + age_dic['Bias_70ëŒ€ ì´ìƒ']) / other40_Douments_num # 

other50_Douments_num = len(df.loc[df['age'] != '50ëŒ€'])
other_dic['Freq_ratio_other50'] = (age_dic['Freq_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Freq_20ëŒ€'] + age_dic['Freq_30ëŒ€'] + age_dic['Freq_40ëŒ€'] + age_dic['Freq_60ëŒ€'] + age_dic['Freq_70ëŒ€ ì´ìƒ']) / other50_Douments_num # 
other_dic['Bias_ratio_other50'] = (age_dic['Bias_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Bias_20ëŒ€'] + age_dic['Bias_30ëŒ€'] + age_dic['Bias_40ëŒ€'] + age_dic['Bias_60ëŒ€'] + age_dic['Bias_70ëŒ€ ì´ìƒ']) / other50_Douments_num # 

other60_Douments_num = len(df.loc[df['age'] != '60ëŒ€'])
other_dic['Freq_ratio_other60'] = (age_dic['Freq_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Freq_20ëŒ€'] + age_dic['Freq_30ëŒ€'] + age_dic['Freq_40ëŒ€'] + age_dic['Freq_50ëŒ€'] + age_dic['Freq_70ëŒ€ ì´ìƒ']) / other60_Douments_num
other_dic['Bias_ratio_other60'] = (age_dic['Bias_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Bias_20ëŒ€'] + age_dic['Bias_30ëŒ€'] + age_dic['Bias_40ëŒ€'] + age_dic['Bias_50ëŒ€'] + age_dic['Bias_70ëŒ€ ì´ìƒ']) / other60_Douments_num

other70_Douments_num = len(df.loc[df['age'] != '70ëŒ€ ì´ìƒ'])
other_dic['Freq_ratio_other70'] = (age_dic['Freq_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Freq_20ëŒ€'] + age_dic['Freq_30ëŒ€'] + age_dic['Freq_40ëŒ€'] + age_dic['Freq_50ëŒ€'] + age_dic['Freq_60ëŒ€']) / other70_Douments_num
other_dic['Bias_ratio_other70'] = (age_dic['Bias_20ëŒ€ ë¯¸ë§Œ'] + age_dic['Bias_20ëŒ€'] + age_dic['Bias_30ëŒ€'] + age_dic['Bias_40ëŒ€'] + age_dic['Bias_50ëŒ€'] + age_dic['Bias_60ëŒ€']) / other70_Douments_num
```



```python
relative_dic = pd.DataFrame()
relative_dic['Token'] = Occur_dic['Token']
relative_dic['Token_freq'] = Occur_dic['Token_freq']
relative_dic['Total_ratio'] = Occur_dic['Total_ratio']

#ì„±ë³„ì— ëŒ€í•œ 
relative_dic['Gender_Freq'] = np.log(gender_dic['Freq_ratio_ë‚¨ì„±']/gender_dic['Freq_ratio_ì—¬ì„±']) # ì„±ë³„ ìƒëŒ€ ë¹ˆì¶œë„
relative_dic['Gender_Bias'] = np.log(gender_dic['Bias_ratio_ë‚¨ì„±']/gender_dic['Bias_ratio_ì—¬ì„±']) # ì„±ë³„ ìƒëŒ€ í¸í–¥ë„


#ì—°ë ¹ì— ëŒ€í•œ
relative_dic['A10_Freq'] = np.log(age_dic['Freq_ratio_20ëŒ€ ë¯¸ë§Œ']/other_dic['Freq_ratio_other10']) 
relative_dic['A10_Bias'] = np.log(age_dic['Bias_ratio_20ëŒ€ ë¯¸ë§Œ']/other_dic['Bias_ratio_other10']) 

relative_dic['A20_Freq'] = np.log(age_dic['Freq_ratio_20ëŒ€']/other_dic['Freq_ratio_other20']) 
relative_dic['A20_Bias'] = np.log(age_dic['Bias_ratio_20ëŒ€']/other_dic['Bias_ratio_other20']) 

relative_dic['A30_Freq'] = np.log(age_dic['Freq_ratio_30ëŒ€']/other_dic['Freq_ratio_other30']) 
relative_dic['A30_Bias'] = np.log(age_dic['Bias_ratio_30ëŒ€']/other_dic['Bias_ratio_other30'])

relative_dic['A40_Freq'] = np.log(age_dic['Freq_ratio_40ëŒ€']/other_dic['Freq_ratio_other40']) 
relative_dic['A40_Bias'] = np.log(age_dic['Bias_ratio_40ëŒ€']/other_dic['Bias_ratio_other40'])

relative_dic['A50_Freq'] = np.log(age_dic['Freq_ratio_50ëŒ€']/other_dic['Freq_ratio_other50']) 
relative_dic['A50_Bias'] = np.log(age_dic['Bias_ratio_50ëŒ€']/other_dic['Bias_ratio_other50'])

relative_dic['A60_Freq'] = np.log(age_dic['Freq_ratio_60ëŒ€']/other_dic['Freq_ratio_other60']) 
relative_dic['A60_Bias'] = np.log(age_dic['Bias_ratio_60ëŒ€']/other_dic['Bias_ratio_other60'])

relative_dic['A70_Freq'] = np.log(age_dic['Freq_ratio_70ëŒ€ ì´ìƒ']/other_dic['Freq_ratio_other70']) 
relative_dic['A70_Bias'] = np.log(age_dic['Bias_ratio_70ëŒ€ ì´ìƒ']/other_dic['Bias_ratio_other70'])

relative_dic
```




```python
def word_seperator_in_list(token):    
    idx_arry = []
    idx = 0
    for t in str(token):
        if t == '(':
            idx_arry.append(idx)
        else:
            pass
        idx += 1

    s_index = idx_arry[-1]
    w = token[:s_index]
    return(w)

def pos_seperator_in_list(token):    
    idx_arry = []
    idx = 0
    for t in str(token):
        if t == '(':
            idx_arry.append(idx)
        else:
            pass
        idx += 1

    s_index = idx_arry[-1]
    p = token[s_index+1:-1]
    return(p)
```


```python
relative_dic['word'] = relative_dic['Token'].apply(lambda x: ''.join(x.split('(')[:-1]))
relative_dic['pos'] = relative_dic['Token'].apply(lambda x: x.split('(')[-1].replace(')', ''))

relative_dic = relative_dic[['Token', 'word', 'pos', 'Token_freq', 'Total_ratio',
                             'Gender_Freq', 'Gender_Bias', 
                             'A10_Freq', 'A10_Bias', 'A20_Freq', 'A20_Bias', 
                             'A30_Freq', 'A30_Bias', 'A40_Freq', 'A40_Bias',
                             'A50_Freq', 'A50_Bias', 'A60_Freq', 'A60_Bias', 'A70_Freq', 'A70_Bias']] 

relative_dic
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Token</th>
      <th>word</th>
      <th>pos</th>
      <th>Token_freq</th>
      <th>Total_ratio</th>
      <th>Gender_Freq</th>
      <th>Gender_Bias</th>
      <th>A10_Freq</th>
      <th>A10_Bias</th>
      <th>A20_Freq</th>
      <th>...</th>
      <th>A30_Freq</th>
      <th>A30_Bias</th>
      <th>A40_Freq</th>
      <th>A40_Bias</th>
      <th>A50_Freq</th>
      <th>A50_Bias</th>
      <th>A60_Freq</th>
      <th>A60_Bias</th>
      <th>A70_Freq</th>
      <th>A70_Bias</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>?(Punctuation)</td>
      <td>?</td>
      <td>Punctuation</td>
      <td>1865214</td>
      <td>0.532475</td>
      <td>-1.092652</td>
      <td>-1.125282</td>
      <td>1.582501</td>
      <td>1.639460</td>
      <td>4.709142</td>
      <td>...</td>
      <td>3.807751</td>
      <td>3.728915</td>
      <td>1.845276</td>
      <td>1.620177</td>
      <td>1.337092</td>
      <td>1.128453</td>
      <td>-0.383700</td>
      <td>-0.437217</td>
      <td>-2.976230</td>
      <td>-2.885823</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ã…‹ã…‹ã…‹ã…‹ã…‹(KoreanParticle)</td>
      <td>ã…‹ã…‹ã…‹ã…‹ã…‹</td>
      <td>KoreanParticle</td>
      <td>1526323</td>
      <td>0.435730</td>
      <td>-1.762353</td>
      <td>-1.680824</td>
      <td>1.680877</td>
      <td>1.665422</td>
      <td>4.995337</td>
      <td>...</td>
      <td>3.763199</td>
      <td>3.676334</td>
      <td>-0.645103</td>
      <td>-0.455351</td>
      <td>-1.662415</td>
      <td>-1.496536</td>
      <td>-3.916414</td>
      <td>-3.791524</td>
      <td>-11.291175</td>
      <td>-10.853299</td>
    </tr>
    <tr>
      <th>2</th>
      <td>ì—(Josa)</td>
      <td>ì—</td>
      <td>Josa</td>
      <td>1376727</td>
      <td>0.393024</td>
      <td>-1.354588</td>
      <td>-1.337686</td>
      <td>1.516475</td>
      <td>1.556426</td>
      <td>4.815396</td>
      <td>...</td>
      <td>3.781199</td>
      <td>3.756026</td>
      <td>1.484062</td>
      <td>1.430571</td>
      <td>0.922323</td>
      <td>0.894123</td>
      <td>-0.292875</td>
      <td>-0.305520</td>
      <td>-3.032982</td>
      <td>-3.025299</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ê°€(Josa)</td>
      <td>ê°€</td>
      <td>Josa</td>
      <td>1144262</td>
      <td>0.326660</td>
      <td>-1.330090</td>
      <td>-1.316493</td>
      <td>1.611810</td>
      <td>1.626960</td>
      <td>4.836498</td>
      <td>...</td>
      <td>3.745612</td>
      <td>3.730188</td>
      <td>1.468008</td>
      <td>1.432860</td>
      <td>0.856439</td>
      <td>0.860018</td>
      <td>-0.144169</td>
      <td>-0.195058</td>
      <td>-2.928766</td>
      <td>-2.935806</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ì´(Josa)</td>
      <td>ì´</td>
      <td>Josa</td>
      <td>919870</td>
      <td>0.262602</td>
      <td>-1.318207</td>
      <td>-1.311745</td>
      <td>1.399605</td>
      <td>1.440919</td>
      <td>4.757490</td>
      <td>...</td>
      <td>3.840997</td>
      <td>3.813802</td>
      <td>1.523747</td>
      <td>1.475925</td>
      <td>1.019233</td>
      <td>0.979362</td>
      <td>0.074839</td>
      <td>-0.029539</td>
      <td>-2.691890</td>
      <td>-2.770597</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>18356</th>
      <td>ì•”ë°(Noun)</td>
      <td>ì•”ë°</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
      <td>-1.481605</td>
      <td>-1.486564</td>
      <td>1.325666</td>
      <td>1.337359</td>
      <td>4.437628</td>
      <td>...</td>
      <td>4.217711</td>
      <td>4.220826</td>
      <td>1.701743</td>
      <td>1.713539</td>
      <td>0.075802</td>
      <td>0.087326</td>
      <td>0.492802</td>
      <td>0.095993</td>
      <td>-2.913589</td>
      <td>-2.902128</td>
    </tr>
    <tr>
      <th>18357</th>
      <td>ë¼ë“œ(Noun)</td>
      <td>ë¼ë“œ</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
      <td>-1.094817</td>
      <td>-1.090738</td>
      <td>0.757370</td>
      <td>0.795543</td>
      <td>4.912486</td>
      <td>...</td>
      <td>3.725778</td>
      <td>3.774007</td>
      <td>1.336311</td>
      <td>1.374823</td>
      <td>0.774695</td>
      <td>0.812868</td>
      <td>0.783361</td>
      <td>0.821534</td>
      <td>-2.913589</td>
      <td>-2.875849</td>
    </tr>
    <tr>
      <th>18358</th>
      <td>ì•¡ìƒ(Noun)</td>
      <td>ì•¡ìƒ</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
      <td>-0.445995</td>
      <td>-0.421465</td>
      <td>0.466524</td>
      <td>0.173400</td>
      <td>5.630803</td>
      <td>...</td>
      <td>3.085743</td>
      <td>3.107728</td>
      <td>0.767727</td>
      <td>0.184045</td>
      <td>0.075516</td>
      <td>0.190725</td>
      <td>-2.916976</td>
      <td>-2.802428</td>
      <td>-2.913874</td>
      <td>-2.799325</td>
    </tr>
    <tr>
      <th>18359</th>
      <td>ìŠ¤í‹°(Noun)</td>
      <td>ìŠ¤í‹°</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
      <td>-1.704748</td>
      <td>-1.638637</td>
      <td>0.983399</td>
      <td>1.039861</td>
      <td>4.927054</td>
      <td>...</td>
      <td>3.824710</td>
      <td>3.831202</td>
      <td>0.477456</td>
      <td>0.533584</td>
      <td>0.075802</td>
      <td>0.131765</td>
      <td>0.084469</td>
      <td>0.140431</td>
      <td>-2.913589</td>
      <td>-2.857938</td>
    </tr>
    <tr>
      <th>18380</th>
      <td>ë‹¤ë¥¸íŒ€(Noun)</td>
      <td>ë‹¤ë¥¸íŒ€</td>
      <td>Noun</td>
      <td>351</td>
      <td>0.000100</td>
      <td>-1.500599</td>
      <td>-1.477266</td>
      <td>0.466524</td>
      <td>0.501595</td>
      <td>5.463502</td>
      <td>...</td>
      <td>3.345351</td>
      <td>3.364846</td>
      <td>0.068836</td>
      <td>0.103804</td>
      <td>-0.620491</td>
      <td>-0.585624</td>
      <td>-2.916976</td>
      <td>-2.882200</td>
      <td>-2.913874</td>
      <td>-2.879098</td>
    </tr>
  </tbody>
</table>
<p>18381 rows Ã— 21 columns</p>






```python
relative_dic.to_csv('350ë§Œ_feature_dic.csv', index=False)
```


